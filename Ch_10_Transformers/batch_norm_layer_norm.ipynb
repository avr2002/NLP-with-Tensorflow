{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f45bb64-95c9-4542-a4a0-04a9b212c474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "1. [by AssemblyAI](https://lnkd.in/gg7Huvag)\n",
    "2. [by Ajay Halthor](https://lnkd.in/gCdDKpCN)\n",
    "3. Batch Norm Videos by DeepLearning.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a284a-440b-4a78-8282-60b6ab08ebd9",
   "metadata": {},
   "source": [
    "ùêíùê≠ùêöùêßùêùùêöùê´ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß ùêöùêßùêù ùêçùê®ùê´ùê¶ùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß is a way to bring our data to the same scale so that our neural network (or any ML algo.) can learn the patterns faster & efficiently.\n",
    "\n",
    "- In the context of neural networks, it's a way to avoid the famous vanishing/exploding gradient problem, which in simple terms, can be explained as our loss function becoming too small or too large during backpropagation.\n",
    "\n",
    "- Standardization brings values b/w 0-1, and Normalization makes mean=0 and variance/std_dev = 1 for the features in the data.\n",
    "\n",
    "- Normally Standardization/Normalization is done before passing our data to the neural net. Though it is seen that, applying normalization to the subsequent outputs of the layers(each neuron) of the neural networks helps in achieving better performance/accuracy.\n",
    "\n",
    "* **\n",
    "\n",
    "ùêãùêûùê≠'ùê¨ ùê¨ùêûùêû ùê°ùê®ùê∞ ùêÅùêöùê≠ùêúùê° ùêçùê®ùê´ùê¶ ùê∞ùê®ùê´ùê§ùê¨,\n",
    "\n",
    "- ùêÅùêöùê≠ùêúùê° ùêçùê®ùê´ùê¶ùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß: does the same thing, makes the data's mean=0 and variance=1 but with a little bit of twist.\n",
    "\n",
    "    1. First of all, it works on the batches of the data, as we pass it to the network.\n",
    "\n",
    "    2. It introdues two new learnable parameters(gamma & beta), which basically scales(to a diff. variance) and off-sets/shifts(to a diff. mean) the normalized output. [ y_new = gamma * y + beta ]\n",
    "\n",
    "    3. Why scale and shift the normalized outputs?\n",
    "\n",
    "        - In practice, not all features or layers in a neural network have the same importance. Some may be more or less relevant to the task at hand. By learning gamma and beta, the model can automatically adjust the scaling and shifting to account for these differences in scale & importance of layers, making the network more robust to various inputs. [This is a very simplified explanation]\n",
    "\n",
    "* **\n",
    "\n",
    "ùêÅùêûùêßùêûùêüùê¢ùê≠ùê¨ ùê®ùêü ùêÅùêöùê≠ùêúùê° ùêçùê®ùê´ùê¶ùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß:\n",
    "\n",
    "1. Seepds up training, leading to faster convergence and can lead to better performance.\n",
    "\n",
    "2. No need to manually normalize/standardize your data before passing it to the neural network. Simply add a Batch Normalization layer before the first layer of the model.\n",
    "\n",
    "3. It also regularizes the model, a little bit, helping with overfitting. Although you might need to separately add regularization depending upon the problem.\n",
    "\n",
    "    - This is due to its inherent noise introduced by normalizing each mini-batch differently, which can reduce overfitting to some extent.\n",
    "\n",
    "4. In addition to the reduced risk of overfitting, you can often use higher learning rates when batch normalization is employed, further accelerating the convergence of the network.\n",
    "\n",
    "5. Another advantage of batch normalization is its ability to reduce the risk of vanishing/exploding gradients.\n",
    "\n",
    "ùêñùê°ùêûùê´ùêû ùê≠ùê® ùêÆùê¨ùêû ùêÅùêöùê≠ùêúùê° ùêçùê®ùê´ùê¶ùêöùê•ùê¢ùê≥ùêöùê≠ùê¢ùê®ùêß?\n",
    "\n",
    "- **The batch normalization can be applied before and after the activation function.** However, the authors suggest it is best when applied before the activation function. It's an experimental thing.\n",
    "\n",
    "\n",
    "* **\n",
    "\n",
    ">‚ñ∂Ô∏è **From Comments of [AssemblyAI](https://lnkd.in/gg7Huvag) Video**\n",
    "\n",
    "As someone who is familiar with Batch Normalization I was personally missing a few important information which is why I add them here for the community:\n",
    "\n",
    "- *The normalisation is happening over the batch dimension (in contrast to other variants such as layer normalisation where we normalise over the layer dimension), meaning that we normalize the feature over the mini-batch*\n",
    "\n",
    "- **which is why it does not work well for smaller batch sizes (usually 16+)**\n",
    "\n",
    "- *another advantage for the scale and the offset parameter is that it allows the network to undo the BN, meaning that BN can't make your result worse*\n",
    "\n",
    "- *during test time with e.g one sample only we can't compute mean and std since we don't have a batch. This is why we use running statistics of mean and variance calculated during training*\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4836aa-76b1-412e-ab58-f53a02b31bf3",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/batch_norm.png' title=\"Image Credit: Pinecone - https://www.pinecone.io/learn/batch-layer-normalization/\">\n",
    "    <img src='images/batch_norm_working.png' title=\"Image Credit: Pinecone - https://www.pinecone.io/learn/batch-layer-normalization/\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f236b-c6db-433d-96b0-7371f5c30927",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Layer Normalization\n",
    "\n",
    "- [by Assembly AI](https://youtu.be/2V3Uduw1zwQ?si=ddoOlYhEpxQduO3y) : *This is dangerously tasty* üòã, *very simple to understand, watch it for visual understanding*.\n",
    "\n",
    "- [Why Does Batch Norm Work?](https://youtu.be/nUUqwaxLnWs?si=hh175YgH_ZDsWnbc) by DeepLearningAI - Visual understanding of Covariate Shift, black cat and colored cat example!\n",
    "\n",
    "- [by CodeEmporium](https://youtu.be/G45TuC6zRf4?si=wXm7WrWAipk_YOCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565d2bd-6bd3-481f-93a5-adc0ffcc41a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Why LayerNorm? Problems with BatchNorm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318c406-fe90-445f-b91d-b17ca3605cec",
   "metadata": {},
   "source": [
    "**Most of the issue with Batch Norm arises due to it's dependency on batch-size while training the network.**\n",
    "\n",
    "- **Hard to use with Sequence Data**\n",
    "    - *as sequences are of varying length, making calculations difficult*<br></br>\n",
    "\n",
    "- **Doesn't work well with small batch sizes**\n",
    "    - *Since Batch Norm calculates mean & variance for batches of data, thus mean & variance over small batch wouldn't represent the overall data well.*<br></br>\n",
    "    \n",
    "-  **Parallelization**\n",
    "    - *Cannot parallelize the network while using Batch Norm.*\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1210f-a7fd-46f3-8566-a0ba7ce61a61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### What is Layer Normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ba06e-c056-410e-a01c-dcf7d73942a1",
   "metadata": {},
   "source": [
    "- $\\textbf{Layer normalization}$ layer is similar to [$\\textit{batch normalization}$](https://youtu.be/yXOMHOpbon8?si=ajS12rDA2Y0Md8jC), is a way to reduce the $\\textbf{covariate shift}$ in neural networks, allowing them to be trained faster and achieve better performance. \n",
    "\n",
    "- In simple terms, **Covariate shift** refers to changes in the distribution of neural network activations as it trains, caused by changes in the data distribution like scale, mean, variance, etc.\n",
    "\n",
    "- **Batch normalization** computes the mean and variance of activations as an average over the samples in the batch, causing its performance to rely on mini-batches used to train the model.\n",
    "\n",
    "- **However, layer normalization computes the mean and variance (that is, the normalization terms) of the activations in such a way that the normalization terms are the same for every hidden unit in a layer.**\n",
    "\n",
    "- *In other words, layer normalization has a single mean and a variance value for all the hidden units in a layer. This is in contrast to batch normalization, which maintains individual mean and variance values for each hidden unit in a layer.* \n",
    "\n",
    "- *Moreover, unlike batch normalization, layer normalization does not average over the samples in the batch; instead, it leaves the averaging out and has different normalization terms for different inputs. By having a mean and variance per-sample, layer normalization gets rid of the dependency on the mini-batch size.*\n",
    "\n",
    "* **\n",
    "\n",
    "- **Benefits of Layer Norm:**\n",
    "    - *can deal with sequences like RNNs*\n",
    "    - *any batch number works*\n",
    "    - *can parallelize*<br></br>\n",
    "    \n",
    "- **Layer Norm doesn't work well with CNNs, Batch Norm is preferred in case of CNNs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0fa8d-5416-4d00-b2c3-8b76d13a6bd4",
   "metadata": {},
   "source": [
    "* **\n",
    "\n",
    "<div align='center'>\n",
    "    <h4>Batch Normalization</h4>\n",
    "    <img src='images/batch_norm_working.png' title=\"Image Credit: Pinecone - https://www.pinecone.io/learn/batch-layer-normalization/\">\n",
    "    <h4>Layer Normalization</h4>\n",
    "    <img src='images/layer_norm_working.png' title=\"Image Credit: Pinecone - https://www.pinecone.io/learn/batch-layer-normalization/\">\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965664d-6ae5-47ca-8758-1f3e973c8896",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Covariate Shift & Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a189011-dbb5-486e-bb87-3af7b4b6d674",
   "metadata": {},
   "source": [
    "1. $\\texttt{Covariate Shift:}$\n",
    "    - $\\textit{Covariate Shift}$ refers to changes in the distribution of activations or features within a neural network as the model goes through training. \n",
    "\n",
    "    - *In simpler terms, it's the phenomenon where the statistical properties of the i/p to a neural network change over time. This change can be caused by various factors, such as changes in the data distribution, changes in the model's parameters, or the inherent non-stationarity of the data.*\n",
    "\n",
    "    - *For instance, during the training of a neural network, the distribution of data that it sees can change as the model adapts to new examples. This can lead to differences in the scale, mean, or variance of the activations within the network. When this happens, the network may need to continuously adapt to these changes, making training slower and less stable.*\n",
    "    \n",
    "* **\n",
    "\n",
    "2. $\\texttt{Reducing Covariate Shift:}$\n",
    "\n",
    "    - **Batch Normalization (BatchNorm):** \n",
    "        - Batch normalization is a technique used to mitigate covariate shift. \n",
    "        \n",
    "        - It works by normalizing (scaling and shifting) the activations within each mini-batch of data during training. \n",
    "        \n",
    "        - This helps stabilize the distribution of activations, making training more efficient and enabling the use of higher learning rates.<br></br>\n",
    "\n",
    "    - **Layer Normalization (LayerNorm):** \n",
    "        - Layer normalization is similar to batch normalization but operates at a different level. \n",
    "        \n",
    "        - While batch normalization normalizes activations across a mini-batch, layer normalization normalizes activations across the features at each layer. \n",
    "        \n",
    "        - In other words, it normalizes the activations for a single training example, independently for each feature, rather than relying on statistics computed over a mini-batch.\n",
    "    \n",
    "* **\n",
    "\n",
    "3. $\\texttt{Benefits of Layer Normalization:}$<br>*Layer normalization offers several advantages:*\n",
    "\n",
    "    - **Reducing Covariate Shift:** Layer normalization, like batch normalization, helps reduce the effects of covariate shift by ensuring that the mean and variance of the activations within each layer remain relatively constant during training. This stabilizes the training process.\n",
    "\n",
    "    - **Independence from Batch Size:** Unlike batch normalization, layer normalization is less dependent on the mini-batch size. It is often used in scenarios where batch sizes are small or even when processing single examples (like in RNNs).\n",
    "\n",
    "    - **Applicability to Different Architectures:** Layer normalization is used in a wide range of neural network architectures, including Transformers, RNNs, and feedforward networks.\n",
    "    \n",
    "* **\n",
    "\n",
    "*$\\textit{In summary}$, covariate shift, which is the change in the distribution of neural network activations during training, can hinder the training process and negatively impact model performance. Techniques like layer normalization, by ensuring stable statistics of activations at each layer, help alleviate this problem and make training more efficient and effective, ultimately leading to better model performance.*\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982af1b-7cfb-4ca8-85de-7c90f5e57c8e",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/residual_conn_and_layer_normalization.png' title='NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012bfd1d-84ac-4dc1-8899-f77539c97a1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training & Testing Time in BatchNorm & LayerNorm\n",
    "\n",
    "**Credit: ChatGPT**\n",
    "\n",
    "- [Why Does Batch Norm Work?](https://youtu.be/nUUqwaxLnWs?si=hh175YgH_ZDsWnbc) by DeepLearningAI - Visual understanding of Covariate Shift, black cat and colored cat example!\n",
    "- [Batch Norm At Test Time](https://youtu.be/5qefnAek8OA?si=Ao0d0ELZ6FpoK-MN) by DeepLearningAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a8232-08e8-4b5a-9b9c-eb504e368e95",
   "metadata": {
    "tags": []
   },
   "source": [
    "Batch normalization and layer normalization are applied differently during training and testing (or inference) in neural networks. They help ensure that the network performs consistently in both phases. Here's how they work at training and test times, along with the key differences:\n",
    "\n",
    "#### **Training Time:**\n",
    "\n",
    "- **Batch Normalization (BatchNorm) during Training:**\n",
    "\n",
    "    1. During training, BatchNorm operates on each mini-batch of data independently. For each mini-batch, it calculates the mean and variance of the activations within that batch.\n",
    "\n",
    "    2. It then normalizes the activations within that mini-batch, scaling them to have zero mean and unit variance.\n",
    "\n",
    "    3. After normalization, BatchNorm applies learnable parameters (gamma and beta) to each feature to allow for scaling and shifting.\n",
    "\n",
    "    4. These learnable parameters (gamma and beta) are optimized during training via backpropagation.\n",
    "\n",
    "    5. The purpose of BatchNorm during training is to reduce covariate shift, stabilize training, and speed up convergence.\n",
    "\n",
    "- **Layer Normalization during Training:**\n",
    "\n",
    "    1. During training, LayerNorm works on a single training example independently. It calculates the mean and variance of the activations for each feature across all the elements of a training example (e.g., for a single time step in RNNs).\n",
    "\n",
    "    2. It normalizes the activations for that training example, making the mean zero and the variance one for each feature.\n",
    "\n",
    "    3. LayerNorm then applies learnable parameters (gamma and beta) to each feature for scaling and shifting.\n",
    "\n",
    "    4. As with BatchNorm, these learnable parameters are optimized during training through backpropagation.\n",
    "\n",
    "    5. LayerNorm at training time aims to stabilize the activations within each training example.\n",
    "\n",
    "* **\n",
    "\n",
    "#### **Test Time (Inference):**\n",
    "\n",
    "- **Batch Normalization at Test Time:**\n",
    "\n",
    "    1. During inference, BatchNorm typically doesn't have the luxury of processing data in mini-batches because you might be dealing with a single data point.\n",
    "\n",
    "    2. Instead, the mean and variance statistics calculated during training are used to normalize the activations during inference. These statistics are usually stored, so they can be applied consistently to all test examples, by computing exponentially weighted moving average of mean & variance. [see deepL.ai video](https://youtu.be/5qefnAek8OA?si=Ao0d0ELZ6FpoK-MN)\n",
    "\n",
    "    3. The scaling and shifting parameters (gamma and beta) learned during training are applied as well, just as during training.\n",
    "\n",
    "- **Layer Normalization at Test Time:**\n",
    "\n",
    "    1. During inference, LayerNorm processes each example independently, just like during training. This makes it suitable for scenarios with varying batch sizes or even when processing a single example.\n",
    "\n",
    "    2. The mean and variance statistics calculated during training are not used during inference. Instead, LayerNorm independently calculates the mean and variance for each feature within the current test example.\n",
    "\n",
    "    3. Similar to BatchNorm, the scaling and shifting parameters (gamma and beta) learned during training are applied during inference.\n",
    "\n",
    "* **\n",
    "\n",
    "#### **Key Differences:**\n",
    "\n",
    "1. **Data Independence:** BatchNorm operates on mini-batches during training and test time, whereas LayerNorm operates on individual examples. This makes LayerNorm more suitable for situations where you don't have fixed batch sizes.\n",
    "\n",
    "2. **Statistics Computation:** BatchNorm uses the mean and variance statistics calculated during training for normalization during inference. In contrast, LayerNorm recalculates these statistics independently during inference.\n",
    "\n",
    "3. **Applicability:** BatchNorm is more commonly used in feedforward neural networks, while LayerNorm is prevalent in recurrent neural networks and Transformers due to its sequential nature.\n",
    "\n",
    "\n",
    "Both normalization techniques aim to stabilize training, but their design and behavior at training and test times differ to suit the specific requirements of different types of networks and data scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
