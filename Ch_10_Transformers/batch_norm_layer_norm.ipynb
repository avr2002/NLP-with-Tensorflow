{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f45bb64-95c9-4542-a4a0-04a9b212c474",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a284a-440b-4a78-8282-60b6ab08ebd9",
   "metadata": {},
   "source": [
    "ğ’ğ­ğšğ§ğğšğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ is a way to bring our data to the same scale so that our neural network (or any ML algo.) can learn the patterns faster & efficiently.\n",
    "\n",
    "- In the context of neural networks, it's a way to avoid the famous vanishing/exploding gradient problem, which in simple terms, can be explained as our loss function becoming too small or too large during backpropagation.\n",
    "\n",
    "- Standardization brings values b/w 0-1, and Normalization makes mean=0 and variance/std_dev = 1 for the features in the data.\n",
    "\n",
    "- Normally Standardization/Normalization is done before passing our data to the neural net. Though it is seen that, applying normalization to the subsequent outputs of the layers(each neuron) of the neural networks helps in achieving better performance/accuracy.\n",
    "\n",
    "* **\n",
    "\n",
    "ğ‹ğğ­'ğ¬ ğ¬ğğ ğ¡ğ¨ğ° ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ ğ°ğ¨ğ«ğ¤ğ¬,\n",
    "\n",
    "- ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§: does the same thing, makes the data's mean=0 and variance=1 but with a little bit of twist.\n",
    "\n",
    "    1. First of all, it works on the batches of the data, as we pass it to the network.\n",
    "\n",
    "    2. It introdues two new learnable parameters(gamma & beta), which basically scales(to a diff. variance) and off-sets/shifts(to a diff. mean) the normalized output. [ y_new = gamma * y + beta ]\n",
    "\n",
    "    3. Why scale and shift the normalized outputs?\n",
    "\n",
    "        - In practice, not all features or layers in a neural network have the same importance. Some may be more or less relevant to the task at hand. By learning gamma and beta, the model can automatically adjust the scaling and shifting to account for these differences in scale & importance of layers, making the network more robust to various inputs. [This is a very simplified explanation]\n",
    "\n",
    "* **\n",
    "\n",
    "ğğğ§ğğŸğ¢ğ­ğ¬ ğ¨ğŸ ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§:\n",
    "\n",
    "1. Seepds up training, leading to faster convergence and can lead to better performance.\n",
    "\n",
    "2. No need to manually normalize/standardize your data before passing it to the neural network. Simply add a Batch Normalization layer before the first layer of the model.\n",
    "\n",
    "3. It also regularizes the model, a little bit, helping with overfitting. Although you might need to separately add regularization depending upon the problem.\n",
    "\n",
    "    - This is due to its inherent noise introduced by normalizing each mini-batch differently, which can reduce overfitting to some extent.\n",
    "\n",
    "4. In addition to the reduced risk of overfitting, you can often use higher learning rates when batch normalization is employed, further accelerating the convergence of the network.\n",
    "\n",
    "5. Another advantage of batch normalization is its ability to reduce the risk of vanishing/exploding gradients.\n",
    "\n",
    "ğ–ğ¡ğğ«ğ ğ­ğ¨ ğ®ğ¬ğ ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§?\n",
    "\n",
    "- The batch normalization can be applied before and after the activation function. However, the authors suggest it is best when applied before the activation function. It's an experimental thing.\n",
    "\n",
    "\n",
    "* **\n",
    "\n",
    "As someone who is familiar with Batch Normalization I was personally missing a few important information which is why I add them here for the community:\n",
    "\n",
    "- The normalisation is happening over the batch dimension (in contrast to other variants such as layer normalisation where we normalise over the layer dimension), meaning that we normalize the feature over the mini-batch\n",
    "\n",
    "- which is why it does not work well for smaller batch sizes (usually 16+)\n",
    "\n",
    "- another advantage for the scale and the offset parameter is that it allows the network to undo the BN, meaning that BN can't make your result worse\n",
    "\n",
    "- during test time with e.g one sample only we can't compute mean and std since we don't have a batch. This is why we use running statistics of mean and variance calculated during training\n",
    "\n",
    "* **\n",
    "\n",
    "Refrences:\n",
    "1. [by AssemblyAI](https://lnkd.in/gg7Huvag)\n",
    "2. [by Ajay Halthor](https://lnkd.in/gCdDKpCN)\n",
    "3. [by deeplizard](https://lnkd.in/gZmBmkQt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f236b-c6db-433d-96b0-7371f5c30927",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ba06e-c056-410e-a01c-dcf7d73942a1",
   "metadata": {},
   "source": [
    "- $\\textbf{Layer normalization}$ layer is similar to [$\\textit{batch normalization}$](https://youtu.be/yXOMHOpbon8?si=ajS12rDA2Y0Md8jC), is a way to reduce the $\\textbf{covariate shift}$ in neural networks, allowing them to be trained faster and achieve better performance. \n",
    "\n",
    "- **Covariate shift** refers to changes in the distribution of neural network activations (caused by changes in the data distribution), which transpires as the model goes through model training. These changes in the distribution damage consistency during model training and negatively impact the model.\n",
    "\n",
    "- **Batch normalization** computes the mean and variance of activations as an average over the samples in the batch, causing its performance to rely on mini-batches used to train the model.\n",
    "\n",
    "- **However, layer normalization computes the mean and variance (that is, the normalization terms) of the activations in such a way that the normalization terms are the same for every hidden unit in a layer.**\n",
    "\n",
    "- *In other words, layer normalization has a single mean and a variance value for all the hidden units in a layer. This is in contrast to batch normalization, which maintains individual mean and variance values for each hidden unit in a layer.* \n",
    "\n",
    "- *Moreover, unlike batch normalization, layer normalization does not average over the samples in the batch; instead, it leaves the averaging out and has different normalization terms for different inputs. By having a mean and variance per-sample, layer normalization gets rid of the dependency on the mini-batch size.*\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a189011-dbb5-486e-bb87-3af7b4b6d674",
   "metadata": {},
   "source": [
    "1. $\\texttt{Covariate Shift:}$\n",
    "    - $\\textit{Covariate Shift}$ refers to changes in the distribution of activations or features within a neural network as the model goes through training. \n",
    "\n",
    "    - *In simpler terms, it's the phenomenon where the statistical properties of the i/p to a neural network change over time. This change can be caused by various factors, such as changes in the data distribution, changes in the model's parameters, or the inherent non-stationarity of the data.*\n",
    "\n",
    "    - *For instance, during the training of a neural network, the distribution of data that it sees can change as the model adapts to new examples. This can lead to differences in the scale, mean, or variance of the activations within the network. When this happens, the network may need to continuously adapt to these changes, making training slower and less stable.*\n",
    "    \n",
    "* **\n",
    "\n",
    "2. $\\texttt{Reducing Covariate Shift:}$\n",
    "\n",
    "    - **Batch Normalization (BatchNorm):** \n",
    "        - Batch normalization is a technique used to mitigate covariate shift. \n",
    "        \n",
    "        - It works by normalizing (scaling and shifting) the activations within each mini-batch of data during training. \n",
    "        \n",
    "        - This helps stabilize the distribution of activations, making training more efficient and enabling the use of higher learning rates.<br></br>\n",
    "\n",
    "    - **Layer Normalization (LayerNorm):** \n",
    "        - Layer normalization is similar to batch normalization but operates at a different level. \n",
    "        \n",
    "        - While batch normalization normalizes activations across a mini-batch, layer normalization normalizes activations across the features at each layer. \n",
    "        \n",
    "        - In other words, it normalizes the activations for a single training example, independently for each feature, rather than relying on statistics computed over a mini-batch.\n",
    "    \n",
    "* **\n",
    "\n",
    "3. $\\texttt{Benefits of Layer Normalization:}$<br>*Layer normalization offers several advantages:*\n",
    "\n",
    "    - **Reducing Covariate Shift:** Layer normalization, like batch normalization, helps reduce the effects of covariate shift by ensuring that the mean and variance of the activations within each layer remain relatively constant during training. This stabilizes the training process.\n",
    "\n",
    "    - **Independence from Batch Size:** Unlike batch normalization, layer normalization is less dependent on the mini-batch size. It is often used in scenarios where batch sizes are small or even when processing single examples (like in RNNs).\n",
    "\n",
    "    - **Applicability to Different Architectures:** Layer normalization is used in a wide range of neural network architectures, including Transformers, RNNs, and feedforward networks.\n",
    "    \n",
    "* **\n",
    "\n",
    "*$\\textit{In summary}$, covariate shift, which is the change in the distribution of neural network activations during training, can hinder the training process and negatively impact model performance. Techniques like layer normalization, by ensuring stable statistics of activations at each layer, help alleviate this problem and make training more efficient and effective, ultimately leading to better model performance.*\n",
    "\n",
    "* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
