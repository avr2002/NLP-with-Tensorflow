{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90450be9-e9f6-46f5-8cb3-17b3849f8d0c",
   "metadata": {},
   "source": [
    "#### READ/WATCH THIS LATER:\n",
    "\n",
    "<ul>\n",
    "  <li><a href='https://www.youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1'>‚úÖ StatQuest - Watch from Word-Embeddings to Transformers(Also revise RRN, LSTM videos)</a></li>\n",
    "  <li><a href=\"https://e2eml.school/transformers\">‚úÖ Transformers from Scratch - Brandon Rohrer</a></li>\n",
    "  <li><a href='https://youtu.be/rPFkX5fJdRY?si=TjTc9X4ltfCZmIhP'>‚úÖ Transformers: Zero to Hero - CodeEmporium</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86807607-20c0-45cd-8840-b6b53154ea2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Intoduction ü•∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca902-fa24-4717-8a3b-737b09cc798f",
   "metadata": {},
   "source": [
    "- Transformer models has revolutionized solving machine learning problems that involve sequential data. \n",
    "\n",
    "- They have advanced the SOTA by a significant margin compared to the previous leaders, RNN-based models. Transformer have outperformed other sequential models such as LSTMs and GRUs\n",
    "\n",
    "- *One of the primary reasons that the Transformer model is so performant is that it has access to the whole sequence of items (e.g. sequence of tokens), as opposed to RNN-based models, which look at one item at a time.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb940ea7-9376-4d79-a69f-c3e1e2d22596",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Transformer Architecture ü§ñ\n",
    "\n",
    "$\\rightarrow$ Orginal Paper: [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb83c6f-34d7-432a-b645-68784400cfc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Key Points on Transformers\n",
    "\n",
    "- *A Transformer is a type of Seq2Seq model.*\n",
    "- *Transformer models can work with both image & text data.*\n",
    "- *The Transformer model takes in sequence of inputs and maps that to a seq. of outputs.*\n",
    "- *Just like Seq2Seq model, the Transformer consists of an encoder-decoder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337afbc-de74-46d3-9fa6-40fb305e0ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Overview of how Transformer Model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6105c-bce8-487c-8a7f-de3ca3ddf818",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let‚Äôs understand how the Transformer model works in context of Language Translation task:\n",
    "\n",
    "- *The encoder takes in a sequence of source language tokens and produces a sequence of interim outputs.* \n",
    "\n",
    "- *Then the decoder takes in a sequence of target language tokens and predicts the next token for each time step (the teacher forcing technique).* \n",
    "\n",
    "    - Teacher Forcing Technique: The objective of the decoder is, given the last encoder state and the previous token the decoder predicted, predict the next token.<br></br>\n",
    "\n",
    "- *Both the encoder and the decoder use attention mechanisms to improve performance.* \n",
    "\n",
    "    - For example, the decoder uses attention to inspect all the past encoder states and previous decoder inputs.<br></br>\n",
    "\n",
    "- *The attention mechanism is conceptually similar to Bahdanau attention.* [ [üîóLink to my Notes on this.](https://github.com/avr2002/NLP-with-Tensorflow/blob/main/Ch_09_Seq-to-Seq%20Learning/01_Seq-to-Seq%20Learning-NMT.ipynb) ]\n",
    "    \n",
    "<div align='center'>\n",
    "    <img src='images/enc_dec.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6ad8-ad23-4052-8167-6d47e986f0c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encoder & Decoder ‚òØÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26675434-3a8e-4945-ab85-6365845890b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Overview: Encoder, Decoder, Self-Attention Layer, FCC-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea71b69-2a8f-48dc-9e32-17c96b506bd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Understanding basic building blocks of a Transformer layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efe914-7ccc-4a06-bca5-5f8957cf5bfa",
   "metadata": {},
   "source": [
    "1. The Encoder and Decoder has almost same architecture with few differences. Both of them are designed to consume a sequence of i/p items at a time, but their goals during the task differ:\n",
    "\n",
    "    1. *The encoder produces a latent representation with the inputs, whereas*\n",
    "    \n",
    "    2. *The decoder produces a target o/p with it's inputs and the encoder's outputs.*\n",
    "\n",
    "2. To perform these computations, these inputs are propagated through several stacked layers. \n",
    "    - *Each layer within these models takes in a sequence of elements and outputs another sequence of elements.* \n",
    "    \n",
    "    - *Each layer is also made from several sub-layers that encapsulate different computations performed on a sequence of input tokens to produce a sequence of outputs.*\n",
    "    \n",
    "3. **A layer found in the Transformer mainly comprises the following two sub-layers:**\n",
    "    - A **Self-Attention Layer**\n",
    "    - *A Fully Connected Layer $\\text{(FCC-L)}$*<br></br>\n",
    "    \n",
    "    \n",
    "<div align='center'>\n",
    "    <h5>Diff. b/w Self-Attention & FCC Layer</h5>\n",
    "    <img src='images/self_attention_and_fcc_layers.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9050f-0b65-4040-9c97-f8ae25966a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901690ee-3f5f-42c1-9954-c5691479436b",
   "metadata": {},
   "source": [
    "4. The self-attention layer produces its o/p using matrix multiplications & activation functions, similar to a FCC-Layer. It takes in a seq. of i/ps and produces a seq. of o/ps.\n",
    "\n",
    "    - *However,* **a special characteristic of the self-attention layer** *is that, when producing an output at each time step, it has access to all the other inputs in that sequence.* \n",
    "\n",
    "    - *This makes learning and remembering long sequences of inputs trivial for this layer.* \n",
    "\n",
    "        - *For comparison, RNNs struggle to remember long sequences of inputs as they need to go through each input sequentially.*<br></br> \n",
    "\n",
    "    - *Additionally, by design, the self-attention layer can select and combine different inputs at each time step based on the task it‚Äôs solving.* \n",
    "\n",
    "**This makes Transformers very powerful in sequential learning tasks.** \n",
    "\n",
    "* **\n",
    "\n",
    "##### $\\rightarrow$ Let's understand this:\n",
    ">*Additionally, by design, the self-attention layer can select and combine different inputs at each time step based on the task it‚Äôs solving.*\n",
    "\n",
    "$\\textit{Why it‚Äôs important to selectively combine different input elements this way?}$\n",
    "\n",
    "- In an NLP context, the self-attention layer enables the model to peek at other words while processing a certain word. \n",
    "\n",
    "- This means that while the encoder is processing the word $\\textit{it}$ in the sentence $\\textit{I kicked the ball and it disappeared}$, the model can attend to the word $\\textit{ball}$ and understand the context of the word $\\textit{it}$. \n",
    "\n",
    "- **By doing this, the Transformer can learn dependencies and disambiguate words, which leads to better language understanding.**\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <a href='https://amzn.eu/d/7bbPm74'><i>Self-Attention Intutive Exp. by the author</i></a>\n",
    "</div>\n",
    "<div align='center'>\n",
    "    <img src='images/self_attention_exp.png'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2046904-3d85-470b-945d-df759b384fc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ The Fully Connected Layer $\\textit{(FCC-L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb5688-1a33-46d6-a2d6-a45a17ae0f0c",
   "metadata": {},
   "source": [
    "- The self-attention layer is followed by a **FCC-Layer**, which has all the i/p nodes connected to all the o/p nodes, (optionally) followed by a non-linear activation function.\n",
    "\n",
    "-  It takes the o/p elements produced by the self-attention sub-layer and produces a hidden representation for each o/p element. \n",
    "\n",
    "- Unlike the self-attention layer, the fully connected layer treats individual sequence items independently, performing computations on them in an element-wise fashion.\n",
    "\n",
    "- They introduce non-linear transformations while making the model deeper, thus allowing the model to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458066ef-3b52-4660-b270-50f68029bd33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\rightarrow$ a bit more on Enoder & Decoder\n",
    "\n",
    "Breakdown of Encoder's & Decoder's sub-layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a844093-ee2a-4391-b60d-5c7ca97d0dbb",
   "metadata": {},
   "source": [
    "$\\textit{Before diving in, let's establish some basics:}$\n",
    "\n",
    "- *The encoder takes in an i/p seq. and the decoder takes in an i/p seq. as well (a diff. sequence to the encoder i/p). Then the decoder produces an o/p seq.*\n",
    "\n",
    "- Let's call a single item in these sequences a $\\textit{token}$.\n",
    "\n",
    "* **\n",
    "\n",
    "$\\large{\\textit{Encoder:}}$\n",
    "\n",
    "The encoder consists of a stack of layers, where each layer consists of $2$ sub-layers:\n",
    "- A $\\textit{Self-Attention Layer}$ :\n",
    "    - *Generates a latent representation for each encoder i/p token in the sequence.* \n",
    "    \n",
    "    - *For each i/p token, this layer looks at the whole seq. and selects other tokens in the seq. that enrich the semantics of the generated hidden o/p for that token (i.e., $\\textit{‚Äòattended‚Äô representation}$).*\n",
    "\n",
    "- A $\\textit{FCC-Layer}$ :\n",
    "    - Generates an element-wise deeper hidden representation of the **attended representation of the encoder**.\n",
    "    \n",
    "* **\n",
    "\n",
    "$\\large{\\textit{Decoder:}}$\n",
    "\n",
    "The decoder layer consists of $3$ sub-layers:\n",
    "- $\\textit{A Masked Self-Attention Layer}$ :\n",
    "    - *For each decoder i/p, a token looks at all the tokens to the left of it.*\n",
    "    \n",
    "    - *The decoder needs to mask words to the right to prevent the model from seeing words in the future.* \n",
    "    \n",
    "    - *Having access to successive words during prediction can make the prediction task trivial for the decoder.* \n",
    "\n",
    "- $\\textit{An Attention Layer}$ :\n",
    "    - For each i/p token in the decoder, it looks at both the $\\textit{encoder's outputs}$ and the $\\textit{decoder‚Äôs masked attended output}$ to generate a semantically rich hidden output.\n",
    "    \n",
    "    - *Since this layer is not only focused on decoder inputs, we‚Äôll call this an* $\\textit{attention layer.}$\n",
    "\n",
    "- $\\textit{A FCC-Layer}$ :\n",
    "    - Generates an element-wise deeper hidden representation of the *attended representation of the decoder*.\n",
    "    \n",
    "* **\n",
    "$\\textit{English to French Transalation :}$\n",
    "$$\\textit{dogs are great} \\rightarrow \\textit{les chiens sont super}$$\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/transformer_eng_to_french.png' title='Source: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c87101-aed2-44ee-a82c-eb8fc181fb33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## *Mechanics of the* $\\textit{Self-Attention Layer}$ üîß "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d97256-2740-4504-a8e3-ef9437f8e996",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Computing the output of the $\\textit{self-attention layer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06a389-1273-4570-a1d5-4159b7e4c094",
   "metadata": {},
   "source": [
    "$\\rightarrow Definition: \\textit{Query(Q), Key(K), & Value(V)}$\n",
    "\n",
    "1. *There are 3 key concepts to understand the computations of self-attention technique:* $\\textit{Query(Q), Key(K), & Value(V)}:$\n",
    "\n",
    "2. The $\\textit{query}$ and the $\\textit{key}$ are used to generate an $\\textit{affinity matrix}$. \n",
    "\n",
    "3. *For the decoder's attention layer, the affinity matrix‚Äôs position $i,j$ represents how similar the encoder state $(key, K)$ $\\textit{ i }$ is to the decoder input $j$ $(query, Q)$.* \n",
    "\n",
    "4. *Then, we create a weighted average of encoder states $(value, V)$ for each position, where the weights are given by the $\\textit{affinity matrix}$.*\n",
    "\n",
    "* **\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/example_q_k_v.png' title='Example as given by the author: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fad59-cc4f-4806-ace1-e614a4e0ff22",
   "metadata": {},
   "source": [
    "- To compute the $\\textit{query, key, and value}$, we use [linear projection](https://stackoverflow.com/questions/37889914/what-is-a-projection-layer-in-the-context-of-neural-networks) of the actual i/ps provided using weight matrices. The 3 weight matrices are:\n",
    "    - Query weights matrix $(W_q)$\n",
    "    - Key weights matrix $(W_k)$\n",
    "    - Value weights matrix $(W_v)$<br></br>\n",
    "\n",
    "- Each matrix produces 3 o/ps for a given token (at position $i$) in a given i/p seq. by multiplying with the weight matrix:\n",
    "\n",
    "$$Q_i = W_q q_i, K_i = W_k k_i \\text{ and } V_i = W_v v_i$$\n",
    "\n",
    "- $Q, K, \\text{ and } V$ are $[B, T, d]$ *sized tensor, where*\n",
    "    - $B = \\text{batch size}$, \n",
    "    - $T = \\text{# of time-steps}$, and \n",
    "    - $d = \\text{hyperparameter, represents dimensionality of the latent representation}$.<br></br>\n",
    "\n",
    "- **These are then used to compute the affinity matrix:**\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/affinity_matrix.png' title='Affinity Matrix: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "\n",
    "- The $\\textit{affinity matrix P}$ is computed as: $$P = softmax \\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "- *Then the final $\\textit{attended output of the self-attention layer}$ is computed as follows:* $$h = P \\cdot V = softmax \\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right) \\cdot V$$\n",
    "\n",
    "\n",
    "Here, $\\textit{Q = queries tensor}$, $\\textit{K = keys tensor}$, and $\\textit{V = values tensor}$. \n",
    "\n",
    "*This is what makes Transformer models so powerful; unlike LSTM models, Transformer models aggregate all tokens in a sequence to a single matrix multiplication, making these models highly parallelizable.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b5a62-775b-4894-ba4b-242d4572f68d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Embedding layers in the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907069c7-6779-4fa3-95d0-dc627aec381d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Primer on Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e5ebd-6c57-40a5-8eea-3aeadc18968c",
   "metadata": {},
   "source": [
    "- Word Embeddings are vector representation of words in such a way that words with similar meaning/context remain closer to each other, i.e. have similar word-vectors.\n",
    "\n",
    "- For example, the words `cat` and `dog` will have similar representations, whereas `cat` and `volcano` will have vastly different representations.\n",
    "\n",
    "- It comes in two variants: `skip-gram` and `continuous bag-of-words`. \n",
    "\n",
    "- Embeddings work by first defining a large matrix of size $V \\times E$, where $V = \\text{size of vocabulary}$ and $E = \\text{size of embeddings}$. \n",
    "\n",
    "- $E$ is a hyperparameter; a larger $E$ typically leads to more powerful word embeddings. **In practice, an embedding size beyond 300 is not useful.**\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc6f51-f109-4c16-8bec-9880aa513c59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### How ML models use word-embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f64cc4-a34a-401d-b1e3-86f1250e68b6",
   "metadata": {},
   "source": [
    "The following general approach (along with pre-training later to fine-tune these embeddings) is taken to incorporate word embeddings into a machine learning model:\n",
    "\n",
    "- Define a randomly initialized word embedding matrix (or pre-trained embeddings)\n",
    "\n",
    "- Define the model (randomly initialized) that uses word embeddings as the inputs and produces an output (for example, sentiment, or a language translation)\n",
    "\n",
    "- Train the whole model (embeddings and the model) end-to-end on the task\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b09228-c541-46d2-8cde-da1475762776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Embeddings in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a42181-f894-4dbc-bed7-783f9ea128b0",
   "metadata": {},
   "source": [
    "The same method is also used in Transformer models. Though Transformers have 2 diff. embeddings :\n",
    "\n",
    "1. $\\textit{Token Embeddings :}$ *provide a unique representation for each token seen by the model in an i/p sequence.*\n",
    "\n",
    "    - *The $\\textit{token embeddings}$ have a unique embedding vector for each token (such as character, word, and sub-word), depending on the model‚Äôs tokenizing mechanism.*\n",
    "\n",
    "* **\n",
    "\n",
    "\n",
    "2. $\\textit{Positional Embeddings :}$ *provide a unique representation for each position in the i/p seq.*\n",
    "\n",
    "    - **The $\\textit{positional embeddings}$ are used to signal the model where a token is appearing.**\n",
    "    \n",
    "    - *The primary purpose of $\\textit{positional embeddings}$ server is to inform the Transformer model where a word is appearning.*\n",
    "    \n",
    "    - **This is because unlike LSTMs/GRUs, Transformer models don't have a notion of sequence, as it processes the whole text in one go.**\n",
    "    \n",
    "    \n",
    "    - *Furthermore, a change to the position of a word can alter the meaning of a sentence/or a word.* *For example:*\n",
    "\n",
    "        - *Ralph loves his tennis ball.* **It** *likes to chase the ball*\n",
    "        - *Ralph loves his tennis ball.* *Ralph likes to chase* **it**<br></br>\n",
    "\n",
    "    - *In the sentences above, the word* **it** *refers to different things and the position of the word it can be used as a cue to identify this difference.*\n",
    "    \n",
    "    \n",
    "*The original Transformer paper uses the foll. equations to generate $\\textit{positional embeddings :}$*\n",
    "$$PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)$$\n",
    "\n",
    "$$PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)$$\n",
    "\n",
    "\n",
    "**where,**\n",
    "- $pos$ denotes the position in the sequence\n",
    "- $i$ denotes the $i^{th}$ feature dimension $(0 \\leq i < d)$.\n",
    "- *$d$ is a hyperparameter that defines the dimensionality of the latent representation*\n",
    "- *Even-numbered features use a sine function* and \n",
    "- *odd numbered features use a cosine function*\n",
    "\n",
    ">It is not entirely clear how the authors came up with the exact equation.üòÇüòÇ However, they do mention that they did not see a significant performance diff. b/w the\n",
    "above eq. and letting the model learn positional embeddings jointly during the training.\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/pos_embeddings.png' title='Positonal Embeddings: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "- **Above fig. presents how positional embeddings change as the time step and the feature position change. It can be seen that feature positions with higher indices have lower-frequency sinusoidal waves.**\n",
    "\n",
    "\n",
    ">‚úíÔ∏è**In Summary:** *Since Transformers don't have a built-in sense of word order (unlike RNNs or LSTMs), positional encodings are added to the input embeddings. These encodings provide information about the position of each word in the sequence, allowing the model to learn the sequential relationships.*\n",
    "\n",
    "* **\n",
    "\n",
    ">üóùÔ∏è**Note:** *both token and positional embeddings will have the same dimensionality $d$, making it possible to perform element-wise addition.* \n",
    "\n",
    "- **Finally, as the input to the model, the token embeddings and the positional embeddings are summed to form a single hybrid embedding vector.**\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/embeddings.png' title='Embeddings: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813eadc2-939b-4520-966e-c2735b85c33f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Residual & Normalization\n",
    "\n",
    "> üóùÔ∏è**Notes on** [**BatchNorm, LayerNorm, & Covariate Shift**](https://github.com/avr2002/NLP-with-Tensorflow/blob/main/Ch_10_Transformers/batch_norm_layer_norm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac53c6-43f5-456b-ae55-e30cffda3662",
   "metadata": {},
   "source": [
    "Another important characteristic of the Transformer models is the existence of the **residual connections and the normalization layers** in between the individual layers of the Transformer model.\n",
    "\n",
    "\n",
    "- $\\textit{Residual Connections :}$ *are formed by adding `\"a given layer's o/p\"` to the `\"o/p of one or more layers ahead\"`. This in-turn forms shortcut connections through the model and provides a stronger gradient flow by reducing vanishing gradient problem.*\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/residual_connection.png' title='NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "*In Transformer models, in each layer, $\\textit{residual connections}$ are created as:*\n",
    "- *Input to the self-attention sub-layer is added to the o/p of the self-attention sub-layer.*\n",
    "- *Input to the FCC sub-layer is added to the o/p of the FCC sub-layer.*\n",
    "\n",
    "\n",
    "Next, the o/p reinforced by the $\\textit{residual connections}$ goes via a $\\textit{\"layer-normalization\"}$ layer.\n",
    "\n",
    "- $\\textbf{Layer normalization}$ layer is similar to [$\\textit{batch normalization}$](https://youtu.be/yXOMHOpbon8?si=ajS12rDA2Y0Md8jC), is a way to reduce the $\\textbf{covariate shift}$ in neural networks, allowing them to be trained faster and achieve better performance. \n",
    "\n",
    "* **\n",
    "\n",
    "- $\\texttt{Covariate Shift:}$\n",
    "    - $\\textit{Covariate Shift}$ refers to changes in the distribution of activations or features within a neural network as the model goes through training. \n",
    "\n",
    "    - *In simpler terms, it's the phenomenon where the statistical properties of the i/p to a neural network change over time. This change can be caused by various factors, such as changes in the data distribution, changes in the model's parameters, or the inherent non-stationarity of the data.*\n",
    "\n",
    "    - *For instance, during the training of a neural network, the distribution of data that it sees can change as the model adapts to new examples. This can lead to differences in the scale, mean, or variance of the activations within the network. When this happens, the network may need to continuously adapt to these changes, making training slower and less stable.*\n",
    "    \n",
    "* **\n",
    "\n",
    "- **Batch normalization** computes the mean and variance of activations as an average over the samples in the batch, causing its performance to rely on mini-batches used to train the model.\n",
    "\n",
    "- **However, layer normalization computes the mean and variance (that is, the normalization terms) of the activations in such a way that the normalization terms are the same for every hidden unit in a layer.**\n",
    "\n",
    "- *In other words, layer normalization has a single mean and a variance value for all the hidden units in a layer. This is in contrast to batch normalization, which maintains individual mean and variance values for each hidden unit in a layer.* \n",
    "\n",
    "- *Moreover, unlike batch normalization, layer normalization does not average over the samples in the batch; instead, it leaves the averaging out and has different normalization terms for different inputs. By having a mean and variance per-sample, layer normalization gets rid of the dependency on the mini-batch size.*\n",
    "\n",
    "* **\n",
    "\n",
    "üóùÔ∏è**Notes on** [**BatchNorm, LayerNorm, & Covariate Shift**](https://github.com/avr2002/NLP-with-Tensorflow/blob/main/Ch_10_Transformers/batch_norm_layer_norm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8871cd1-c5b6-493c-95dd-b8587e087183",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/residual_conn_and_layer_normalization.png' title='NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e759c-5b22-4299-91d7-fa36b6ce5069",
   "metadata": {},
   "source": [
    "## Summary üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45f05d-5377-4f20-8b86-d040977bc4f7",
   "metadata": {},
   "source": [
    "We have discussed all the bells and whistles of the Transformer model.\n",
    "\n",
    "- The Transformer model is an encoder-decoder based model. \n",
    "\n",
    "- Both the encoder and the decoder have the same structure, apart from a few small differences. \n",
    "\n",
    "- The Transformer uses self-attention, a powerful parallelizable attention mechanism to attend to other inputs at every time step. \n",
    "\n",
    "- The Transformer also uses several embedding layers, such as token embeddings and positional embeddings, to inject information about tokens and their positioning. \n",
    "\n",
    "- The Transformer also uses residual connections and layer normalization to improve the performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
