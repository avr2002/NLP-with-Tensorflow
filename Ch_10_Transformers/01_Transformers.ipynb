{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90450be9-e9f6-46f5-8cb3-17b3849f8d0c",
   "metadata": {},
   "source": [
    "#### READ/WATCH THIS LATER:\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://e2eml.school/transformers\">‚úÖ Transformers from Scratch - Brandon Rohrer</a></li>\n",
    "  <li><a href='https://youtu.be/rPFkX5fJdRY?si=TjTc9X4ltfCZmIhP'>‚úÖ Transformers: Zero to Hero - CodeEmporium</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86807607-20c0-45cd-8840-b6b53154ea2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Intoduction ü•∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca902-fa24-4717-8a3b-737b09cc798f",
   "metadata": {},
   "source": [
    "- Transformer models has revolutionized solving machine learning problems that involve sequential data. \n",
    "\n",
    "- They have advanced the SOTA by a significant margin compared to the previous leaders, RNN-based models. Transformer have outperformed other sequential models such as LSTMs and GRUs\n",
    "\n",
    "- *One of the primary reasons that the Transformer model is so performant is that it has access to the whole sequence of items (e.g. sequence of tokens), as opposed to RNN-based models, which look at one item at a time.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb940ea7-9376-4d79-a69f-c3e1e2d22596",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Transformer Architecture ü§ñ\n",
    "\n",
    "$\\rightarrow$ Orginal Paper: [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb83c6f-34d7-432a-b645-68784400cfc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Key Points on Transformers\n",
    "\n",
    "- *A Transformer is a type of Seq2Seq model.*\n",
    "- *Transformer models can work with both image & text data.*\n",
    "- *The Transformer model takes in sequence of inputs and maps that to a seq. of outputs.*\n",
    "- *Just like Seq2Seq model, the Transformer consists of an encoder-decoder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337afbc-de74-46d3-9fa6-40fb305e0ddd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Overview of how Transformer Model works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6105c-bce8-487c-8a7f-de3ca3ddf818",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let‚Äôs understand how the Transformer model works in context of Language Translation task:\n",
    "\n",
    "- *The encoder takes in a sequence of source language tokens and produces a sequence of interim outputs.* \n",
    "\n",
    "- *Then the decoder takes in a sequence of target language tokens and predicts the next token for each time step (the teacher forcing technique).* \n",
    "\n",
    "    - Teacher Forcing Technique: The objective of the decoder is, given the last encoder state and the previous token the decoder predicted, predict the next token.<br></br>\n",
    "\n",
    "- *Both the encoder and the decoder use attention mechanisms to improve performance.* \n",
    "\n",
    "    - For example, the decoder uses attention to inspect all the past encoder states and previous decoder inputs.<br></br>\n",
    "\n",
    "- *The attention mechanism is conceptually similar to Bahdanau attention.* [ [üîóLink to my Notes on this.](https://github.com/avr2002/NLP-with-Tensorflow/blob/main/Ch_09_Seq-to-Seq%20Learning/01_Seq-to-Seq%20Learning-NMT.ipynb) ]\n",
    "    \n",
    "<div align='center'>\n",
    "    <img src='images/enc_dec.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6ad8-ad23-4052-8167-6d47e986f0c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Encoder & Decoder ‚òØÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26675434-3a8e-4945-ab85-6365845890b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Overview: Encoder, Decoder, Self-Attention Layer, FCC-Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea71b69-2a8f-48dc-9e32-17c96b506bd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Understanding basic building blocks of a Transformer layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efe914-7ccc-4a06-bca5-5f8957cf5bfa",
   "metadata": {},
   "source": [
    "1. The Encoder and Decoder has almost same architecture with few differences. Both of them are designed to consume a sequence of i/p items at a time, but their goals during the task differ:\n",
    "\n",
    "    1. *The encoder produces a latent representation with the inputs, whereas*\n",
    "    \n",
    "    2. *The decoder produces a target o/p with it's inputs and the encoder's outputs.*\n",
    "\n",
    "2. To perform these computations, these inputs are propagated through several stacked layers. \n",
    "    - *Each layer within these models takes in a sequence of elements and outputs another sequence of elements.* \n",
    "    \n",
    "    - *Each layer is also made from several sub-layers that encapsulate different computations performed on a sequence of input tokens to produce a sequence of outputs.*\n",
    "    \n",
    "3. **A layer found in the Transformer mainly comprises the following two sub-layers:**\n",
    "    - A **Self-Attention Layer**\n",
    "    - *A Fully Connected Layer $\\text{(FCC-L)}$*<br></br>\n",
    "    \n",
    "    \n",
    "<div align='center'>\n",
    "    <h5>Diff. b/w Self-Attention & FCC Layer</h5>\n",
    "    <img src='images/self_attention_and_fcc_layers.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9050f-0b65-4040-9c97-f8ae25966a15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901690ee-3f5f-42c1-9954-c5691479436b",
   "metadata": {},
   "source": [
    "4. The self-attention layer produces its o/p using matrix multiplications & activation functions, similar to a FCC-Layer. It takes in a seq. of i/ps and produces a seq. of o/ps.\n",
    "\n",
    "    - *However,* **a special characteristic of the self-attention layer** *is that, when producing an output at each time step, it has access to all the other inputs in that sequence.* \n",
    "\n",
    "    - *This makes learning and remembering long sequences of inputs trivial for this layer.* \n",
    "\n",
    "        - *For comparison, RNNs struggle to remember long sequences of inputs as they need to go through each input sequentially.*<br></br> \n",
    "\n",
    "    - *Additionally, by design, the self-attention layer can select and combine different inputs at each time step based on the task it‚Äôs solving.* \n",
    "\n",
    "**This makes Transformers very powerful in sequential learning tasks.** \n",
    "\n",
    "* **\n",
    "\n",
    "##### $\\rightarrow$ Let's understand this:\n",
    ">*Additionally, by design, the self-attention layer can select and combine different inputs at each time step based on the task it‚Äôs solving.*\n",
    "\n",
    "$\\textit{Why it‚Äôs important to selectively combine different input elements this way?}$\n",
    "\n",
    "- In an NLP context, the self-attention layer enables the model to peek at other words while processing a certain word. \n",
    "\n",
    "- This means that while the encoder is processing the word $\\textit{it}$ in the sentence $\\textit{I kicked the ball and it disappeared}$, the model can attend to the word $\\textit{ball}$ and understand the context of the word $\\textit{it}$. \n",
    "\n",
    "- **By doing this, the Transformer can learn dependencies and disambiguate words, which leads to better language understanding.**\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <a href='https://amzn.eu/d/7bbPm74'><i>Self-Attention Intutive Exp. by the author</i></a>\n",
    "</div>\n",
    "<div align='center'>\n",
    "    <img src='images/self_attention_exp.png'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2046904-3d85-470b-945d-df759b384fc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### $\\rightarrow$ The Fully Connected Layer $\\textit{(FCC-L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb5688-1a33-46d6-a2d6-a45a17ae0f0c",
   "metadata": {},
   "source": [
    "- The self-attention layer is followed by a **FCC-Layer**, which has all the i/p nodes connected to all the o/p nodes, (optionally) followed by a non-linear activation function.\n",
    "\n",
    "-  It takes the o/p elements produced by the self-attention sub-layer and produces a hidden representation for each o/p element. \n",
    "\n",
    "- Unlike the self-attention layer, the fully connected layer treats individual sequence items independently, performing computations on them in an element-wise fashion.\n",
    "\n",
    "- They introduce non-linear transformations while making the model deeper, thus allowing the model to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458066ef-3b52-4660-b270-50f68029bd33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\rightarrow$ a bit more on Enoder & Decoder\n",
    "\n",
    "Breakdown of Encoder's & Decoder's sub-layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a844093-ee2a-4391-b60d-5c7ca97d0dbb",
   "metadata": {},
   "source": [
    "$\\textit{Before diving in, let's establish some basics:}$\n",
    "\n",
    "- *The encoder takes in an i/p seq. and the decoder takes in an i/p seq. as well (a diff. sequence to the encoder i/p). Then the decoder produces an o/p seq.*\n",
    "\n",
    "- Let's call a single item in these sequences a $\\textit{token}$.\n",
    "\n",
    "* **\n",
    "\n",
    "$\\large{\\textit{Encoder:}}$\n",
    "\n",
    "The encoder consists of a stack of layers, where each layer consists of $2$ sub-layers:\n",
    "- A $\\textit{Self-Attention Layer}$ :\n",
    "    - *Generates a latent representation for each encoder i/p token in the sequence.* \n",
    "    \n",
    "    - *For each i/p token, this layer looks at the whole seq. and selects other tokens in the seq. that enrich the semantics of the generated hidden o/p for that token (i.e., $\\textit{‚Äòattended‚Äô representation}$).*\n",
    "\n",
    "- A $\\textit{FCC-Layer}$ :\n",
    "    - Generates an element-wise deeper hidden representation of the **attended representation of the encoder**.\n",
    "    \n",
    "* **\n",
    "\n",
    "$\\large{\\textit{Decoder:}}$\n",
    "\n",
    "The decoder layer consists of $3$ sub-layers:\n",
    "- $\\textit{A Masked Self-Attention Layer}$ :\n",
    "    - *For each decoder i/p, a token looks at all the tokens to the left of it.*\n",
    "    \n",
    "    - *The decoder needs to mask words to the right to prevent the model from seeing words in the future.* \n",
    "    \n",
    "    - *Having access to successive words during prediction can make the prediction task trivial for the decoder.* \n",
    "\n",
    "- $\\textit{An Attention Layer}$ :\n",
    "    - For each i/p token in the decoder, it looks at both the $\\textit{encoder's outputs}$ and the $\\textit{decoder‚Äôs masked attended output}$ to generate a semantically rich hidden output.\n",
    "    \n",
    "    - *Since this layer is not only focused on decoder inputs, we‚Äôll call this an* $\\textit{attention layer.}$\n",
    "\n",
    "- $\\textit{A FCC-Layer}$ :\n",
    "    - Generates an element-wise deeper hidden representation of the *attended representation of the decoder*.\n",
    "    \n",
    "* **\n",
    "$\\textit{English to French Transalation :}$\n",
    "$$\\textit{dogs are great} \\rightarrow \\textit{les chiens sont super}$$\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/transformer_eng_to_french.png' title='Source: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c87101-aed2-44ee-a82c-eb8fc181fb33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## *Mechanics of the* $\\textit{Self-Attention Layer}$ üîß "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d97256-2740-4504-a8e3-ef9437f8e996",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computing the output of the $\\textit{self-attention layer}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a06a389-1273-4570-a1d5-4159b7e4c094",
   "metadata": {},
   "source": [
    "$\\rightarrow Definition: \\textit{Query(Q), Key(K), & Value(V)}$\n",
    "\n",
    "1. *There are 3 key concepts to understand the computations of self-attention technique:* $\\textit{Query(Q), Key(K), & Value(V)}:$\n",
    "\n",
    "2. The $\\textit{query}$ and the $\\textit{key}$ are used to generate an $\\textit{affinity matrix}$. \n",
    "\n",
    "3. *For the decoder's attention layer, the affinity matrix‚Äôs position $i,j$ represents how similar the encoder state $(key, K)$ $\\textit{ i }$ is to the decoder input $j$ $(query, Q)$.* \n",
    "\n",
    "4. *Then, we create a weighted average of encoder states $(value, V)$ for each position, where the weights are given by the $\\textit{affinity matrix}$.*\n",
    "\n",
    "* **\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/example_q_k_v.png' title='Example as given by the author: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fad59-cc4f-4806-ace1-e614a4e0ff22",
   "metadata": {},
   "source": [
    "- To compute the $\\textit{query, key, and value}$, we use [linear projection](https://stackoverflow.com/questions/37889914/what-is-a-projection-layer-in-the-context-of-neural-networks) of the actual i/ps provided using weight matrices. The 3 weight matrices are:\n",
    "    - Query weights matrix $(W_q)$\n",
    "    - Key weights matrix $(W_k)$\n",
    "    - Value weights matrix $(W_v)$<br></br>\n",
    "\n",
    "- Each matrix produces 3 o/ps for a given token (at position $i$) in a given i/p seq. by multiplying with the weight matrix:\n",
    "\n",
    "$$Q_i = W_q q_i, K_i = W_k k_i \\text{ and } V_i = W_v v_i$$\n",
    "\n",
    "$Q, K, \\text{ and } V$ are $[B, T, d]$ *sized tensor*, where $B = \\text{batch size}$, $T = \\text{# of time-steps}$, and $d = \\text{hyperparameter, represents dimensionality of the latent representation}$.\n",
    "\n",
    "- **These are then used to compute the affinity matrix:**\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/affinity_matrix.png' title='Affinity Matrix: NLP with TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "\n",
    "- The $\\textit{affinity matrix P}$ is computed as: $$P = softmax \\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "- *Then the final $\\textit{attended output of the self-attention layer}$ is computed as follows:* $$h = P \\cdot V = softmax \\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "Here, $\\textit{Q = queries tensor}$, $\\textit{K = keys tensor}$, and $\\textit{V = values tensor}$. \n",
    "\n",
    "This is what makes Transformer models so powerful; unlike LSTM models, Transformer models aggregate all tokens in a sequence to a single matrix multiplication, making these models highly parallelizable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b5a62-775b-4894-ba4b-242d4572f68d",
   "metadata": {},
   "source": [
    "### Embedding layers in the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89359c4d-ac81-4a6e-8edf-3de2685121ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
