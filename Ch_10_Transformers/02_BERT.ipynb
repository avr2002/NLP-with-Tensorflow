{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68b6221-83b0-4375-86a3-1b8775a50f53",
   "metadata": {},
   "source": [
    "# BERT \n",
    "**Bidirectional Encoder Representation from Transformers (BERT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6a678-5df2-4957-8143-f098cab9a31f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Understanding BERT\n",
    "\n",
    "$\\rightarrow$ [**BERT Paper**](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f659d63b-0e15-4399-950a-d33588aafef3",
   "metadata": {
    "tags": []
   },
   "source": [
    "- *The Transformer models are divided into 2 main factors:*\n",
    "    - **Encoder-based models**\n",
    "    - **Decoder-based models (auto-regressive)**<br></br>\n",
    "    \n",
    "- *In other words, either the encoder or the decoder part of the Transformer provides the foundation for these models, compared to using both the encoder and the decoder.* **The main diff. b/w the two is how attention is used.**\n",
    "\n",
    "- **Encoder-based models use bi-directional attention, whereas Decoder-based models use auto-regressive(i.e., left to right) attention.**\n",
    "\n",
    "**BERT is an encoder-based Transformer model. It takes an i/p seq.(collection of tokens) and produces an encoded o/p seq.**\n",
    "\n",
    "<div alin='center'>\n",
    "    <img src='images/bert_high_level_architecture.png', title='NLP w/ TensorFlow by Thushan Ganegedara'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08585a59-5bf1-4c22-8706-04aa05237e32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Input Processing for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce624ca0-8cf1-456d-a4f5-cfa029c562ce",
   "metadata": {},
   "source": [
    "**BERT inserts some special tokens into the i/p, while taking it:**\n",
    "\n",
    "- At the beginning, it inserts a `[CLS]` token (`term_classification`) that is used to generate the final hidden rep. for certain types of task (like seq. classification). It represents the o/p after attending to all the tokens.\n",
    "\n",
    "- Next, it also inserts a `[SEP]` token (`seperation`) depending on the type of i/p. The `[SEP]` token marks the end and the beginning of diff. sequences in the i/p.\n",
    "\n",
    "    - For example, in question-answering, the model takes a question and a context (such as a paragraph) that may have the answer as an input, and `[SEP]` is used in between the question and the context.<br></br>\n",
    "    \n",
    "- Also, `[PAD]` token is used to pad short sequences to a required length.\n",
    "\n",
    "* **\n",
    "\n",
    "The `[CLS]` token is appended to any i/p seq. fed to BERT. This denotes the beginning of the input. It also forms the basis for the i/p fed into the classification head used on top of BERT to solve your NLP task. As you know, BERT produces a hidden representation for each input token in the sequence. As a convention, the hidden representation corresponding to the `[CLS]` token is used as the input to the classification model that sits on top of BERT.\n",
    "\n",
    "* **\n",
    "\n",
    "- *Next, the final embedding of the tokens is generated using 3 diff. embedding space.* \n",
    "    1. The **token embedding** has unique vector for each token in the vocab.\n",
    "    \n",
    "    2. The **positional embedding** encode the position of each token.\n",
    "    \n",
    "    3. Finally, the **segment embedding** provides a distinct rep. for each sub-component in the i/p, when the i/p consists of multiple components.\n",
    "        - *For example, in question-answering, the question will have a unique vector as its segment embedding vector and the context will have a different embedding vector.*\n",
    "        \n",
    "        - *This is done by having $n$ embedding vectors for the $n$ different components in the i/p sequence. Depending on the component index specified for each token in the i/p, the corresponding segment embedding vector is retrieved.* \n",
    "        \n",
    "        - *$n$ needs to be specified in advance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab0867-b194-43ed-acbd-c9afa6d22a52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tasks solved by BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df338b7d-3dac-4518-8782-4b6485797405",
   "metadata": {},
   "source": [
    "The task-specific NLP tasks solved by BERT can be classified into 4 diff. categories:\n",
    "\n",
    "- $\\textit{Sequence Classification}$\n",
    "    - *Here, a single input sequence is given and the model is asked to predict a label for the whole sequence (for example, sentiment analysis or spam identification).*<br></br>\n",
    "\n",
    "- $\\textit{Token Classification}$\n",
    "    - *Here, a single input sequence is given and the model is asked to predict a label for each token in the sequence (for example, named entity recognition or part-of-speech tagging).*<br></br>\n",
    "\n",
    "- $\\textit{Question-Answering}$\n",
    "    - *Here, the input consists of two sequences: a question and a context. The question and the context are separated by a `[SEP]` token. The model is trained to predict the starting and ending indices of the span of tokens belonging to the answer.*<br></br>\n",
    "\n",
    "- $\\textit{Multiple Choice}$\n",
    "    - *Here, the input consists of multiple sequences; a question followed by multiple candidates that may or may not be the answer to the question. These multiple sequences are separated by the token `[SEP]` and provided as a single input sequence to the model. The model is trained to predict the correct answer (that is, the class label) for that question.*\n",
    "\n",
    "* **\n",
    "\n",
    "- In tasks that involve multiple sequences (such as multiple-choice questions), you need the model to tell different inputs belonging to different segments apart (i.e., which tokens are the question and which tokens are the context in a question-answering task). \n",
    "\n",
    "- In order to make that distinction, the `[SEP]` token is used. A `[SEP]` token is inserted between the different sequences. \n",
    "\n",
    "- For example, if you are solving a question-answering problem, you might have the following input:\n",
    "\n",
    "    - Question: What color is the ball?<br>Paragraph: Tippy is a dog. She loves to play with her red ball.\n",
    "\n",
    "    - Then the input to BERT might look like this:<br>`[CLS] What color is the ball [SEP] Tippy is a dog She loves to play with her red ball [SEP]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f936b7-0492-4b1a-a31d-45b93d0de715",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/bert_tasks.png' title='NLP w/ TensorFlow by Thushan Ganegedara'/>\n",
    "</div>\n",
    "\n",
    "**BERT is designed in such a way that it can be used to complete these tasks without any modifications to the base model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b065c32-eb84-40a5-ae2d-dcb4ac4b7c1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Key Points on BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c080e00-eb20-4245-8c1b-e3f20e164be2",
   "metadata": {},
   "source": [
    "Now that we have discussed all the elements of BERT so we can use it successfully to solve a downstream NLP task, let’s reiterate the key points about BERT:\n",
    "\n",
    "- *BERT is an encoder-based Transformer*\n",
    "\n",
    "- *BERT outputs a hidden representation for every token in the input sequence*\n",
    "\n",
    "- *BERT has 3 embedding spaces: token embedding, positional embedding, and segment embedding*\n",
    "\n",
    "- *BERT uses a special token `[CLS]` to denote the beginning of an input and is used as the input to a downstream classification model*\n",
    "\n",
    "- *BERT is designed to solve four types of NLP tasks: sequence classification, token classification, free-text question-answering, and multiple-choice question-answering*\n",
    "\n",
    "- *BERT uses the special token `[SEP]` to separate between sequence A and sequence B*\n",
    "\n",
    "\n",
    "The power within BERT doesn’t just lie within its structure. BERT is pre-trained on a large corpus of text using a few different pre-training techniques. In other words, BERT already comes with a solid understanding of the language, making downstream NLP tasks easier to solve. \n",
    "\n",
    "Next, let’s discuss how BERT is pre-trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5da964-d5ee-4a41-b5a4-c77b7d1036b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How BERT is pre-trained\n",
    "\n",
    "The real value of BERT comes from the fact that it has been pre-trained on a large corpus of data in a self-supervised fashion. In the pre-training stage, BERT is trained on 2 diff. tasks:\n",
    "\n",
    "- $\\text{Masked Language Modeling (MLM)}$\n",
    "- $\\text{Next Sentence Prediction (NSP)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc1327-c067-4478-86e6-f195e875e1d7",
   "metadata": {},
   "source": [
    "### **Masked Language Modeling (MLM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887076af-1115-4c03-8408-7aacfd47a361",
   "metadata": {},
   "source": [
    "- The MLM task is inspired by the Cloze test, where a student is given a sentence with one or more blanks and is asked to fill the blanks.\n",
    "\n",
    "- Similarly, given a text corpus, words are masked from sentences and then the model is asked to predict the masked tokens.\n",
    "\n",
    "- For e.g.:<br>*My mental health is declinging*<br>might become:<br>*My mental `[MASK]` is declinging*\n",
    "\n",
    "    - BERT uses a special token, `[MASK]`, to represent masked words. \n",
    "\n",
    "    - Then the target for the model will be the word *health*.\n",
    "\n",
    "- **But this introduces a practical issue to the model:**\n",
    "    - The special `[MASK]` token does not appear in the actual text. \n",
    "    \n",
    "    - This means that the text the model will see during the finetuning phase (i.e, when training on a classification problem) will be different to what it will see during pre-training. \n",
    "\n",
    "    - **This is sometimes referred to as the pre-training-finetuning discrepancy.**\n",
    "\n",
    "- *Therefore, the authors of BERT suggest the foll. approach to cope with the issue.<br>When masking a word, do one of the following:*\n",
    "    - Use the `[MASK]` token as it is (with $80\\%$ probability)\n",
    "    - Use a random word (with $10\\%$ probability)\n",
    "    - Use the true word (with $10\\%$ probability\n",
    "\n",
    "- In other words, instead of always seeing `[MASK]`, the model will see actual words on certain occasions, alleviating the discrepancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef1a7b-6895-4c41-b390-de3a6db2ee65",
   "metadata": {},
   "source": [
    "### **Next Sentence Prediction (NSP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e036a-880f-482a-aa12-7b137b09545b",
   "metadata": {},
   "source": [
    "- In the NSP task, the model is given a pair of sentences, A and B (in that order), and is asked to predict whether the B is the next sentence after A. \n",
    "\n",
    "- This can be done by fitting a binary classifier onto BERT and training the whole model from end to end on selected pairs of sentences. \n",
    "\n",
    "- Generating pairs of sentences as inputs for the model is not hard and can be done in an unsupervised manner:\n",
    "    - A sample with the label TRUE is generated by picking two sentences that are adjacent to each other\n",
    "    \n",
    "    - A sample with the label FALSE is generated by picking two sentences randomly that are not adjacent to each other\n",
    "\n",
    "Following this approach, a labeled dataset is generated for the next sentence prediction task. Then BERT, along with the binary classifier, is trained from end to end using the labeled dataset to solve a downstream task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
