{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b8df5-ad16-48d3-a477-237cc67ab35a",
   "metadata": {},
   "source": [
    "### Summary of Topic Covered..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084bf9-5b50-48f0-b11f-159b3bd49b02",
   "metadata": {},
   "source": [
    "- **Transformer, inner workings of the model:**\n",
    "    - Transformers use self-attention, a powerful technique to attend to other inputs in the text sequences while processing one input. \n",
    "\n",
    "    - Also Transformers use positional embeddings to inform the model about the relative position of tokens in addition to token embeddings. \n",
    "    \n",
    "    - Transformers also leverage residual connections(i.e., shortcut connections) and layer normalization in order to improve model training. \n",
    "    \n",
    "* **\n",
    "\n",
    "- **BERT**\n",
    "    - BERT, an encoder-based Transformer model. \n",
    "    \n",
    "    - BERT accepts i/p data in a particular format and the special tokens it uses in the input like `CLS`,`SEP`, `MASK`, etc. \n",
    "    \n",
    "    - BERT can solve 4 diff. types of task: *sequence classification, token classification, multiple-choice, and question-answering*\n",
    "\n",
    "    - BERT is pre-trained on a large corpus of text.\n",
    "    \n",
    "    - In the pre-training stage, BERT is trained on 2 diff. tasks: Masked Language Modeling (MLM) & Next Sentence Prediction(NSP).\n",
    "    \n",
    "* **\n",
    "\n",
    "- **Use Case: QnA with BERT**\n",
    "    - To implement this, got introduced to `transformers` library by Hugging Face.\n",
    "    \n",
    "    - Dataset used SQuAD-v2.0, in v2.0 of this dataset we have question with no answers, that means BERT must be able provide no answers for such question\n",
    "    \n",
    "    - added `answer_end` indexing in data-preprocessing step\n",
    "    \n",
    "    - Used BertTokenizerFast `BertTokenizerFast.from_pretrained(\"bert-base-uncased\")`\n",
    "    \n",
    "    - After few other preprocessing  steps like dealing with truncated answers, moving from character-level indexing to token-level indexing for answer texts\n",
    "    \n",
    "    - created a data-generator and `tf.data.Dataset` to stream the data in batches. The use of python partial functionality was new for me `from functools import partial`\n",
    "    \n",
    "    - Then used the pre-trained BERT model for QnA `from transformers import BertConfig, TFBertForQuestionAnswering` and wrapped this using keras functional api\n",
    "    \n",
    "    - This was the first time working with multi-output model, where we need to define multiple loss functions; in QnA task, two losses, one to predict the starting index of the answer and the other to predict the end.\n",
    "    \n",
    "    - Ran for 2 epchos, took ~6 hours to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
