{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b154a8f5-635c-48e0-a1fc-b880f3b67aeb",
   "metadata": {},
   "source": [
    "# Use case: Using BERT to answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a871cb-6f60-4fe4-bcef-c9f603cf4a93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Introduction to the Hugging Face `transformers` library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d2a97-797a-4834-9f3a-716e1297bb88",
   "metadata": {},
   "source": [
    "Few points on Hugging Face `transformers` library by author:<br>\n",
    "(Source: NLP w/ TensorFlow by by Thushan Ganegedara)\n",
    "\n",
    "- The `transformers` library is a high-level API that is built on top of TensorFlow, PyTorch, and JAX. \n",
    "\n",
    "- It provides easy access to pre-trained Transformer models that can be downloaded and fine-tuned with ease. You can find models in the Hugging Face‚Äôs model registry at https://huggingface.co/models. \n",
    "\n",
    "- You can filter models by task, examine the underlying deep learning frameworks, and more.\n",
    "\n",
    "- The transformers library was designed with the aim of providing a very low barrier for entry to using complex Transformer models. \n",
    "\n",
    "* **\n",
    "\n",
    "- For this reason, there‚Äôs only a handful of concepts that you need to learn in order to hit the ground running with the library. **Three important classes are required to load and use a model successfully:**\n",
    "    - **Model class** (such as `TFBertModel`) - *Contains the trained weights of the model in the form of `tf.keras.models.Model` or the PyTorch equivalent.*\n",
    "    \n",
    "    - **Configuration** (such as `BertConfig`) - *Stores various parameters and hyperparameters needed to load the model. If you‚Äôre using the pre-trained model as is, you don‚Äôt need to explicitly define its configuration.*\n",
    "    \n",
    "    - **Tokenizer** (such as `BertTokenizerFast`) - *Contains the vocabulary and token-to-ID mapping needed to tokenize the words for the model.*\n",
    "    \n",
    "    \n",
    "- All of these classes can be used with two straightforward functions:\n",
    "    - `from_pretrained()` ‚Äì Provides a way to instantiate a model/configuration/tokenizer available from the model repository or locally\n",
    "    \n",
    "    - `save_pretrained()` ‚Äì Provides a way to save the model/configuration/tokenizer so that it can be reloaded later\n",
    "    \n",
    "* **\n",
    "\n",
    "> üóùÔ∏è**Resource:** *Official TensorFlow text processing tutorials, includes:* `Text generation, Text classification, NLP with BERT, Embeddings` -> üîó[**Link**](https://www.tensorflow.org/text/tutorials/)\n",
    "\n",
    "* **\n",
    "\n",
    "It is also important to note the side-effects of having such an easy-to-grasp interface for using models. Due to serving the very specific purpose of providing a way to use Transformer models built with TensorFlow, PyTorch, or Jax, you don‚Äôt have the modularity or flexibility found in TensorFlow, for example. In other words, you cannot use the transformers library in the same way you would use TensorFlow to build a tf.keras.models.Model using tf.keras.layers.Layer objects.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c25c7-9b93-4a0d-a05f-192f5892ff07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3246af81-0b52-4f46-9527-438fb39a5633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertConfig, TFDistilBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "    try:\n",
    "        transformers.trainer_utils.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: transformers module is not imported. Setting the seed for transformers failed.\")\n",
    "        \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "set_random_seed(random_seed)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print(\"No GPU found!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11213d55-cee6-4008-b139-ea7035873e33",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75ca6d-dfbf-4aff-acde-05b54dde5c4c",
   "metadata": {},
   "source": [
    "- [10 Question-Answering Datasets To Build Robust Chatbot Systems](https://analyticsindiamag.com/10-question-answering-datasets-to-build-robust-chatbot-systems/)\n",
    "\n",
    "- [Hugging Face Tutorial on SQuAD_v2](https://huggingface.co/transformers/v3.3.1/custom_datasets.html#question-answering-with-squad-2-0)\n",
    "\n",
    "- SQuAD2.0 dataset constains questions with no answers\n",
    "\n",
    "- SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.\n",
    "\n",
    "- [Official SQuAD2.0 Dataset Website](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "\n",
    ">**For now I'm removing questions with no answers; as I can't figure out what to do while adding `answer_end` index for empty answers. I replaced `answer_start` & `answer_end` with NaN values, also `answer_text` with empty string. But was getting float error in `Dealing with truncated answers` section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e4432-f999-4346-99f6-3e6650be6c2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### download & read from url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c26be5-1807-4bae-afe6-42d2be7b0c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shortcut way to download\n",
    "\n",
    "# !mkdir squad\n",
    "# !curl https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -o squad/train-v2.0.json\n",
    "# !curl https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -o squad/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eadd4b0b-451d-4d3b-b11a-047a458c0595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "squad_v2/train-v2.0.json: 40.2MB [00:01, 37.2MB/s]                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "squad_v2/dev-v2.0.json: 4.17MB [00:00, 14.6MB/s]                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev dataset downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create the \"squad_v2\" folder if it doesn't exist\n",
    "if not os.path.exists(\"squad_v2\"):\n",
    "    os.makedirs(\"squad_v2\")\n",
    "\n",
    "# URLs of the SQuAD dataset\n",
    "train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "dev_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "\n",
    "# Function to download a file with a progress bar\n",
    "def download_file(url, local_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "    \n",
    "    with open(local_path, \"wb\") as file, tqdm(\n",
    "        desc=local_path,\n",
    "        total=file_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            file.write(data)\n",
    "            bar.update(len(data))\n",
    "\n",
    "# Check if the train dataset is already downloaded\n",
    "train_file_path = \"squad_v2/train-v2.0.json\"\n",
    "if not os.path.exists(train_file_path):\n",
    "    download_file(train_url, train_file_path)\n",
    "    print(\"Train dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Train dataset is already downloaded.\")\n",
    "\n",
    "# Check if the dev dataset is already downloaded\n",
    "dev_file_path = \"squad_v2/dev-v2.0.json\"\n",
    "if not os.path.exists(dev_file_path):\n",
    "    download_file(dev_url, dev_file_path)\n",
    "    print(\"Dev dataset downloaded successfully.\")\n",
    "else:\n",
    "    print(\"Dev dataset is already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd4bb01-5202-4cfb-b566-c6a0b4910939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return contexts, questions, answers\n",
    "\n",
    "train_contexts, train_questions, train_answers = read_squad('squad_v2/train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers = read_squad('squad_v2/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68fcf94-e158-4ed5-8da5-85172241185c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39aa0f30-5566-410a-aa48-14383af76db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "# dataset = load_dataset(\"squad\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b9028-303b-4863-a22a-d378a86856f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Print the first 5 samples in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3915e928-4a3f-4796-a4ee-83967ad366c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "When did Beyonce start becoming popular?\n",
      "Answer:\n",
      "{'text': ['in the late 1990s'], 'answer_start': [269]}\n",
      "\n",
      "Question:\n",
      "What areas did Beyonce compete in when she was growing up?\n",
      "Answer:\n",
      "{'text': ['singing and dancing'], 'answer_start': [207]}\n",
      "\n",
      "Question:\n",
      "When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer:\n",
      "{'text': ['2003'], 'answer_start': [526]}\n",
      "\n",
      "Question:\n",
      "In what city and state did Beyonce  grow up? \n",
      "Answer:\n",
      "{'text': ['Houston, Texas'], 'answer_start': [166]}\n",
      "\n",
      "Question:\n",
      "In which decade did Beyonce become famous?\n",
      "Answer:\n",
      "{'text': ['late 1990s'], 'answer_start': [276]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 samples in the training set\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "for q, a in zip(train_data['question'][:5], train_data['answers'][:5]):\n",
    "    print(f\"Question:\\n{q}\\nAnswer:\\n{a}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cdebc-f722-420d-9c0e-dd1201cdacbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Here, `answer_start` indicates the character index at which this answer starts in the context provided.\n",
    "\n",
    "- **When training the model, we will be asking the model to predict the start and end indices of the answer.** \n",
    "\n",
    "- In its original form, only the `answer_start` is present. We will need to manually add `answer_end` to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5f833-2f30-4ec4-8119-ff0f62d46705",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Adding `answer_end` index\n",
    "\n",
    "The answers are provided by means of the, starting index (`answer_start`) and the answer it self (`text`). We will add `answer_end`, which will denote the index of the position the answer ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a114a39-de3e-4490-8a2a-3ae34b97841e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_end_index(answers, contexts):\n",
    "    \"\"\"Add end index to answers\"\"\"\n",
    "    \n",
    "    modified_answers = []\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        # we have some questions with no answers\n",
    "        if len(answer['text']) == 0 or len(answer['answer_start'])==0:\n",
    "            answer['text'] = ''\n",
    "            answer['answer_start'] = np.NaN\n",
    "            answer['answer_end'] = np.NaN\n",
    "        else:\n",
    "            # here we are replacing the list with just the element\n",
    "            gold_text = answer['text'][0]\n",
    "            answer['text'] = gold_text\n",
    "\n",
    "            start_idx = answer['answer_start'][0]\n",
    "            answer['answer_start'] = start_idx\n",
    "\n",
    "            # Make sure the starting index is valid and there is an answer\n",
    "            assert start_idx >=0 and len(gold_text.strip()) > 0\n",
    "\n",
    "            end_idx = start_idx + len(gold_text)\n",
    "            answer['answer_end'] = end_idx\n",
    "\n",
    "            # Make sure the corresponding context matches the actual answer\n",
    "            assert context[start_idx:end_idx] == gold_text\n",
    "        \n",
    "        modified_answers.append(answer)\n",
    "    \n",
    "    return modified_answers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41be12ef-96eb-4468-a501-3b64e4ca6cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data corrections\n",
      "Validation data correction\n"
     ]
    }
   ],
   "source": [
    "train_questions = dataset[\"train\"][\"question\"]\n",
    "\n",
    "print(\"Training data corrections\")\n",
    "\n",
    "train_answers, train_contexts = compute_end_index(dataset[\"train\"][\"answers\"], \n",
    "                                                  dataset[\"train\"][\"context\"])\n",
    "\n",
    "#################\n",
    "\n",
    "test_questions = dataset[\"validation\"][\"question\"]\n",
    "\n",
    "print(\"Validation data correction\")\n",
    "\n",
    "test_answers, test_contexts = compute_end_index(dataset[\"validation\"][\"answers\"], \n",
    "                                                dataset[\"validation\"][\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21789f2c-1ea3-4f89-877d-1322a1c6af89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def find_empty_ans_indices(answers):\n",
    "#     return [idx for idx, answer in enumerate(answers) if answer['text'] == '']\n",
    "\n",
    "# def remove_empty_answers(arr, empty_ans_idx, arr_name):\n",
    "#     print(f\"Removed {len(empty_ans_idx)} data points with no answer from {arr_name}\")\n",
    "#     return np.delete(arr, empty_ans_idx)\n",
    "\n",
    "# train_empty_ans_idx = find_empty_ans_indices(train_answers)\n",
    "# test_empty_ans_idx = find_empty_ans_indices(test_answers)\n",
    "\n",
    "# print(\"No. of questions with no answers in SQuAD_v2 dataset:\")\n",
    "# print(f\"\\tTrain: {len(train_empty_ans_idx)}/{len(train_answers)}\")\n",
    "# print(f\"\\tTest: {len(test_empty_ans_idx)}/{len(test_answers)}\\n\")\n",
    "\n",
    "# train_questions = remove_empty_answers(train_questions, train_empty_ans_idx, \"train_questions\")\n",
    "# train_answers = remove_empty_answers(train_answers, train_empty_ans_idx, \"train_answers\")\n",
    "# train_contexts = remove_empty_answers(train_contexts, train_empty_ans_idx, \"train_contexts\")\n",
    "\n",
    "# print()\n",
    "\n",
    "# test_questions = remove_empty_answers(test_questions, test_empty_ans_idx, \"test_questions\")\n",
    "# test_answers = remove_empty_answers(test_answers, test_empty_ans_idx, \"test_answers\")\n",
    "# test_contexts = remove_empty_answers(test_contexts, test_empty_ans_idx, \"test_contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429fb58-cb70-4fb6-a1db-c36992a2c139",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Implementing BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2a9d9-2ef9-40a9-9d41-7f21da31b77a",
   "metadata": {},
   "source": [
    "To use a pre-trained Transformer Model from Hugging Face, we need 3 components:\n",
    "\n",
    "- `Tokenizer` - Responsible for splitting a long bit of text (such as a sentence) into smaller tokens\n",
    "\n",
    "- `config` ‚Äì Contains the configuration of the model\n",
    "\n",
    "- `Model` ‚Äì Takes in the tokens, looks up the embeddings, and produces the final output(s) using the provided inputs\n",
    "\n",
    "\n",
    "We can ignore the config as we are using the pre-trained model as is. However, to paint a full picture, we will use the configuration nevertheless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c512c-72b6-4708-8db5-017e0d7ec4de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Implementing Tokenizer & Visualizing the token_mappings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d485e-71ed-4dc4-bae7-ce8fde2cf602",
   "metadata": {},
   "source": [
    "We will be using a Tokenizer called `bert-base-uncased`. It is the Tokenizer developed for the BERT base model and is uncased (that is, there's no distinction between uppercase and lowercase characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c82b8bb-5065-43e8-8127-1277d2c9d1a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download/define the tokenizer\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae66f9a-3d70-47a6-9bd9-9c15c2856c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[ 101, 2023, 2003, 1996, 6123,  102, 2023, 2003, 1996, 3160,  102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer in action\n",
    "\n",
    "context = \"This is the context\"\n",
    "question = \"This is the question\"\n",
    "\n",
    "token_ids = tokenizer(text=context, text_pair=question,\n",
    "                      padding=False, return_tensors='tf')\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48918fa9-d52f-453a-a213-0e3e1df3665f",
   "metadata": {},
   "source": [
    "Let‚Äôs understand the arguments provided to the tokenizer‚Äôs call:\n",
    "\n",
    "- `text` ‚Äì A single or batch of text sequences to be encoded by the tokenizer. Each text sequence is a string.\n",
    "\n",
    "- `text_pair` ‚Äì An optional single or batch of text sequences to be encoded by the tokenizer. \n",
    "    - *It‚Äôs useful in situations where the model takes a multi-part input (such as a question and a context in question-answering).*<br></br>\n",
    "\n",
    "- `padding` ‚Äì Indicates the padding strategy. \n",
    "    - If set to `True`, it will be padded to the maximum sequence length in the dataset. \n",
    "    \n",
    "    - If set to `max_length`, it will be padded to the length specified by the `max_length` argument. \n",
    "    \n",
    "    - If set to `False`, no padding will be done.\n",
    "\n",
    "- `return_tensors` ‚Äì An argument that defines the type of tensors returned. It could be either `pt` (PyTorch) or `tf` (TensorFlow). Since we want TensorFlow tensors, we define it as `'tf'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b9f415-a093-479e-a265-bbab2485c2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed700ff0-88de-4995-8958-a4fc22bf189e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids -\n",
      "[ 101 2023 2003 1996 6123  102 2023 2003 1996 3160  102]\n",
      "\n",
      "token_type_ids -\n",
      "[0 0 0 0 0 0 1 1 1 1 1]\n",
      "\n",
      "attention_mask -\n",
      "[1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in token_ids.items():\n",
    "    print(key, \"-\")\n",
    "    print(value.numpy()[0], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b60af9-8e47-4091-95ab-3f30d1a75062",
   "metadata": {},
   "source": [
    "This outputs a `transformers.tokenization_utils_base.BatchEncoding` object, **which is essentially a dictionary. It has three keys and tensors as values:**\n",
    "\n",
    "- `input_ids` ‚Äì \n",
    "    - Provides the IDs of the tokens for the tokens found in the text sequences. \n",
    "    \n",
    "    - Additionally, it introduces the `[CLS]` token ID at the beginning of the sequence and \n",
    "    \n",
    "    - two instances of the `[SEP]` token ID, one between the question and context, and the other one at the end.\n",
    "\n",
    "- `token_type_ids` ‚Äì This is the segment ID we use for the segment embedding.\n",
    "\n",
    "- `attention_mask` ‚Äì \n",
    "    - The attention mask represents the words that are allowed to be attended to during the forward pass. \n",
    "    \n",
    "    - Since BERT is an encoder model, any token can pay attention to any other token. The only exception is the padded tokens that will be ignored during the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a096bd32-4df6-43d6-849f-4adb1bd3020b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids:\n",
      "\t [ 101 2023 2003 1996 6123  102 2023 2003 1996 3160  102]\n",
      "\n",
      "mappings:\n",
      "\t ['[CLS]', 'this', 'is', 'the', 'context', '[SEP]', 'this', 'is', 'the', 'question', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# We could also convert these token IDs to actual tokens to know what they represent.\n",
    "print(\"token_ids:\")\n",
    "print(\"\\t\", token_ids['input_ids'].numpy()[0])\n",
    "\n",
    "print(\"\\nmappings:\")\n",
    "print(\"\\t\", tokenizer.convert_ids_to_tokens(token_ids['input_ids'].numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c27728-42d5-4094-877c-b70c122c9aa8",
   "metadata": {},
   "source": [
    ">üóùÔ∏è **We can see how the tokenizer inserts special tokens like `[CLS]` and `[SEP]` into the text sequence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c26405-fe22-4b3e-ad40-147695d2a317",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoding the train & test data using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276fac05-de91-4b72-8d3f-aaacb20d5770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_encodings.shape: (130319, 512)\n",
      "test_encodings.shape: (11873, 512)\n"
     ]
    }
   ],
   "source": [
    "# Encode train data\n",
    "train_encodings = tokenizer(train_contexts, train_questions,\n",
    "                            truncation=True, padding=True, \n",
    "                            return_tensors='tf')\n",
    "\n",
    "print(f\"train_encodings.shape: {train_encodings['input_ids'].shape}\")\n",
    "\n",
    "\n",
    "# Encode test data\n",
    "test_encodings = tokenizer(test_contexts, test_questions,\n",
    "                           truncation=True, padding=True,\n",
    "                           return_tensors='tf')\n",
    "\n",
    "print(f\"test_encodings.shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f52ab0-4a5a-4194-bffd-b0e3b7646a8b",
   "metadata": {},
   "source": [
    "The maximum sequence length in our dataset is 512. Therefore, we see that the maximum length of the sequences is 512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f931192-92d6-4c0e-a0c7-877d050d89c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dealing with truncated answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebae560-0231-40ce-b1c2-2581e1f95535",
   "metadata": {},
   "source": [
    "Once we tokenize our data, we need to perform one more data processing step. \n",
    "\n",
    "\n",
    "- In the original dataset the `answer_start` and `answer_end` denote the `character-level position` of the answer. But in the model, since we deal in tokens we need the` token-level position` of the answer. \n",
    "\n",
    "- For that, we will use the `char_to_token` function in the `tokenizer`. It will convert the character index to a token index.\n",
    "\n",
    "- Because we are enforcing a maximum sequence length of 512, some answers will be inevitably truncated if they are present after the 512th token. Although this is rare, we still need to take care of this as it can result in numerical errors otherwise. Therefore, if the positions are `None` (i.e. couldn't find the answer), it is set to the maximum position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0913d2-1a22-4f63-b437-599c8381558b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def replace_char_with_token_indices(encodings, answers):\n",
    "#     start_positions = []\n",
    "#     end_positions = []\n",
    "#     n_updates = 0\n",
    "    \n",
    "#     # Go through all the answers\n",
    "#     for i in range(len(answers)):\n",
    "#         # if answers[i]['text']=='':\n",
    "#         #     continue\n",
    "        \n",
    "#         # get the token position for both start & end char positions\n",
    "#         start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        \n",
    "#         end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']-1))\n",
    "        \n",
    "        \n",
    "#         if start_positions[-1] is None or end_positions[-1] is None:\n",
    "#             n_updates += 1\n",
    "            \n",
    "#         # if start position is None, the answer passage has been truncated\n",
    "#         # In the guide, https://huggingface.co/transformers/custom_datasets.html#qa-squad\n",
    "#         # they set it to model_max_length, but this will result in NaN losses as the last\n",
    "#         # available label is model_max_length-1 (zero-indexed)\n",
    "        \n",
    "#         if start_positions[-1] is None:\n",
    "#             start_positions[-1] = tokenizer.model_max_length - 1\n",
    "            \n",
    "#         if end_positions[-1] is None:\n",
    "#             end_positions[-1] = tokenizer.model_max_length - 1\n",
    "            \n",
    "#     print(\"{}/{} had answers truncated\".format(n_updates, len(answers)))\n",
    "    \n",
    "#     encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2eec980-3498-4fef-b7a4-21dfff488647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_char_with_token_indices(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    n_updates = 0\n",
    "\n",
    "    # Go through all the answers\n",
    "    for i in range(len(answers)):\n",
    "        if pd.notna(answers[i]['answer_start']) and pd.notna(answers[i]['answer_end']):\n",
    "            # get the token position for both start & end char positions\n",
    "            start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        else:\n",
    "            # Handle the case where the answer is empty or the positions are NaN\n",
    "            start_positions.append(None)\n",
    "            end_positions.append(None)\n",
    "\n",
    "        if start_positions[-1] is None or end_positions[-1] is None:\n",
    "            n_updates += 1\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        # In this case, you can set it to some specific value or leave it as None\n",
    "        # Here, we set it to model_max_length - 1 if it's None\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length - 1\n",
    "\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length - 1\n",
    "\n",
    "    print(\"{}/{} had answers truncated\".format(n_updates, len(answers)))\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0af30d-76cd-4214-9dcb-8e3d756a2cb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "- This function takes in a set of `BatchEncodings` called `encodings` generated by the tokenizer and a set of answers (a list of dictionaries). \n",
    "\n",
    "- Then it updates the provided encodings with two new keys: `start_positions` and `end_positions`. \n",
    "\n",
    "- *These keys respectively hold the token-based indices denoting the start and end of the answer. If the answer is not found, we set the start and end indices to the last token.*\n",
    "\n",
    "- To convert our existing character-based indices to token-based indices, we use a function called `char_to_token()` provided by the `BatchEncodings` class. \n",
    "\n",
    "- It takes a character index as the input and provides the corresponding token index as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e4de6d-2202-4d8a-8b69-61cca61f8aef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43508/130319 had answers truncated\n",
      "5951/11873 had answers truncated\n"
     ]
    }
   ],
   "source": [
    "replace_char_with_token_indices(train_encodings, train_answers)\n",
    "replace_char_with_token_indices(test_encodings, test_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092aa11-2935-48c0-ac83-66fc0fd5810f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining a TensorFlow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9723a-16de-4037-9790-e19752893f75",
   "metadata": {},
   "source": [
    "- Next, let‚Äôs implement a TensorFlow dataset to generate the data for the model. **Our data will consist of two tuples: one containing inputs and the other containing the targets.** \n",
    "\n",
    "- The input tuple contains:\n",
    "    - Input token IDs ‚Äì A batch of padded token IDs of size `[batch size, sequence_len]`\n",
    "    - Attention mask ‚Äì A batch of attention masks of size `[batch size, seq_len]`\n",
    "\n",
    "- The output tuple contains:\n",
    "    - Start index of the answer ‚Äì A batch of start indices of the answer\n",
    "    - End index of the answer ‚Äì A batch of end indices of the answer\n",
    "    \n",
    "    \n",
    "[`from functools import partial` - GFG](https://www.geeksforgeeks.org/partial-functions-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2818ebf-ccc2-4704-a6bf-c2098c705abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train & validation data\n",
      "\tDone\n",
      "Creating test data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "train_batch_size = 4\n",
    "test_batch_size = 8\n",
    "\n",
    "\n",
    "def data_gen(input_ids, attention_mask, start_positions, end_positions):\n",
    "    \"\"\"Generator for data\"\"\"\n",
    "    for inps, attn, start_pos, end_pos in zip(input_ids, attention_mask, \n",
    "                                              start_positions, end_positions):\n",
    "        yield (inps, attn),(start_pos, end_pos)\n",
    "        \n",
    "        \n",
    "print(\"Creating train & validation data\")\n",
    "\n",
    "# Define the generator as a callable (not the generator itself)\n",
    "# We define a partial func. that we can simply call later without passing any arguments:\n",
    "train_data_gen = partial(data_gen,\n",
    "                         input_ids = train_encodings['input_ids'], \n",
    "                         attention_mask = train_encodings['attention_mask'], \n",
    "                         start_positions = train_encodings['start_positions'], \n",
    "                         end_positions = train_encodings['end_positions'])\n",
    "\n",
    "# Define the dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "                    train_data_gen,\n",
    "                    output_types = (('int32', 'int32'),('int32', 'int32'))\n",
    "                )\n",
    "\n",
    "# We then shuffle the data in our training dataset. \n",
    "# When shuffling a TF dataset we need to provide a buffer_size. \n",
    "# buffer_size defines how many samples are chosen to shuffle. Here 1000 samples:\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "\n",
    "# Splitting train data into train-set & validation-set\n",
    "\n",
    "# Valid set is taken as the first 10000 samples in the shuffled set\n",
    "valid_dataset = train_dataset.take(10000)\n",
    "valid_dataset = valid_dataset.batch(train_batch_size)\n",
    "\n",
    "# Rest is kept as the training data\n",
    "train_dataset = train_dataset.skip(10000)\n",
    "train_dataset = train_dataset.batch(train_batch_size)\n",
    "\n",
    "print('\\tDone')\n",
    "\n",
    "# Creating test data\n",
    "print(\"Creating test data\")\n",
    "\n",
    "# Define the generator as a callable\n",
    "test_data_gen = partial(data_gen,\n",
    "                        input_ids=test_encodings['input_ids'], \n",
    "                        attention_mask=test_encodings['attention_mask'],\n",
    "                        start_positions=test_encodings['start_positions'], \n",
    "                        end_positions=test_encodings['end_positions']\n",
    "                       )\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    test_data_gen, output_types=(('int32', 'int32'), ('int32', 'int32'))\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.batch(test_batch_size)\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc8320-e5cb-493f-8b6a-221c5b1c25fb",
   "metadata": {},
   "source": [
    "## BERT for QnA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a39094-bee5-40ad-9c07-622f4b8f1888",
   "metadata": {},
   "source": [
    "**Modifications that we'll introduce on the top of pre-trained BERT Model to leverage it for Question-Answering:**\n",
    "\n",
    "- *First, model takes in a question followed by a context, the context may or may not contain the answer to the queation.*\n",
    "\n",
    "- *i/p format:* `[CLS] <question_token> [SEP] <context_token> [SEP]`\n",
    "\n",
    "- Then, for each token position of the context, we have two classification heads predicting a probability: \n",
    "    - *One head predicts the probability of each context token being the start of the answer, whereas* \n",
    "    - *the other one predicts the probability of each context token being the end of the answer.*<br></br>\n",
    "    \n",
    "- *Once we figure out the start and end indices of the answer, we can simply extract the answer from the context using those indices.*\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/bert_q_a.png' title=\"NLP w/ TensorFlow by Thushan Ganegedara\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5e76a-e16c-4567-a468-38363429c0dd",
   "metadata": {},
   "source": [
    "### Defining the Config & the Model\n",
    "\n",
    "Here we define a **DistilBert model** (particularly a TF variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f975cf6-dd5e-4815-b7e2-bae5f3e06e1b",
   "metadata": {},
   "source": [
    "In Hugging Face, we have several variants of each Transformer model. These variants are based on different tasks solved by these models. \n",
    "\n",
    "For example, for BERT we have:\n",
    "- `TFBertForPretraining` ‚Äì The pre-trained model without a task-specific head\n",
    "- `TFBertForSequenceClassification` ‚Äì Used for classifying a sequence of text\n",
    "- `TFBertForTokenClassification` ‚Äì Used for classifying each token in the sequence of text\n",
    "- `TFBertForMultipleChoice` ‚Äì Used for answering multiple-choice questions\n",
    "- `TFBertForQuestionAnswering` ‚Äì Used for extracting answers to a question from a given context\n",
    "- `TFBertForMaskedLM` ‚Äì Used for pre-training BERT on the masked language modeling task\n",
    "- `TFBertForNextSentencePrediction` ‚Äì Used for pre-training BERT to predict the next sentence\n",
    "\n",
    "we are interested in `TFBertForQuestionAnswering`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03cc2c7c-cb79-45df-8da2-0a45f1277739",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"return_dict\": false,\n",
      "  \"transformers_version\": \"4.33.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e006b57329648588c690dd24a642001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertForQuestionAnswering\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", return_dict=False)\n",
    "print(config, end='\\n\\n')\n",
    "\n",
    "model = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413e770-da04-42d0-9cc1-834939c187a5",
   "metadata": {},
   "source": [
    ">üóùÔ∏è**Above warning:** *This is expected and totally fine. It‚Äôs saying that there are some layers that have not been initialized from the pre-trained model; the output heads of the model need to be introduced as new layers, thus they are not pre-initialized.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0cbee-8056-4c69-9a79-a5e680e0e238",
   "metadata": {},
   "source": [
    "After that, we will define a function that will wrap the returned model as a `tf.keras.models.Model object`. We need to perform this step because if we try to use the model as it is, TensorFlow returns the following error: \n",
    "```\n",
    "TypeError: The two structures don't have the same sequence type. Input structure has type <class 'tuple'>, while shallow structure has type \n",
    "<class 'transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput'>.\n",
    "```\n",
    "\n",
    "Therefore, we will define 2 i/p layers: one takes in the i/p token IDs(`input_ids`) and the other takes the `attention_mask` and passes it to the model. Finally, we get the output of the model. We then define a `tf.keras.models.Model` using these inputs and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b95f1d32-f90f-47b9-a34f-30c7f39b0cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, None) dtype=int32 (created by layer 'input_ids')>,\n",
       " <KerasTensor: shape=(None, None) dtype=int32 (created by layer 'attention_mask')>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc4d018b-3d15-4dc9-b310-76ba1254f1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_wrap_model(model):\n",
    "    \"\"\"Wraps the huggingface's model with in the Keras Functional API\"\"\"\n",
    "    \n",
    "    # If this is not wrapped in a keras model by taking the correct tensors from\n",
    "    # TFQuestionAnsweringModelOutput produced, you will get the following error\n",
    "    # setting return_dict did not seem to work as it should\n",
    "    \n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.layers.Input([None,], dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    # Define the output (TFQuestionAnsweringModelOutput)\n",
    "    # out = model([input_ids, attention_mask])\n",
    "    start_logits, end_logits = model([input_ids, attention_mask])\n",
    "    \n",
    "    # Get the correct attributes in the produced object to generate an output tuple\n",
    "    wrap_model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n",
    "                                       # outputs=(out.start_logits, out.end_logits)\n",
    "                                       outputs=[start_logits, end_logits]\n",
    "                                      )\n",
    "    return wrap_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6f8e1-3741-4907-b5a4-58b852c06629",
   "metadata": {},
   "source": [
    "- *As we learned when studying the structure of the model, the question-answering BERT has two heads:* \n",
    "    - *one to predict the starting index of the answer and*\n",
    "    - *the other to predict the end.* \n",
    "    \n",
    "- **Therefore, we have to optimize 2 losses coming from the two heads. This means we need to add the two losses to get the final loss. When we have a multi-output model such as this, we can pass multiple loss functions aimed at each output head.** \n",
    "\n",
    "- *Here, we define a single loss function. This means the same loss will be used across both heads and will be summed to generate the final loss:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93d3248-cc4d-4b28-b2cc-33f2a5622ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define and compile the model\n",
    "\n",
    "# Keras will assign a separate loss for each output and add them together. \n",
    "\n",
    "# So we'll just use the standard CE loss instead of using the built-in \n",
    "# model.compute_loss, which expects a dict of outputs and averages the two terms.\n",
    "\n",
    "# Note that this means the loss will be 2x of when using TFTrainer \n",
    "# since we're adding instead of averaging them.\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "model_v2 = tf_wrap_model(model)\n",
    "model_v2.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eec2a70a-64b6-46ca-af79-c305c038e756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_for_question_answering  ((None, None),      108893186   ['input_ids[0][0]',              \n",
      "  (TFBertForQuestionAnswering)   (None, None))                    'attention_mask[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,893,186\n",
      "Trainable params: 108,893,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19a0f415-cd4c-45f0-80a2-fcda1b57cd88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108891648 \n",
      "                                                                 \n",
      " qa_outputs (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,893,186\n",
      "Trainable params: 108,893,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c19f13-5d20-4ae9-90a0-57586873612b",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2148bdbb-513d-477e-be21-f8e43d15a0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "30080/30080 [==============================] - 9669s 321ms/step - loss: 2.6537 - tf_bert_for_question_answering_loss: 1.3695 - tf_bert_for_question_answering_1_loss: 1.2842 - tf_bert_for_question_answering_sparse_categorical_accuracy: 0.5884 - tf_bert_for_question_answering_1_sparse_categorical_accuracy: 0.6043 - val_loss: 2.7652 - val_tf_bert_for_question_answering_loss: 1.3812 - val_tf_bert_for_question_answering_1_loss: 1.3840 - val_tf_bert_for_question_answering_sparse_categorical_accuracy: 0.6189 - val_tf_bert_for_question_answering_1_sparse_categorical_accuracy: 0.6438\n",
      "Epoch 2/2\n",
      "30080/30080 [==============================] - 10900s 362ms/step - loss: 1.6820 - tf_bert_for_question_answering_loss: 0.8764 - tf_bert_for_question_answering_1_loss: 0.8056 - tf_bert_for_question_answering_sparse_categorical_accuracy: 0.7144 - tf_bert_for_question_answering_1_sparse_categorical_accuracy: 0.7371 - val_loss: 2.6233 - val_tf_bert_for_question_answering_loss: 1.3253 - val_tf_bert_for_question_answering_1_loss: 1.2980 - val_tf_bert_for_question_answering_sparse_categorical_accuracy: 0.6722 - val_tf_bert_for_question_answering_1_sparse_categorical_accuracy: 0.6941\n",
      "\n",
      "Training Time: 342.82 mins\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model_v2.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=2\n",
    ")\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"\\nTraining Time: {((t2 - t1) / 60):.2f} mins\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3cb767-28e5-4382-9688-6232bfd3b0e4",
   "metadata": {},
   "source": [
    ">Validation accuracy between ~67% and ~70%. This is quite high, given we only trained the model for 2 epochs. This performance can be attributed to the high level of language understanding the pre-trained model already had when we downloaded it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9527fd-ed25-45e0-9462-64243205d457",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77481ef9-725c-4337-8c5b-a33c36160e89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_for_question_answering  ((None, None),      108893186   ['input_ids[0][0]',              \n",
      "  (TFBertForQuestionAnswering)   (None, None))                    'attention_mask[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,893,186\n",
      "Trainable params: 108,893,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_v2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebaa5cf1-adb1-4a19-ab2b-a726feaa53fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers\\\\bert_qa\\\\tokenizer_config.json',\n",
       " 'tokenizers\\\\bert_qa\\\\special_tokens_map.json',\n",
       " 'tokenizers\\\\bert_qa\\\\vocab.txt',\n",
       " 'tokenizers\\\\bert_qa\\\\added_tokens.json',\n",
       " 'tokenizers\\\\bert_qa\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create folders\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "if not os.path.exists('tokenizers'):\n",
    "    os.makedirs('tokenizers')\n",
    "    \n",
    "\n",
    "# Save the model\n",
    "model_v2.get_layer(\"tf_bert_for_question_answering\").save_pretrained(os.path.join('models', 'bert_qa'))\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(os.path.join('tokenizers', 'bert_qa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce89af-8d8a-4905-a324-95513713cc5b",
   "metadata": {},
   "source": [
    "## Testing on unseen data\n",
    "\n",
    "$\\sim 60\\% accuracy$ on test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9635d543-2034-42e6-8fb3-4e60ff8978fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1485/1485 [==============================] - 281s 189ms/step - loss: 3.9820 - tf_bert_for_question_answering_loss: 1.9975 - tf_bert_for_question_answering_1_loss: 1.9844 - tf_bert_for_question_answering_sparse_categorical_accuracy: 0.5904 - tf_bert_for_question_answering_1_sparse_categorical_accuracy: 0.5986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.9819936752319336,\n",
       " 1.9975428581237793,\n",
       " 1.9844495058059692,\n",
       " 0.5904152393341064,\n",
       " 0.5985850095748901]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b90a78-9677-46c4-9b70-cb50bec2f554",
   "metadata": {},
   "source": [
    "## Ask BERT a question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8897816e-1fb2-47c4-8fc4-e891ea91b2df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "\t Who did King Charles III swear fealty to? \n",
      "\n",
      "Context\n",
      "\t Who did King Charles III swear fealty to? \n",
      "\n",
      "Answer (char indexed)\n",
      "\t {'text': '', 'answer_start': nan, 'answer_end': nan} \n",
      "\n",
      "================================================== \n",
      "\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "73-3 token ids contain the answer\n",
      "Answer (predicted)\n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 7\n",
    "\n",
    "sample_q = test_questions[i]\n",
    "sample_c = test_contexts[i]\n",
    "sample_a = test_answers[i]\n",
    "\n",
    "# Get the input in the format BERT accepts:\n",
    "# The input to the model needs to have a batch dimension. \n",
    "# Therefore we use the [i:i+1] syntax to make sure the batch dimension is not flattened:\n",
    "\n",
    "sample_input = (test_encodings[\"input_ids\"][i:i+1], \n",
    "                test_encodings[\"attention_mask\"][i:i+1])\n",
    "\n",
    "def ask_bert(sample_input, tokenizer, model):\n",
    "    \"\"\" This function takes an input, a tokenizer, a model and returns the prediciton \"\"\"\n",
    "    out = model.predict(sample_input)\n",
    "    pred_ans_start = tf.argmax(out[0][0])\n",
    "    pred_ans_end = tf.argmax(out[1][0])\n",
    "    \n",
    "    print(f\"{pred_ans_start}-{pred_ans_end} token ids contain the answer\")\n",
    "    \n",
    "    ans_tokens = sample_input[0][0][pred_ans_start:pred_ans_end-1]\n",
    "    \n",
    "    return \" \".join(tokenizer.convert_ids_to_tokens(ans_tokens))\n",
    "\n",
    "\n",
    "print(\"Question:\")\n",
    "print(\"\\t\", sample_q, \"\\n\")\n",
    "\n",
    "print(\"Context\")\n",
    "print(\"\\t\", sample_q, \"\\n\")\n",
    "\n",
    "print(\"Answer (char indexed)\")\n",
    "print(\"\\t\", sample_a, \"\\n\")\n",
    "print('='*50,'\\n')\n",
    "\n",
    "sample_pred_ans = ask_bert(sample_input, tokenizer, model_v2)\n",
    "\n",
    "print(\"Answer (predicted)\")\n",
    "print(sample_pred_ans)\n",
    "print('='*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15e1288c-5818-4e5f-a14f-bb6853dd54e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "\t When were the Normans in Normandy? \n",
      "\n",
      "Context\n",
      "\t When were the Normans in Normandy? \n",
      "\n",
      "Answer (char indexed)\n",
      "\t {'text': '10th and 11th centuries', 'answer_start': 94, 'answer_end': 117} \n",
      "\n",
      "================================================== \n",
      "\n",
      "1/1 [==============================] - 0s 342ms/step\n",
      "28-31 token ids contain the answer\n",
      "Answer (predicted)\n",
      "10th and\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "sample_q = test_questions[i]\n",
    "sample_c = test_contexts[i]\n",
    "sample_a = test_answers[i]\n",
    "\n",
    "sample_input = (test_encodings[\"input_ids\"][i:i+1], \n",
    "                test_encodings[\"attention_mask\"][i:i+1])\n",
    "\n",
    "\n",
    "print(\"Question:\")\n",
    "print(\"\\t\", sample_q, \"\\n\")\n",
    "\n",
    "print(\"Context\")\n",
    "print(\"\\t\", sample_q, \"\\n\")\n",
    "\n",
    "print(\"Answer (char indexed)\")\n",
    "print(\"\\t\", sample_a, \"\\n\")\n",
    "print('='*50,'\\n')\n",
    "\n",
    "sample_pred_ans = ask_bert(sample_input, tokenizer, model_v2)\n",
    "\n",
    "print(\"Answer (predicted)\")\n",
    "print(sample_pred_ans)\n",
    "print('='*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bf659-e9e6-464f-99ab-1459e20d0984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
