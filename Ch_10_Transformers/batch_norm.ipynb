{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f45bb64-95c9-4542-a4a0-04a9b212c474",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a284a-440b-4a78-8282-60b6ab08ebd9",
   "metadata": {},
   "source": [
    "ğ’ğ­ğšğ§ğğšğ«ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ is a way to bring our data to the same scale so that our neural network (or any ML algo.) can learn the patterns faster & efficiently.\n",
    "\n",
    "- In the context of neural networks, it's a way to avoid the famous vanishing/exploding gradient problem, which in simple terms, can be explained as our loss function becoming too small or too large during backpropagation.\n",
    "\n",
    "- Standardization brings values b/w 0-1, and Normalization makes mean=0 and variance/std_dev = 1 for the features in the data.\n",
    "\n",
    "- Normally Standardization/Normalization is done before passing our data to the neural net. Though it is seen that, applying normalization to the subsequent outputs of the layers(each neuron) of the neural networks helps in achieving better performance/accuracy.\n",
    "\n",
    "* **\n",
    "\n",
    "ğ‹ğğ­'ğ¬ ğ¬ğğ ğ¡ğ¨ğ° ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ ğ°ğ¨ğ«ğ¤ğ¬,\n",
    "\n",
    "- ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§: does the same thing, makes the data's mean=0 and variance=1 but with a little bit of twist.\n",
    "\n",
    "    1. First of all, it works on the batches of the data, as we pass it to the network.\n",
    "\n",
    "    2. It introdues two new learnable parameters(gamma & beta), which basically scales(to a diff. variance) and off-sets/shifts(to a diff. mean) the normalized output. [ y_new = gamma * y + beta ]\n",
    "\n",
    "    3. Why scale and shift the normalized outputs?\n",
    "\n",
    "        - In practice, not all features or layers in a neural network have the same importance. Some may be more or less relevant to the task at hand. By learning gamma and beta, the model can automatically adjust the scaling and shifting to account for these differences in scale & importance of layers, making the network more robust to various inputs. [This is a very simplified explanation]\n",
    "\n",
    "* **\n",
    "\n",
    "ğğğ§ğğŸğ¢ğ­ğ¬ ğ¨ğŸ ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§:\n",
    "\n",
    "1. Seepds up training, leading to faster convergence and can lead to better performance.\n",
    "\n",
    "2. No need to manually normalize/standardize your data before passing it to the neural network. Simply add a Batch Normalization layer before the first layer of the model.\n",
    "\n",
    "3. It also regularizes the model, a little bit, helping with overfitting. Although you might need to separately add regularization depending upon the problem.\n",
    "\n",
    "    - This is due to its inherent noise introduced by normalizing each mini-batch differently, which can reduce overfitting to some extent.\n",
    "\n",
    "4. In addition to the reduced risk of overfitting, you can often use higher learning rates when batch normalization is employed, further accelerating the convergence of the network.\n",
    "\n",
    "5. Another advantage of batch normalization is its ability to reduce the risk of vanishing/exploding gradients.\n",
    "\n",
    "ğ–ğ¡ğğ«ğ ğ­ğ¨ ğ®ğ¬ğ ğğšğ­ğœğ¡ ğğ¨ğ«ğ¦ğšğ¥ğ¢ğ³ğšğ­ğ¢ğ¨ğ§?\n",
    "\n",
    "- The batch normalization can be applied before and after the activation function. However, the authors suggest it is best when applied before the activation function. It's an experimental thing.\n",
    "\n",
    "\n",
    "* **\n",
    "\n",
    "As someone who is familiar with Batch Normalization I was personally missing a few important information which is why I add them here for the community:\n",
    "\n",
    "- The normalisation is happening over the batch dimension (in contrast to other variants such as layer normalisation where we normalise over the layer dimension), meaning that we normalize the feature over the mini-batch\n",
    "\n",
    "- which is why it does not work well for smaller batch sizes (usually 16+)\n",
    "\n",
    "- another advantage for the scale and the offset parameter is that it allows the network to undo the BN, meaning that BN can't make your result worse\n",
    "\n",
    "- during test time with e.g one sample only we can't compute mean and std since we don't have a batch. This is why we use running statistics of mean and variance calculated during training\n",
    "\n",
    "* **\n",
    "\n",
    "Refrences:\n",
    "1. [by AssemblyAI](https://lnkd.in/gg7Huvag)\n",
    "2. [by Ajay Halthor](https://lnkd.in/gCdDKpCN)\n",
    "3. [by deeplizard](https://lnkd.in/gZmBmkQt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a40c2-68ef-44ae-88c5-7954757aba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
