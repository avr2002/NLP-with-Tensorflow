{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7e7f77-bd58-48c3-a11e-d9f094ee95ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Understanding LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4be336-1045-4267-af3c-dee1d3e83b78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LSTM Into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c295d-c01b-4fc2-bd10-9f5ccee33760",
   "metadata": {},
   "source": [
    "- LSTMs are designed to avoid the problem of the vanishing gradient.\n",
    "The main practical limitation posed by the vanishing gradient is that it prevents the model from learning long-term dependencies. \n",
    "\n",
    "- However, by avoiding the vanishing gradient problem, LSTMs have the ability to store memory for longer than ordinary RNNs (for hundreds of time steps). \n",
    "\n",
    "- In contrast to RNNs, which only maintain a single hidden state, LSTMs have many more parameters as well as better control over what memory to store and what to discard at a given training step. \n",
    "\n",
    "- For example, RNNs are not able to decide which memory to store and which to discard, as the hidden state is forced to be updated at every training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8433d-16c7-4ef9-b4e0-faf60c210f4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### What is an LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9c647-48af-4b83-bb6c-7eaa872887a9",
   "metadata": {},
   "source": [
    "LSTMs can be seen as a more complex and capable family of RNNs. An LSTM is mainly composed of five different components:\n",
    "\n",
    "- **Cell state:** This is the internal cell state (that is, memory) of an LSTM cell\n",
    "\n",
    "- **Hidden state:** This is the external hidden state exposed to other layers and used to calculate predictions\n",
    "\n",
    "- **Input gate:** This determines how much of the current input is read into the cell state\n",
    "\n",
    "- **Forget gate:** This determines how much of the previous cell state is sent into the current cell state\n",
    "\n",
    "- **Output gate:** This determines how much of the cell state is output into the hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b8909-9ce6-431e-8456-cf20dadfd883",
   "metadata": {},
   "source": [
    "- We can wrap the RNN to a cell architecture as follows: \n",
    "\n",
    "    - the cell will output some state (with a nonlinear activation function) that is dependent on the previous cell state and the current input.\n",
    "\n",
    "    - However, in RNNs, the cell state is continuously updated with every incoming input. This behavior is quite undesirable for storing long-term dependencies.\n",
    "\n",
    "- LSTMs can decide when to add, update, or forget information stored in each neuron in the cell state. In other words, LSTMs are equipped with a mechanism to keep the cell state unchanged(if warranted for better performance), giving them the ability to store long-term dependencies.\n",
    "\n",
    "- This is achieved by introducing a gating mechanism. LSTMs possess gates for each operation the cell needs to perform. The gates are continuous (often sigmoid functions) between 0 and 1, where 0 means no information flows through the gate and 1 means all the information flows through the gate. \n",
    "\n",
    "- An LSTM uses one such gate for each neuron in the cell. These gates control the following:\n",
    "    - How much of the current input is written to the cell state (input gate)\n",
    "    \n",
    "    - How much information is forgotten from the previous cell state (forget gate)\n",
    "    \n",
    "    - How much information is output into the final hidden state from the cell state (output gate)\n",
    "    \n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/abstract_lstm.png'/>\n",
    "</div>\n",
    "The thickness of each line\n",
    "represents how much information is flowing from/to that gate (in some hypothetical scenarios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38046215-21d0-4240-96c3-ed1726d0a45e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LSTMs in more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff89d6-ef57-4fa6-9637-23f06e58036c",
   "metadata": {},
   "source": [
    "As discussed, LSTMs have a gating mechanism composed of the following three gates:\n",
    "\n",
    "- ***Input gate:*** A gate that outputs values between 0 (**the current input is not written to the cell state**) and 1 (**the current input is fully written to the cell state**). Sigmoid activation is used to squash the output to between 0 and 1.\n",
    "\n",
    "- ***Forget gate:*** A sigmoidal gate that outputs values between 0 (**the previous cell state is fully forgotten for calculating the current cell state**) and 1 (**the previous cell state is fully read in when calculating the current cell state**).\n",
    "\n",
    "- ***Output gate:*** A sigmoidal gate that outputs values between 0 (**the current cell state is fully discarded for calculating the final state**) and 1 (**the current cell state is fully used when calculating the final hidden state**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26d39e-da45-4e26-99c1-6437cdea8267",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### How LSTMs differ from standard RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6389abec-88fe-44f1-be6e-d891676998fc",
   "metadata": {},
   "source": [
    "An LSTM has a more intricate structure compared to a standard RNN. \n",
    "\n",
    "- One of the primary differences is that an LSTM has two different states: a cell state $c_t$ and a final hidden state $h_t$. However, an RNN only has a single hidden state $h_t$. \n",
    "\n",
    "- The next primary difference is that, since an LSTM has three different gates, an LSTM has much more control over how the current input and the previous cell state are handled when computing the final hidden state $h_t$.\n",
    "\n",
    "- Having the two different states is quite advantageous. With this mechanism, we can decouple the model’s short-term and long-term memory. \n",
    "\n",
    "- In other words, even when the cell state is changing quickly, the final hidden state will still be changed more slowly. So, while the cell state is learning both short-term and long-term dependencies, the final hidden state can reflect either only the short-term dependencies, only the long-term dependencies, or both.\n",
    "\n",
    "- Next, the gating mechanism is composed of three gates: the input, forget, and output gates.\n",
    "\n",
    "- It is quite evident that this is a more principled approach (especially compared to the standard RNNs) that permits better control over how much the current input and the previous cell state contribute to the current cell state. Also, the output gate gives better control over how much the cell state contributes to the final hidden state.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35558e27-d6cd-494b-85b0-4b66c3ef763c",
   "metadata": {},
   "source": [
    "## 2. Improving LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea315544-8ec7-457c-8181-ece45e755b8b",
   "metadata": {},
   "source": [
    "Numerous extensions have been developed to help LSTMs perform better at the prediction stage:- \n",
    "- **Greedy Sampling**,\n",
    "\n",
    "- **Beam Search**, \n",
    "\n",
    "- using **Word Vectors** instead of a one-hot-encoded representation of words, and \n",
    "\n",
    "- using **bidirectional LSTMs**\n",
    "\n",
    "It is important to note that these optimization techniques are not specific to LSTMs; rather, any sequential model can benefit from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c178e9-f38f-4c08-82d4-bfbb0a8d8446",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.1 Greedy Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba9829-b0be-4855-bc4a-db990d503c97",
   "metadata": {},
   "source": [
    "If we try to always predict the word with the highest probability, the LSTM will tend to produce very monotonic results. For example, due to the frequent occurrence of stop words (e.g. the), it may repeat them many times before switching to another word.\n",
    "\n",
    "\n",
    "- One way to get around this is to use **greedy sampling**, where we pick the predicted best *n* and sample from that set. This helps to break the monotonic nature of the predictions.\n",
    "\n",
    "Let’s consider the first sentence of the previous example:\n",
    "`John gave Mary a puppy.`\n",
    "\n",
    "Say, we start with the first word and want to predict the next four words: `John __ __ _ __`.\n",
    "\n",
    "If we attempt to choose samples deterministically, the LSTM might output something like the following:\n",
    "\n",
    "`John gave Mary gave John.`\n",
    "\n",
    "However, by sampling the next word from a subset of words in the vocabulary (most highly probable ones), the LSTM is forced to vary the prediction and might output the following:\n",
    "\n",
    "`John gave Mary a puppy.`\n",
    "\n",
    "Alternatively, it might give the following output:\n",
    "\n",
    "`John gave puppy a puppy.`\n",
    "\n",
    "- However, even though greedy sampling helps to add more flavor/diversity to the generated text, this method does not guarantee that the output will always be realistic, especially when outputting longer sequences of text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dffd9-b0d3-48b0-973d-8b43bee93f36",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2 Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33278142-a932-4834-a65f-050d444fa68e",
   "metadata": {},
   "source": [
    "- Beam search is a way of helping with the quality of the predictions produced by the LSTM. \n",
    "\n",
    "- In this, the predictions are found by solving a search problem. Particularly, we predict several steps ahead for multiple candidates at each step. This gives rise to a tree-like structure with candidate sequences of words (below fig.). \n",
    "\n",
    "- The crucial idea of beam search is to produce the $b$ outputs (that is, $y_t, y_{t+1}, \\cdots, y_{t+b}$) at once instead of a single output $y_t$. \n",
    "\n",
    "    - Here, $b$ is known as the length of the beam, and the $b$ outputs produced are known as the beam.<br></br> \n",
    "\n",
    "- More technically, we pick the beam that has the highest joint probability $P(y_t, y_{t+1}, \\cdots, y_{t+b}|x_t)$ instead of picking the highest probable\n",
    "$P(y_t|x_t)$. \n",
    "\n",
    "- We are looking farther into the future before making a prediction, which usually leads to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb62e5-6d95-43f5-8a1e-faf7ab1e90bf",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/beam_search_1.png'/>\n",
    "</div>\n",
    "<div align='center'>\n",
    "    <img src='images/beam_search_2.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ab444-0f9b-4888-a5b0-0910f739f4ae",
   "metadata": {},
   "source": [
    "### 2.3 Bidirectional LSTMs (BiLSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71d245-dbdb-40f7-804c-8d69bbd19721",
   "metadata": {},
   "source": [
    "- Making LSTMs bidirectional is another way of improving the quality of the predictions of an LSTM. **By this we mean training the LSTM with text read in both directions: from the beginning to the end and the end to the beginning.**\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/biLSTM.png'/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "- **Another application of BiLSTMs is neural machine translation, where we translate a sentence of a source language to a target language**. \n",
    "    - As there is no specific alignment between the translation of one language to another, having access to both sides of a given token in the source language can greatly help to understand the context better, thus producing better translations. \n",
    "    \n",
    "    - As an example, consider a translation task of translating Filipino to English. In Filipino, sentences are usually written having verb-object-subject in that order, whereas in English, it is subject-verb-object. \n",
    "    \n",
    "    - In this translation task, it will be extremely helpful to read sentences both forward and backward to make a good translation.\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47792f22-15cb-45da-b416-2703b9f51ec2",
   "metadata": {},
   "source": [
    "$\\rightarrow$ ***Architecture of BiLSTM Network***\n",
    "\n",
    "- **A BiLSTM is essentially two separate LSTM networks. One network learns data from the beginning to the end, and the other network learns data from the end to the beginning.** \n",
    "\n",
    "- Training occurs in two phases:\n",
    "    - First, the solid-colored network is trained with data created by reading the text from the beginning to the end. This network represents the normal training procedure used for standard LSTMs. \n",
    "    \n",
    "    - Secondly, the dashed network is trained with data generated by reading the text in the reversed direction. \n",
    "    \n",
    "    - Then, at the inference phase, we use both the solid and dashed states' information (by concatenating both states and creating a vector) to predict the missing word.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/biLSTM_architecture.png'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c2bfb-4ce5-4c01-90ca-371bae3a9f6b",
   "metadata": {},
   "source": [
    "## 3. Other variants of LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b4f032-826d-44bd-9856-3c9f49d9b798",
   "metadata": {},
   "source": [
    "- **Peephole connections**\n",
    "\n",
    "    - Peephole connections allow gates to see not only the current input and the previous final hidden state, but also the previous cell state. This increases the number of weights in the LSTM cell. Having such connections has been shown to produce better results.\n",
    "    \n",
    "    - Vanilla LSTMs, when calculating the gates, only look at the current input and the hidden state. With peephole connections, we make the gate computations dependent on all: the current input, and the hidden and cell states. <br></br>\n",
    "    \n",
    "- **Gated Recurrent Units(GRUs)**\n",
    "\n",
    "    - GRUs are a much more elegant variant of vanilla LSTMs that simplify LSTMs without compromising on performance. GRUs have only two gates and a single state, whereas vanilla LSTMs have three gates and two states.\n",
    "    \n",
    "* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
