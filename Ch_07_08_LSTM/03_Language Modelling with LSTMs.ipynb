{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d45ee8-7c7a-4632-a794-2675526397af",
   "metadata": {},
   "source": [
    "<h1 align='center'>Applications of LSTM – Generating Text</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93852709-64aa-4270-b31d-d38ccef07e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba5316-f785-486e-862a-ed8ad65766bc",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "[Dowloading Stories](https://www.cs.cmu.edu/~spok/grimmtmp/). The dataset consists of 209 stories. These are the translations of some folk stories by the Grimm brothers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870cc2f5-e126-4842-af32-17ea4fdfe930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "      \n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        \n",
    "    return filepath\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 209\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1, num_files+1)]\n",
    "\n",
    "# Download each file\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "    \n",
    "# Check if all files are downloaded\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "\n",
    "print(f\"{len(filenames)} files found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f4e92-2e15-4c69-890b-ee58c003ba23",
   "metadata": {},
   "source": [
    "## Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf1362c-c144-412e-8d44-1ae3a59832c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 167 files in the train dataset (e.g. ['data\\\\117.txt', 'data\\\\133.txt', 'data\\\\069.txt'])\n",
      "Got 21 files in the valid dataset (e.g. ['data\\\\023.txt', 'data\\\\078.txt', 'data\\\\176.txt'])\n",
      "Got 21 files in the test dataset (e.g. ['data\\\\129.txt', 'data\\\\207.txt', 'data\\\\170.txt'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 54321\n",
    "\n",
    "filenames = [os.path.join(dir_name, file) for file in os.listdir(dir_name)]\n",
    "\n",
    "# First separate train and valid+test data\n",
    "train_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=0.2, \n",
    "                                                             random_state=random_state)\n",
    "\n",
    "# Separate valid+test data to validation and test data\n",
    "valid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=0.5,\n",
    "                                                   random_state=random_state)\n",
    "\n",
    "# Print out the sizes and some sample filenames\n",
    "for subset_id, subset in zip(('train', 'valid', 'test'), (train_filenames, valid_filenames, test_filenames)):\n",
    "    print(f\"Got {len(subset)} files in the {subset_id} dataset (e.g. {subset[:3]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79445052-1979-4b18-a3f1-12ba19e434be",
   "metadata": {},
   "source": [
    "## Analyzing the vocabulary size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec69868-f10c-4fc0-b6f2-ebd043e16c77",
   "metadata": {},
   "source": [
    "- We will process the text by breaking it into character-level bigrams (n-grams where n=2) and make a vocabulary out of the unique bigrams. \n",
    "\n",
    "- Using character-level bigrams helps us to language model with a reduced vocabulary, leading to faster model training.\n",
    "\n",
    "For e.g.: \"`The king was hunting in the forest`\", would break down to a sequence of bigrams as follows: `[‘th’, ‘e ‘, ‘ki’, ‘ng’, ‘ w’, ‘as’, …]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744f3f8c-b5a9-42ae-9a10-c6815ae49e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 705 unique bigrams\n"
     ]
    }
   ],
   "source": [
    "bigram_set = set()\n",
    "\n",
    "# Go through each file in the training set\n",
    "for file_name in train_filenames:\n",
    "    document = [] # This will hold all the text\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            # Convert text to lower case to reduce input dimensionality\n",
    "            document.append(row.lower())\n",
    "        \n",
    "        # Stiching all the text\n",
    "        document = \" \".join(document)\n",
    "        \n",
    "        # Updating the set with bigram\n",
    "        bigram_set.update([document[i:i+2] for i in range(0, len(document), 2)])\n",
    "        \n",
    "        \n",
    "n_vocab = len(bigram_set)\n",
    "print(f\"Found {n_vocab} unique bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880ef11-7dd7-4c6a-9252-bdf22fafb314",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the `tf.data` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb71dd-26ab-4c20-b8df-12397a6f5992",
   "metadata": {},
   "source": [
    "We will now define a fully fledged data pipeline that is capable of reading the files from the disk and transforming the content into a format or structure that can be used to train the model.\n",
    "\n",
    "<div align='center'>\n",
    "    <b>Process for generating data to train the language model</b>\n",
    "</div>\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/data_pipe.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787efbb-97dc-4f56-982e-fc1a797ffc22",
   "metadata": {},
   "source": [
    "For example assume an ngram_width of 2, batch size of 1, and window_size of 5. This function would take the string `the king was hunting in the forest` and output:\n",
    "\n",
    "```\n",
    "Batch 1: [\"th\", \"e \", \"ki\", \" ng\", \" w\"] -> [\"e \", \"ki\", \"ng\", \" w\", \"as\"]\n",
    "Batch 2: [\"as\", \" h\", \"un\", \"ti\", \"ng\"] -> [\" h\", \"un\", \"ti\", \"ng\", \" i\"]\n",
    "...\n",
    "```\n",
    "The left list in each batch represents the input sequence, and the right list represents the target sequence. \n",
    "\n",
    "Note how the right list is simply the left one shifted one to the right. Also note how\n",
    "there’s no overlap between the inputs in the two records. But in the actual function, we a small overlap between records is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54914238-e91b-4e50-b8cf-b26239069ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tf_dataset(filenames, ngram_width, window_size, batch_size, shuffle=False):\n",
    "    \"\"\" \n",
    "    Generate batched data from a list of files speficied\n",
    "    \n",
    "    Args:\n",
    "        • filenames – A list of filenames containing the text to be used for the model\n",
    "        • ngram_width – Width of the n-grams to be extracted\n",
    "        • window_size – Length of the sequence of n-grams to be used to generate a single data\n",
    "                        point for the model\n",
    "        • batch_size – Size of the batch\n",
    "        • shuffle – (defaults to False) Whether to shuffle the data or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the data \n",
    "    documents = []\n",
    "    for file in filenames:\n",
    "        doc = tf.io.read_file(file)\n",
    "        doc = tf.strings.ngrams( # Create ngram from string\n",
    "                    tf.strings.bytes_split( # Split text into characters\n",
    "                         tf.strings.regex_replace( # Replace new lines with space\n",
    "                              tf.strings.lower(\n",
    "                                  doc\n",
    "                              ), \"\\n\", \" \"\n",
    "                         )\n",
    "                    ),\n",
    "                    ngram_width, separator=''\n",
    "                )\n",
    "        documents.append(doc.numpy().tolist())\n",
    "    \n",
    "    # documents is a list of list of strings, where each string is a story\n",
    "    # From that we generate a ragged tensor\n",
    "    documents = tf.ragged.constant(documents)\n",
    "    \n",
    "    # Create a dataset where each row in the ragged tensor would be a sample\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "    \n",
    "    # We need to perform a quick transformation - tf.strings.ngrams would generate\n",
    "    # all the ngrams (e.g. abcd -> ab, bc, cd) with overlap, however for our data\n",
    "    # we do not need the overlap, so we need to skip the overlapping ngrams\n",
    "    # the following line does that\n",
    "    \n",
    "    # Here, we simply get rid of the overlapping n-grams by taking only every \n",
    "    # nth n-gram in the sequence:\n",
    "    doc_dataset = doc_dataset.map(lambda x: x[::ngram_width])\n",
    "    \n",
    "    # Here we are using a window function to generate windows from text\n",
    "    # For a text sequence with window_size 3 and shift 1 you get\n",
    "    # e.g. ab, cd, ef, gh, ij, ... -> [ab, cd, ef], [cd, ef, gh], [ef, gh, ij], ...\n",
    "    # each of these windows is a single training sequence for our model\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "                        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "                                    x\n",
    "                        ).window(\n",
    "                            size=window_size+1, shift=int(window_size*0.75)\n",
    "                        ).flat_map(\n",
    "                            lambda window: window.batch(window_size+1, drop_remainder=True)\n",
    "                        )\n",
    "                  )\n",
    "    \n",
    "    # From each windowed sequence we generate input and target tuple\n",
    "    # e.g. [ab, cd, ef] -> ([ab, cd], [cd, ef])\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    \n",
    "    # Shuffle the data if required\n",
    "    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if shuffle else doc_dataset\n",
    "    \n",
    "    # Batch the Data\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "    \n",
    "    return doc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d36f32-40d6-44c0-9701-1f771aea9da8",
   "metadata": {},
   "source": [
    "- A RaggedTensor is a special type of tensor that can have dimensions that accept arbitrarily sized inputs. `tf.ragged.constant()`\n",
    "\n",
    "- For example, it is almost impossible that all the stories would have the same number of n-grams in each as they vary from each other a lot. In this case, we will have arbitrarily long sequences of n-grams representing our stories. \n",
    "\n",
    "- Therefore, we can use a RaggedTensor to store these arbitrarily sized sequences.\n",
    "\n",
    "* **\n",
    "\n",
    "Explanation of above code\n",
    "```\n",
    "doc_dataset = doc_dataset.flat_map(\n",
    "                        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "                                    x\n",
    "                        ).window(\n",
    "                            size=window_size+1, shift=int(window_size*0.75)\n",
    "                        ).flat_map(\n",
    "                            lambda window: window.batch(window_size+1, drop_remainder=True)\n",
    "                        )\n",
    "                  )\n",
    "```\n",
    "After removing the overlapping ngrams. The dataset is transformed using `.flat_map` and `.window` to generate sliding windows of size `window_size+1` from the n-gram sequences. The windows are created with a shift that's 75% of `window_size`, leading to 25% overlapping between two consecutive sequences. Each window is then batched using `.batch` with `drop_remainder=True` to ensure consistent batch sizes.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a456e62-7e5c-4619-a296-53dcd2a32830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ngram_length = 2\n",
    "batch_size = 128\n",
    "window_size = 128\n",
    "\n",
    "train_ds = generate_tf_dataset(train_filenames, ngram_length, \n",
    "                               batch_size, window_size, shuffle=True)\n",
    "\n",
    "valid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)\n",
    "test_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56d007-8a84-402d-bf57-51886692699f",
   "metadata": {},
   "source": [
    "### Generate few samples from the dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27513b55-79ef-4c1e-9499-0cf3e030b5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ']] -> [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ' b'a ']]\n",
      "[[b' u' b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph']] -> [[b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph' b'er']]\n",
      "[[b' s' b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ']] -> [[b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ' b'fa']]\n",
      "[[b'wh' b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd ' b'fa']] -> [[b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd ' b'fa' b'r ']]\n",
      "[[b'ea' b'd ' b'fa' b'r ' b'an' b'd ' b'wi' b'de' b' b' b'ec']] -> [[b'd ' b'fa' b'r ' b'an' b'd ' b'wi' b'de' b' b' b'ec' b'au']]\n"
     ]
    }
   ],
   "source": [
    "ds = generate_tf_dataset(train_filenames, ngram_width=2, window_size=10, batch_size=1)\n",
    "\n",
    "for record in ds.take(5):\n",
    "    print(record[0].numpy(), '->', record[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a38266-408b-49b8-aff8-b553e1a117cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementing the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "490ad191-9897-4f38-b8c3-0018f17da98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9a438-9eeb-4b9a-8086-1628d3e29175",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Defining the `TextVectorization` layer\n",
    "\n",
    "define a TextVectorization layer to convert the sequences of n-grams to sequences of integer IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7699b2e-c5ce-404e-9bb1-e55966b7e508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "# The vectorization layer that will convert string bigrams to IDs\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=n_vocab, standardize=None,\n",
    "                                           split=None, input_shape=(window_size,))\n",
    "\n",
    "# The the layer on data\n",
    "text_vectorizer.adapt(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "423a4a96-e746-4ab6-aa23-2f024e3f1cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ', ', ' h']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfec81-553b-4d5c-9c33-519f7db6d170",
   "metadata": {},
   "source": [
    "**Convert the targets from string ngrams to ngram IDs:**\n",
    "\n",
    "Remember that our data pipelines output sequences of n-gram strings as inputs and targets. We need to convert the target sequences to sequences of n-gram IDs so that a loss can be computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50184277-2f62-4918-aad2-9df4e933b2be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "valid_ds = valid_ds.map(lambda x, y: (x, text_vectorizer(y)))\n",
    "test_ds = test_ds.map(lambda x, y: (x, text_vectorizer(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573c83b-a0dd-49d8-bc38-ef7a76fe926f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Defining the LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7523762-9f24-48ec-b94a-49feddaadbfe",
   "metadata": {},
   "source": [
    "Our model will have:\n",
    "- The previously trained TextVectorization layer\n",
    "- An embedding layer randomly initialized and jointly trained with the model\n",
    "- Two LSTM layers each with 512 and 256 nodes respectively\n",
    "- A fully-connected hidden layer with 1024 nodes and `ReLU` activation\n",
    "- The final prediction layer with `n_vocab` nodes and `softmax` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7eb9f49-ea5e-4e3b-8fd2-55e1203e1408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "lm_model = models.Sequential([text_vectorizer,\n",
    "                              layers.Embedding(input_dim=n_vocab+2, output_dim=96),\n",
    "                              layers.LSTM(512, return_state=False, return_sequences=True),\n",
    "                              layers.LSTM(256, return_state=False, return_sequences=True),\n",
    "                              layers.Dense(1024, activation='relu'),\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Dense(n_vocab, activation='softmax')\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12751bf0-ebc4-4b34-bf8b-41d252e1abe8",
   "metadata": {},
   "source": [
    "`K.clear_session()`, which is a function that clears the current TensorFlow session (e.g. layers and variables defined and their states). Otherwise, if you run multiple times in a notebook, it will create an unnecessary number of layers and variables.\n",
    "\n",
    "Parameters of the LSTM layer in more detail:\n",
    "- `return_state` – Setting this to `False` means that the layer outputs only the final output, whereas if set to `True`, it will return state vectors along with the final output of the layer. For example, for an LSTM layer, setting `return_state=True` means you’ll get three outputs: the final output, cell state, and hidden state. Note that the final output and the hidden state will be identical in this case.\n",
    "\n",
    "- `return_sequences` – Setting this to true will cause the layer to output the full output sequences, as opposed to just the last output. For example, setting this to `False` will give you a `[b, n]`-sized output where `b` is the batch size and` n` is the number of nodes in the layer. If `True`, it will output a `[b, t, n]`-sized output, where `t` is the number of time steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10abcfd0-056e-4cbe-953c-16900214276e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 128)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 96)           67872     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128, 512)          1247232   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128, 256)          787456    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128, 1024)         263168    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 1024)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128, 705)          722625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af1247-5ad4-44a7-ad93-8614fa67b26b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Defining metrics and compiling the model\n",
    "\n",
    "- [Evaluation Metrics for Language Modeling - Chip Hyuen](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)\n",
    "\n",
    "- [A Gentle Introduction to Information Entropy - mlmastery](https://machinelearningmastery.com/what-is-information-entropy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786965af-3c94-427d-9b73-ec9e6eb5f57a",
   "metadata": {},
   "source": [
    "Accuracy is used as a general-purpose evaluation metric across different ML tasks. However, accuracy might not be cut out for this task, mainly because it relies on the model choosing the exact word/bigram for a given time step as in the dataset. However, languages are complex and there can be many different choices to generate the next word/bigram given a text. Therefore, NLP practitioners rely on a metric known as **perplexity**, \n",
    "\n",
    "- **Perplexity** *measures how \"perplexed\" or “surprised” the model was to see a `t+1` bigram given `1:t` bigrams.*\n",
    "\n",
    "- **Perplexity is simply the entropy to the power of two.** \n",
    "\n",
    "- Entropy is a measure of the uncertainty or randomness of an event. The more uncertain the outcome of the event, the higher the entropy. Entropy Formula is:\n",
    "\n",
    "$$H(X) = - \\sum_{x \\forall X}p(x) \\log(p(x))$$\n",
    "\n",
    "- In machine learning, to optimize ML models, *we measure the difference between the predicted probability distribution versus the target probability distribution for a given sample*. For that, we use **cross-entropy**, an extension of entropy for two distributions: \n",
    "\n",
    "$$\\text{Categorical Crossentropy}(\\hat{y}_i, y_i) = - \\sum_{c=1}^{c}y_{i,c}\\log(\\hat{y}_{i,c})$$\n",
    "\n",
    "Finally, we define perplexity as:\n",
    "\n",
    "$$Perplexity = 2^{H(X)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c2ffdda-d398-46ae-8c75-62437bc1ff21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspired by https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, \n",
    "                                                                           reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "        # The next 4 lines zero-out the padding from loss calculations, \n",
    "        # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t      \n",
    "        loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "        # Calculating the perplexity steps: \n",
    "        step1 = K.mean(loss_, axis=-1)\n",
    "        perplexity = K.exp(step1)\n",
    "\n",
    "        return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "        perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "        # Remember self.perplexity is a tensor (tf.Variable), \n",
    "        # so using simply \"self.perplexity = perplexity\" will result in error \n",
    "        # because of mixing EagerTensor and Graph operations \n",
    "        super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72b0e227-468b-4591-944d-6af238620cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 128)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 96)           67872     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128, 512)          1247232   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128, 256)          787456    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128, 1024)         263168    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 1024)         0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128, 705)          722625    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_model.compile(loss='sparse_categorical_crossentropy', \n",
    "                 optimizer='adam', \n",
    "                 metrics=['accuracy', PerplexityMetric()])\n",
    "\n",
    "lm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca345cf-7839-40bd-aabe-90af3edb7596",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f712223-5f32-48ff-a19f-fd1ff80d96ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "49/49 [==============================] - 16s 109ms/step - loss: 5.3646 - accuracy: 0.0319 - perplexity: 239.4168 - val_loss: 5.1143 - val_accuracy: 0.0392 - val_perplexity: 167.4550\n",
      "Epoch 2/50\n",
      "49/49 [==============================] - 5s 101ms/step - loss: 5.1404 - accuracy: 0.0347 - perplexity: 172.3481 - val_loss: 5.0856 - val_accuracy: 0.0392 - val_perplexity: 162.8157\n",
      "Epoch 3/50\n",
      "49/49 [==============================] - 5s 93ms/step - loss: 5.0406 - accuracy: 0.0413 - perplexity: 156.4139 - val_loss: 4.8421 - val_accuracy: 0.0729 - val_perplexity: 128.0916\n",
      "Epoch 4/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 4.6998 - accuracy: 0.0888 - perplexity: 112.2538 - val_loss: 4.4433 - val_accuracy: 0.1138 - val_perplexity: 86.2279\n",
      "Epoch 5/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 4.3050 - accuracy: 0.1340 - perplexity: 76.1817 - val_loss: 4.0241 - val_accuracy: 0.1642 - val_perplexity: 56.8829\n",
      "Epoch 6/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.9781 - accuracy: 0.1793 - perplexity: 54.8210 - val_loss: 3.7692 - val_accuracy: 0.2119 - val_perplexity: 44.1542\n",
      "Epoch 7/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.7851 - accuracy: 0.2066 - perplexity: 45.1519 - val_loss: 3.6192 - val_accuracy: 0.2227 - val_perplexity: 38.0347\n",
      "Epoch 8/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.6447 - accuracy: 0.2228 - perplexity: 39.2310 - val_loss: 3.5096 - val_accuracy: 0.2410 - val_perplexity: 34.1061\n",
      "Epoch 9/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.5390 - accuracy: 0.2372 - perplexity: 35.3079 - val_loss: 3.4049 - val_accuracy: 0.2566 - val_perplexity: 30.7410\n",
      "Epoch 10/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.4485 - accuracy: 0.2483 - perplexity: 32.2631 - val_loss: 3.3217 - val_accuracy: 0.2672 - val_perplexity: 28.3591\n",
      "Epoch 11/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.3677 - accuracy: 0.2594 - perplexity: 29.7750 - val_loss: 3.2523 - val_accuracy: 0.2759 - val_perplexity: 26.4897\n",
      "Epoch 12/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 3.2970 - accuracy: 0.2690 - perplexity: 27.7564 - val_loss: 3.1903 - val_accuracy: 0.2843 - val_perplexity: 24.9683\n",
      "Epoch 13/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 3.2353 - accuracy: 0.2773 - perplexity: 26.1089 - val_loss: 3.1288 - val_accuracy: 0.2935 - val_perplexity: 23.4846\n",
      "Epoch 14/50\n",
      "49/49 [==============================] - 5s 89ms/step - loss: 3.1806 - accuracy: 0.2846 - perplexity: 24.7275 - val_loss: 3.0808 - val_accuracy: 0.3003 - val_perplexity: 22.4001\n",
      "Epoch 15/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 3.1291 - accuracy: 0.2918 - perplexity: 23.4991 - val_loss: 3.0378 - val_accuracy: 0.3062 - val_perplexity: 21.4955\n",
      "Epoch 16/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 3.0837 - accuracy: 0.2982 - perplexity: 22.4708 - val_loss: 2.9937 - val_accuracy: 0.3125 - val_perplexity: 20.5570\n",
      "Epoch 17/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 3.0420 - accuracy: 0.3041 - perplexity: 21.5466 - val_loss: 2.9572 - val_accuracy: 0.3195 - val_perplexity: 19.8561\n",
      "Epoch 18/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 3.0024 - accuracy: 0.3100 - perplexity: 20.7193 - val_loss: 2.9209 - val_accuracy: 0.3243 - val_perplexity: 19.1435\n",
      "Epoch 19/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.9691 - accuracy: 0.3154 - perplexity: 20.0443 - val_loss: 2.8910 - val_accuracy: 0.3289 - val_perplexity: 18.6002\n",
      "Epoch 20/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.9338 - accuracy: 0.3205 - perplexity: 19.3513 - val_loss: 2.8591 - val_accuracy: 0.3358 - val_perplexity: 18.0181\n",
      "Epoch 21/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.9020 - accuracy: 0.3256 - perplexity: 18.7489 - val_loss: 2.8293 - val_accuracy: 0.3401 - val_perplexity: 17.4883\n",
      "Epoch 22/50\n",
      "49/49 [==============================] - 5s 90ms/step - loss: 2.8716 - accuracy: 0.3304 - perplexity: 18.1891 - val_loss: 2.8079 - val_accuracy: 0.3434 - val_perplexity: 17.1573\n",
      "Epoch 23/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.8443 - accuracy: 0.3347 - perplexity: 17.6999 - val_loss: 2.7800 - val_accuracy: 0.3486 - val_perplexity: 16.6828\n",
      "Epoch 24/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.8171 - accuracy: 0.3393 - perplexity: 17.2236 - val_loss: 2.7557 - val_accuracy: 0.3522 - val_perplexity: 16.2951\n",
      "Epoch 25/50\n",
      "49/49 [==============================] - 5s 93ms/step - loss: 2.7932 - accuracy: 0.3428 - perplexity: 16.8230 - val_loss: 2.7348 - val_accuracy: 0.3543 - val_perplexity: 15.9596\n",
      "Epoch 26/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.7701 - accuracy: 0.3464 - perplexity: 16.4367 - val_loss: 2.7143 - val_accuracy: 0.3601 - val_perplexity: 15.6189\n",
      "Epoch 27/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.7461 - accuracy: 0.3502 - perplexity: 16.0486 - val_loss: 2.6937 - val_accuracy: 0.3632 - val_perplexity: 15.3322\n",
      "Epoch 28/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.7261 - accuracy: 0.3535 - perplexity: 15.7256 - val_loss: 2.6761 - val_accuracy: 0.3668 - val_perplexity: 15.0661\n",
      "Epoch 29/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.7063 - accuracy: 0.3568 - perplexity: 15.4235 - val_loss: 2.6620 - val_accuracy: 0.3688 - val_perplexity: 14.8490\n",
      "Epoch 30/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.6866 - accuracy: 0.3591 - perplexity: 15.1213 - val_loss: 2.6422 - val_accuracy: 0.3732 - val_perplexity: 14.5485\n",
      "Epoch 31/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.6674 - accuracy: 0.3623 - perplexity: 14.8314 - val_loss: 2.6308 - val_accuracy: 0.3744 - val_perplexity: 14.3874\n",
      "Epoch 32/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.6494 - accuracy: 0.3655 - perplexity: 14.5645 - val_loss: 2.6146 - val_accuracy: 0.3760 - val_perplexity: 14.1698\n",
      "Epoch 33/50\n",
      "49/49 [==============================] - 5s 91ms/step - loss: 2.6319 - accuracy: 0.3678 - perplexity: 14.3129 - val_loss: 2.6031 - val_accuracy: 0.3776 - val_perplexity: 14.0402\n",
      "Epoch 34/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.6176 - accuracy: 0.3702 - perplexity: 14.1115 - val_loss: 2.5897 - val_accuracy: 0.3820 - val_perplexity: 13.8276\n",
      "Epoch 35/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.6007 - accuracy: 0.3728 - perplexity: 13.8724 - val_loss: 2.5759 - val_accuracy: 0.3844 - val_perplexity: 13.6634\n",
      "Epoch 36/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.5857 - accuracy: 0.3750 - perplexity: 13.6657 - val_loss: 2.5689 - val_accuracy: 0.3853 - val_perplexity: 13.5557\n",
      "Epoch 37/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.5707 - accuracy: 0.3775 - perplexity: 13.4626 - val_loss: 2.5525 - val_accuracy: 0.3878 - val_perplexity: 13.3376\n",
      "Epoch 38/50\n",
      "49/49 [==============================] - 5s 96ms/step - loss: 2.5569 - accuracy: 0.3798 - perplexity: 13.2753 - val_loss: 2.5425 - val_accuracy: 0.3889 - val_perplexity: 13.2209\n",
      "Epoch 39/50\n",
      "49/49 [==============================] - 5s 104ms/step - loss: 2.5431 - accuracy: 0.3819 - perplexity: 13.0888 - val_loss: 2.5320 - val_accuracy: 0.3925 - val_perplexity: 13.0993\n",
      "Epoch 40/50\n",
      "49/49 [==============================] - 5s 98ms/step - loss: 2.5294 - accuracy: 0.3839 - perplexity: 12.9087 - val_loss: 2.5215 - val_accuracy: 0.3938 - val_perplexity: 12.9704\n",
      "Epoch 41/50\n",
      "49/49 [==============================] - 5s 93ms/step - loss: 2.5166 - accuracy: 0.3858 - perplexity: 12.7460 - val_loss: 2.5130 - val_accuracy: 0.3960 - val_perplexity: 12.8697\n",
      "Epoch 42/50\n",
      "49/49 [==============================] - 5s 93ms/step - loss: 2.5040 - accuracy: 0.3882 - perplexity: 12.5898 - val_loss: 2.5062 - val_accuracy: 0.3955 - val_perplexity: 12.7805\n",
      "Epoch 43/50\n",
      "49/49 [==============================] - 5s 93ms/step - loss: 2.4920 - accuracy: 0.3893 - perplexity: 12.4356 - val_loss: 2.4991 - val_accuracy: 0.3981 - val_perplexity: 12.6906\n",
      "Epoch 44/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.4793 - accuracy: 0.3911 - perplexity: 12.2764 - val_loss: 2.4845 - val_accuracy: 0.4001 - val_perplexity: 12.4971\n",
      "Epoch 45/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.4676 - accuracy: 0.3939 - perplexity: 12.1322 - val_loss: 2.4778 - val_accuracy: 0.4009 - val_perplexity: 12.4232\n",
      "Epoch 46/50\n",
      "49/49 [==============================] - 5s 92ms/step - loss: 2.4572 - accuracy: 0.3951 - perplexity: 12.0026 - val_loss: 2.4698 - val_accuracy: 0.4025 - val_perplexity: 12.3394\n",
      "Epoch 47/50\n",
      "49/49 [==============================] - 5s 94ms/step - loss: 2.4460 - accuracy: 0.3966 - perplexity: 11.8667 - val_loss: 2.4715 - val_accuracy: 0.4023 - val_perplexity: 12.3883\n",
      "Epoch 48/50\n",
      "49/49 [==============================] - 5s 103ms/step - loss: 2.4347 - accuracy: 0.3986 - perplexity: 11.7377 - val_loss: 2.4656 - val_accuracy: 0.4045 - val_perplexity: 12.3243\n",
      "Epoch 49/50\n",
      "49/49 [==============================] - 5s 99ms/step - loss: 2.4243 - accuracy: 0.4002 - perplexity: 11.6148 - val_loss: 2.4497 - val_accuracy: 0.4069 - val_perplexity: 12.1122\n",
      "Epoch 50/50\n",
      "49/49 [==============================] - 5s 100ms/step - loss: 2.4132 - accuracy: 0.4019 - perplexity: 11.4861 - val_loss: 2.4409 - val_accuracy: 0.4096 - val_perplexity: 11.9853\n"
     ]
    }
   ],
   "source": [
    "lstm_history = lm_model.fit(train_ds, \n",
    "                            validation_data=valid_ds, \n",
    "                            epochs=50, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1837405f-9330-4a16-8021-3281fb110854",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 48ms/step - loss: 2.4974 - accuracy: 0.3920 - perplexity: 12.5615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.497385025024414, 0.3919745683670044, 12.561511039733887]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf0c79c-267b-4284-9fe1-4f89d5dbe89c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the inference model\n",
    "\n",
    "Inferring from the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d4855-96b8-4954-af91-df4e75166975",
   "metadata": {},
   "source": [
    "- During training, we trained our model and evaluated it on sequences of bigrams. This works for us because during training and evaluation, we have the full text available to us. However, when we need to generate new text, we do not have anything available to us. Therefore, we have to make adjustments to our trained model so that it can generate text from scratch.\n",
    "\n",
    "- The way we do this is by defining a recursive model that takes the current time step’s output of the model as the input to the next time step. This way we can keep predicting words/bigrams for an infinite number of steps. We provide the initial seed as a random word/bigram picked from the corpus (or even a sequence of bigrams). \n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"images/infer_model_arch.png\"/>\n",
    "</div>\n",
    "\n",
    "Our inference model is going to be comparatively more sophisticated, as we need to design an iterative process to generate text using previous predictions as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70756ea-6aeb-4929-b119-d0132641deb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
