{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639c3961-85c0-4d07-817d-31220c0bf9de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a19efe-9a48-48b2-821d-3b934d53d7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from itertools import chain\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "# %env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800ae35-5b79-4752-a8e0-d074a415eef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04ec0c4-e4e4-41bf-9194-2863ad556008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of sentences (each sentence a string), \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    # print(\"Reading data ...\")\n",
    "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token.strip())\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end / or reach the beginning of next\n",
    "            # add the data to the master lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    # print('\\tDone')\n",
    "    return sentences, ner_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33320033-09c2-4c74-b5a4-8a1a58dfddc3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14041\n",
      "Valid size: 3250\n",
      "Test size: 3452\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: EU rejects German call to boycott British lamb .\n",
      "Labels: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "\n",
      "Sentence: Peter Blackburn\n",
      "Labels: ['B-PER', 'I-PER']\n",
      "\n",
      "Sentence: BRUSSELS 1996-08-22\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "Sentence: The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .\n",
      "Labels: ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Labels: ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_filepath = 'data\\conllpp_train.txt'\n",
    "train_sentences, train_labels = read_data(train_filepath) \n",
    "# Validation data\n",
    "dev_filepath = 'data\\conllpp_dev.txt'\n",
    "valid_sentences, valid_labels = read_data(dev_filepath) \n",
    "# Test data\n",
    "test_filepath = 'data\\conllpp_test.txt'\n",
    "test_sentences, test_labels = read_data(test_filepath) \n",
    "\n",
    "# Print some stats\n",
    "print(f\"Train size: {len(train_labels)}\")\n",
    "print(f\"Valid size: {len(valid_labels)}\")\n",
    "print(f\"Test size: {len(test_labels)}\")\n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(train_sentences[:5], train_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325ec7b-2bbf-491e-a24e-88cb1ffd895a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Processing\n",
    "\n",
    "**Padding Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0c5933-8b20-46f5-937b-934b4f3ba850",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "def get_lable_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class_label --> class_ID mapping\n",
    "    label_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    \n",
    "    print(\"label_map: {}\".format(label_map))\n",
    "    return label_map\n",
    "\n",
    "labels_map = get_lable_id_map(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedfdc38-c190-48e7-9605-7595514f2e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_padded_int_labels(labels:list[list[str]], labels_map:dict[str, int], \n",
    "                          max_seq_length:int, return_mask:bool = True):\n",
    "    \"\"\"\n",
    "    This function takes sequences of class labels and return sequences of padded \n",
    "    class IDs, with the option to return a mask indicating padded labels.\n",
    "    \n",
    "    This function takes the following arguments:\n",
    "        * labels (List[List[str]]) – A list of lists of strings, where each string is \n",
    "                                     a class label of the string type\n",
    "        \n",
    "        * labels_map (Dict[str, int]) – A dictionary mapping a string label to a \n",
    "                                        class ID of type integer\n",
    "        \n",
    "        * max_seq_length (int) – A maximum length to be padded to (longer sequences \n",
    "                                 will be truncated at this length)\n",
    "        \n",
    "        * return_mask (bool) – Whether to return the mask showing padded labels or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert string labels to integers\n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and\n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "                           tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                int_labels, maxlen=max_seq_length, padding='post',\n",
    "                                truncating='post', value=-1\n",
    "                           )\n",
    "                        )\n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        \n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']\n",
    "        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    else:\n",
    "        # padded_labels = np.array(ner_pad_sequence_func(int_labels, \n",
    "        #                                                value=labels_map['O'])\n",
    "        #                         )\n",
    "        # return padded_labels\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1837cce2-67fc-45c7-b88c-914db7c5ced6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14041, 40) (14041, 40)\n",
      "\n",
      "Lable Map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "\n",
      "Padded Label: [0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "Mask:\t [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 40\n",
    "\n",
    "# Convert string labels to integers for all train/validation/test data\n",
    "# Pad train/validation/test data\n",
    "padded_train_labels, train_mask = get_padded_int_labels(train_labels, labels_map, \n",
    "                                                        max_seq_length, return_mask=True)\n",
    "\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(valid_labels, labels_map, \n",
    "                                                        max_seq_length, return_mask=True)\n",
    "\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(test_labels, labels_map, \n",
    "                                                       max_seq_length, return_mask=True)\n",
    "\n",
    "print(padded_train_labels.shape, train_mask.shape)\n",
    "print(\"\\nLable Map:\", labels_map)\n",
    "print(\"\\nPadded Label:\",padded_train_labels[0])\n",
    "print(\"Mask:\\t\", train_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09c73a-ea8c-440e-b339-6b86fba0dccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d13d1db-d7e1-438f-bc9c-38d86bfd214f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The maximum length of sequences\n",
    "max_seq_length = 40\n",
    "\n",
    "# Size of token embeddings\n",
    "embedding_size = 64\n",
    "\n",
    "# Number of hidden units in the RNN layer\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Number of output nodes in the last layer\n",
    "n_classes = len(labels_map)\n",
    "\n",
    "# Number of samples in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24082b3d-c6e3-4f8f-af96-a94bef6f543a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NER with Character & Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f43ed-2fba-486d-b628-1bf4cb5358ba",
   "metadata": {},
   "source": [
    "- Here we will focus our discussion on a technique that provides the model embeddings at multiple scales, enabling it to understand language better. That is, instead of relying only on **token embeddings**, we can also use **character embeddings**. Then a token embedding is generated with the character embeddings by shifting a convolutional window over the characters in the token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49caf245-4589-4e26-960b-f7599c64e9b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using Convolution to generate Token mbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c613d-09bf-4b43-8848-06e2d08dffa4",
   "metadata": {},
   "source": [
    "A combination of character embeddings and a convolutional kernel can be used to generate token embeddings. The method will be as follows:\n",
    "- Pad each token (e.g. word) to a predefined length\n",
    "- Look up the character embeddings for the characters in the token from an embedding layer\n",
    "- Shift a convolutional kernel over the sequence of character embeddings to generate a token embedding\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/char_token_embedding.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e32b70-75e3-4190-b9a0-bc412d839ba6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Statistics about token lenghts (for char embeddings)\n",
    "\n",
    "The very first thing we need to do is analyze the statistics around how many characters there are for a token in our corpus. Similar to how we did it previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab2e7b7-78de-487d-b6e6-1a5863454292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23623.000000\n",
       "mean         6.832705\n",
       "std          2.749288\n",
       "min          1.000000\n",
       "5%           3.000000\n",
       "50%          7.000000\n",
       "95%         12.000000\n",
       "max         61.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_ser = pd.Series(pd.Series(train_sentences).str.split().explode().unique())\n",
    "vocab_ser.str.len().describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ae1bb-424b-4a7d-9470-b99e35555efa",
   "metadata": {},
   "source": [
    "We can see around $95\\%$ of our words have less than or equal to $12$ characters. \n",
    "\n",
    "Next, we will write a function to pad shorter tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f811b6-0691-4b86-a4e5-8361fb0fc7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_sentences:\n",
      "[['aaaa', 'bb', 'c'], ['d', 'eee'], ['f', 'gg', 'hhh', 'iiii', 'jjjjj']]\n",
      "Padded sequence:\n",
      "[[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']], [['f'], ['gg'], ['hhh']]]\n"
     ]
    }
   ],
   "source": [
    "def prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length):\n",
    "    \"\"\"\n",
    "    Pads each sequence to a maximum length\n",
    "    :args:\n",
    "        - tokenized_sentences: The function takes a set of tokenized sentences\n",
    "                               (i.e. each sentence as a list of tokens, not a string)\n",
    "        \n",
    "        - max_seq_length: maximum sequence length\n",
    "        \n",
    "    This function would then do the following:\n",
    "        - For longer sentences, only return the max_seq_length tokens\n",
    "        - For shorter sentences, append ‘‘ as a token until max_seq_length is reached\n",
    "    \"\"\"\n",
    "    proc_sentences = []\n",
    "    for token in tokenized_sentences:\n",
    "        if len(token) >= max_seq_length:\n",
    "            proc_sentences.append([[t] for t in token[:max_seq_length]])\n",
    "        else:\n",
    "            proc_sentences.append([[t] for t in token+['']*(max_seq_length-len(token))])\n",
    "    \n",
    "    return proc_sentences\n",
    "\n",
    "# Define sample data\n",
    "data = ['aaaa bb c', 'd eee', 'f gg hhh iiii jjjjj']\n",
    "\n",
    "# Pad sequences\n",
    "tokenized_sentences= [d.split() for d in data]\n",
    "padded_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences, 3)\n",
    "\n",
    "print(f\"tokenized_sentences:\\n{tokenized_sentences}\")\n",
    "print(f\"Padded sequence:\\n{padded_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f463e4-4542-49fd-a5bc-73f922cff6b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing `TextVectorization` for char level\n",
    "\n",
    "We define a new `TextVectorization` layer that can cope with the changes we introduced to the data. Instead of tokenizing on the token level, the new `TextVectorization` layer must tokenize on the character level.\n",
    "\n",
    "For this we need to make a few changes. We will again write a function to contain this vectorization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca48023-905e-45bb-a305-f23452b51826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_sentences:\n",
      "[['aaaa', 'bb', 'c'], ['d', 'eee'], ['f', 'gg', 'hhh', 'iiii', 'jjjjj']]\n",
      "\n",
      "Padded sequence:\n",
      "[[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']], [['f'], ['gg'], ['hhh']]]\n",
      "\n",
      "Vectorized output:\n",
      "[[[2 2 2 2]\n",
      "  [6 6 0 0]\n",
      "  [9 0 0 0]]\n",
      "\n",
      " [[8 0 0 0]\n",
      "  [4 4 4 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[7 0 0 0]\n",
      "  [5 5 0 0]\n",
      "  [3 3 3 0]]]\n",
      "\n",
      "Vocabulary: ['', '[UNK]', 'a', 'h', 'e', 'g', 'b', 'f', 'd', 'c']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def split_char(token):\n",
    "    \"\"\" Instead of splitting word by word, split each char\"\"\"\n",
    "    return tf.strings.bytes_split(token)\n",
    "\n",
    "\n",
    "# Define a vectorization layer that splits chars\n",
    "vectorization_layer = TextVectorization(\n",
    "        standardize=None,      \n",
    "        split=split_char,\n",
    ")\n",
    "\n",
    "# Define sample data\n",
    "data = ['aaaa bb c', 'd eee', 'f gg hhh iiii jjjjj']\n",
    "\n",
    "# Pad sequences\n",
    "tokenized_sentences= [d.split() for d in data]\n",
    "padded_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences, 3)\n",
    "\n",
    "print(f\"tokenized_sentences:\\n{tokenized_sentences}\\n\")\n",
    "print(f\"Padded sequence:\\n{padded_sentences}\\n\")\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "vectorization_layer.adapt(padded_sentences)\n",
    "\n",
    "# Print data\n",
    "print(f\"Vectorized output:\\n{vectorization_layer(padded_sentences)}\\n\")\n",
    "print(f\"Vocabulary: {vectorization_layer.get_vocabulary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "029142b1-0841-4987-ad55-321deb722a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[b'a', b'b', b'c', b'd']]>\n",
      "<tf.RaggedTensor [[b'A', b'm', b'i', b't']]>\n"
     ]
    }
   ],
   "source": [
    "print(split_char(['abcd']))\n",
    "print(split_char(tf.constant(['Amit'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f440e-0b91-4d34-97ed-239a76da0ac3",
   "metadata": {},
   "source": [
    "## Implementing the new NER model\n",
    "\n",
    "### Defining an advance RNN model\n",
    "\n",
    "- Token embeddings + Char embeddings\n",
    "- Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0726d-a434-41af-908a-c3143be8d1c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6a2fcf4-f0c5-4f25-b6b7-3f2adacc40c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 40\n",
    "max_token_length = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4569e0a-26a5-4973-9afd-56f8b85e3414",
   "metadata": {},
   "source": [
    "### Define the i/p layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84c18882-2820-40e7-92d7-e7cdfee19978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Input layer(tokens)\n",
    "word_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# The inputs to this layer would be a batch of sentences, where each sentence is a string. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde2d84-26be-4cc4-aed0-680f2b04b2a2",
   "metadata": {},
   "source": [
    "### Defining the token-based TextVectorization layer & token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532d5a3d-2ef5-4dd5-9b44-d8c1c2a1f602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define the layer\n",
    "    vectorization_layer = TextVectorization(max_tokens=vocabulary_size, standardize=None,\n",
    "                                           output_sequence_length=max_seq_length)\n",
    "    \n",
    "    # Fit on the text corpus\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary_size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "    \n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "# Text vectorize layer (token)\n",
    "token_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "token_vectorized_out = token_vectorize_layer(word_input)\n",
    "\n",
    "# Look up embeddings for the returned IDs\n",
    "token_embedding_out = layers.Embedding(input_dim=n_token_vocab, \n",
    "                                       output_dim=64, \n",
    "                                       mask_zero=True)(token_vectorized_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b3128-be44-4925-9204-a7a89b6d76be",
   "metadata": {},
   "source": [
    "### Defining the character-based TextVectorization layer & character_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b1fb4d-e62d-4af1-b263-874b7384ac53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# character-based TextVectorization layer; \n",
    "# same as implemented in \"Testing TextVectorization for char level\"\n",
    "\n",
    "def get_fitted_char_vectorization_layer(corpus, max_seq_length, max_token_length, \n",
    "                                        vocabulary_size=None):\n",
    "    \"\"\"\n",
    "    Fit a TextVectorization layer on given data\n",
    "    \"\"\"\n",
    "    def _split_char(token):\n",
    "        \"\"\"\n",
    "        _split_char() that takes a token (as a tf.Tensor) and returns a char-tokenized tensor\n",
    "        \n",
    "        _funcName() means it is a private function.\n",
    "        Link: https://www.datacamp.com/tutorial/role-underscore-python\n",
    "        \"\"\"\n",
    "        return tf.strings.bytes_split(token)\n",
    "    \n",
    "    # Define a text vectorization layer\n",
    "    vectorization_layer = tf.keras.layers.TextVectorization(standardize=None,\n",
    "                                                            split=_split_char,\n",
    "                                                            output_sequence_length=max_token_length)\n",
    "    # Tokenize the sentences and pad it\n",
    "    tokenized_sentences = [sent.split() for sent in corpus]\n",
    "    padded_tokenized_sentences = prepare_corpus_for_char_embeddings(tokenized_sentences,\n",
    "                                                                    max_seq_length)\n",
    "    \n",
    "    # Fit it on a corpus of data\n",
    "    vectorization_layer.adapt(padded_tokenized_sentences)\n",
    "    \n",
    "    # Get the vocab size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "    \n",
    "    return vectorization_layer, n_vocab\n",
    "\n",
    "\n",
    "char_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, \n",
    "                                                                         max_seq_length, \n",
    "                                                                         max_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97130f9f-8504-46a0-98c9-c66f1357774a",
   "metadata": {},
   "source": [
    "Two key differences between the previous token-based text vectorizer and this char-based text vectorizer are in the input dimensions and the final output dimensions:\n",
    "\n",
    "- **Token-based vectorizer** – Takes in a `[batch_size, 1]`-sized input and produces a `[batch_size, sequence_length]`-sized output.\n",
    "\n",
    "- **Char-based vectorizer** – Takes in a `[batch_size, sequence_length, 1]`-sized input and produces a `[batch_size, sequence_length, token_length]`-sized output.\n",
    "\n",
    "Now we are equipped with the ingredients to implement our new and improved NER classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d88b42-d394-44e8-a75b-d83e635dbe4c",
   "metadata": {},
   "source": [
    "### Processing the inputs for the `char_vectorize_layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7a9c1-1af2-47d1-b845-f6e23980f8b8",
   "metadata": {},
   "source": [
    "To use the same `word_input` for the character vectorization layer, we need to introduce some interim pre-processing to get the i/p to the correct format as intentded for this layer as the input shape needs to be `[batch_size, sequence_length, 1]`-sized tensor.\n",
    "\n",
    "This means the sentences need to be tokenized to a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78a4197c-4a68-4bcd-adcd-d9c48e92a1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_word_input = layers.Lambda(lambda x: tf.strings.split(x).to_tensor(default_value='', \n",
    "                                                                             shape=[None, max_seq_length, 1])\n",
    "                       )(word_input)\n",
    "\n",
    "char_vectorized_out = char_vectorize_layer(tokenized_word_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7014a-54b1-4f50-ae6f-a6003eddacdc",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <b>Explantion of above, as given in book</b>\n",
    "    <img src='images/lambda.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f213b3da-0fce-4ce1-a61a-8f565f7fb50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Produces a [batch size, seq length, token_length, emb size]\n",
    "char_embedding_layer = layers.Embedding(input_dim=n_char_vocab, \n",
    "                                        output_dim=32, \n",
    "                                        mask_zero=True)(char_vectorized_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e385e16-9c6f-48b1-85fb-4408e139fffb",
   "metadata": {},
   "source": [
    "This layer produces a `[batch_size, sequence_length, token_length, 32]`-sized tensor, with a char embedding vector for each character in the tensor. \n",
    "\n",
    "Now it’s time to perform convolution on top of this output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abfef95-bf0e-427c-ad69-2ea9758adfbc",
   "metadata": {},
   "source": [
    "### Performing convolution on the character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567aad17-d209-41b6-b3bf-21f3505572c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A 1D convolutional layer that will generate token embeddings by shifting\n",
    "# a convolutional kernel over the sequence of chars in each token (padded)\n",
    "char_token_output = layers.Conv1D(filters=1, kernel_size=5, strides=1, padding='same',\n",
    "                                  activation='relu')(char_embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99820f-1d4e-489e-8cdc-42390baa3022",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <b>Explantion of above, as given in book</b>\n",
    "    <img src='images/conv1d.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2bd40f-e8ca-46bf-a465-1246b1c0c591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# There is an additional dimension of size 1 (out channel dimension) that\n",
    "# we need to remove\n",
    "char_token_output = layers.Lambda(lambda x: x[:, :, :, 0])(char_token_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2149f6cc-5d13-463f-8182-e5641ec123e1",
   "metadata": {},
   "source": [
    "To get the final output embedding (i.e. a combination of token- and character-based embeddings), we concatenate the two embeddings on the last axis. \n",
    "\n",
    "This would result in a 48 element-long vector (i.e. 32 element-long token embedding + 12 element-long char-based token embedding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab20d1d-79bd-4c1e-a34b-451fc6fe90d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate the token and char embeddings\n",
    "concat_embedding_out = layers.Concatenate()([token_embedding_out, char_token_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbce6a9-98e8-4a29-ab20-abc94b30d87e",
   "metadata": {},
   "source": [
    "### Defining a simple bidirectional RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d2a6e7-1169-401c-afa5-878d149acae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a simple bidirectional RNN layer, it returns an output at each position\n",
    "rnn_layer_1 = layers.Bidirectional(\n",
    "                     layers.SimpleRNN(units=64, activation='tanh', \n",
    "                                      return_sequences=True, \n",
    "                                      #kernel_regularizer='l2', recurrent_regularizer='l2',\n",
    "                                      #dropout=0.05, \n",
    "                                      recurrent_dropout=0.2,\n",
    "                                     )\n",
    "              )\n",
    "\n",
    "rnn_out_1 = rnn_layer_1(concat_embedding_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b5818-a270-4337-8fd3-139cb12d6aa3",
   "metadata": {},
   "source": [
    "seting `return_sequences=True`, which means it will produce an output at each time step, as opposed to only at the last time step. \n",
    "\n",
    "Next, we define the final Dense layer, which has `n_classes` output nodes (i.e. 9) and a `softmax` activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ad47c1b-51aa-4e61-aeb9-4c69aa765d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defines the final prediction layer\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29c61bd0-a058-441b-834d-5472f3c66c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining metric to handle class-imbalance\n",
    "\n",
    "def macro_accuracy(y_true, y_pred):\n",
    "    \n",
    "    #  [batch size, time] => [batch size * time]\n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n",
    "    \n",
    "    # [batch size, sequence length, n_classes] => [batch size * time]\n",
    "    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), 'int32')\n",
    "    \n",
    "    sorted_y_true = tf.sort(y_true)\n",
    "    sorted_inds = tf.argsort(y_true)\n",
    "    \n",
    "    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n",
    "    \n",
    "    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, sorted_y_pred), 'int32')\n",
    "    \n",
    "    # We are adding one to make sure there are no division by zero\n",
    "    correct_for_each_label = tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 'float32') + 1\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), sorted_y_true), 'float32') + 1\n",
    "    \n",
    "    mean_accuracy = tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "# mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy,\n",
    "#                                                           name='macro_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac9302c9-bd87-480f-8800-31ec7430f886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 40, 1)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " text_vectorization_1 (TextVect  (None, 40, 12)      0           ['lambda[0][0]']                 \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 40, 12, 32)   2752        ['text_vectorization_1[0][0]']   \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, 40)          0           ['input_1[0][0]']                \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 40, 12, 1)    161         ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 40, 64)       1512000     ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 40, 12)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 40, 76)       0           ['embedding[0][0]',              \n",
      "                                                                  'lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 40, 128)      18048       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 40, 9)        1161        ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,534,122\n",
      "Trainable params: 1,534,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "char_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    "\n",
    "# Define a macro accuracy measure\n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, \n",
    "                                                          name='macro_accuracy')\n",
    "\n",
    "# Compile the model with a loss optimizer and metrics\n",
    "char_token_embedding_rnn.compile(loss='sparse_categorical_crossentropy', \n",
    "                                 optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "# Summary of the model\n",
    "char_token_embedding_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bf9b2-e2d5-4d4d-8750-670024dde337",
   "metadata": {},
   "source": [
    "This is our final model. The key difference in this model compared to the previous solution is that it used two different embedding types. A standard token-based embedding layer and a complex, char-based embedding that was leveraged to generate token embeddings using the convolution operation. \n",
    "\n",
    "Now let’s train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396e6da-1520-4e2b-9c1d-1d51b8bab526",
   "metadata": {},
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5411d8f0-67f4-4a17-9638-885dcf1dbbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.17500000000000002, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298186, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "def get_label_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class label -> class ID mapping\n",
    "    labels_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    print(f\"labels_map: {labels_map}\")\n",
    "    return labels_map\n",
    "\n",
    "def get_class_weights(train_labels):\n",
    "    \n",
    "    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\n",
    "    label_count_ser = label_count_ser.sum()/label_count_ser\n",
    "    label_count_ser /= label_count_ser.max()\n",
    "    \n",
    "    label_id_map = get_label_id_map(train_labels)\n",
    "    label_count_ser.index = label_count_ser.index.map(label_id_map)\n",
    "    return label_count_ser.to_dict()\n",
    "\n",
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" From the class weights generate sample weights \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"\\nClass weights: {train_class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd127a3a-4812-4551-8cb4-56990de0eb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "220/220 [==============================] - 46s 185ms/step - loss: 0.0334 - macro_accuracy: 0.4616 - val_loss: 0.7106 - val_macro_accuracy: 0.6631\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 40s 181ms/step - loss: 0.0131 - macro_accuracy: 0.8472 - val_loss: 0.2568 - val_macro_accuracy: 0.8263\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 40s 182ms/step - loss: 0.0041 - macro_accuracy: 0.9567 - val_loss: 0.1131 - val_macro_accuracy: 0.8395\n"
     ]
    }
   ],
   "source": [
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "history = char_token_embedding_rnn.fit(train_sentences, padded_train_labels,\n",
    "                                       sample_weight=train_sample_weights,\n",
    "                                       batch_size=64,\n",
    "                                       epochs=3, \n",
    "                                       validation_data=(np.array(valid_sentences), padded_valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f6f85-082b-4dfa-8369-b60bf2b3c356",
   "metadata": {},
   "source": [
    "### Evaluate the model on test data\n",
    "\n",
    "Improvement over using simple embedding and SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809073d6-926b-4ec7-9637-28cfc25c7ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 2s 21ms/step - loss: 0.1131 - macro_accuracy: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11309965699911118, 0.806908369064331]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_token_embedding_rnn.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94bcea-9e68-4f6c-b985-5c4df8fca825",
   "metadata": {},
   "source": [
    "### Visually analysing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c390553-4506-483a-94bd-6912430e9b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 436ms/step\n",
      "Sample:\t SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
      "True:\t O O B-LOC O O O O B-LOC O O O O\n",
      "Pred:\t O O O O O O O B-LOC O O O O \n",
      "\n",
      "Sample:\t Nadim Ladki\n",
      "True:\t B-PER I-PER\n",
      "Pred:\t B-PER O \n",
      "\n",
      "Sample:\t AL-AIN , United Arab Emirates 1996-12-06\n",
      "True:\t B-LOC O B-LOC I-LOC I-LOC O\n",
      "Pred:\t B-LOC O B-LOC I-LOC I-LOC I-LOC \n",
      "\n",
      "Sample:\t Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
      "True:\t B-LOC O O O O O B-MISC I-MISC O O O O O O O B-LOC O O O O O O O O O\n",
      "Pred:\t B-LOC O O O O O B-MISC I-MISC I-MISC O O B-ORG O O O B-LOC O O O O O O O O O \n",
      "\n",
      "Sample:\t But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
      "True:\t O B-LOC O O O O O O O O O O O O O O O O O O O O O B-LOC O\n",
      "Pred:\t O B-LOC O O O O O O O O O O O O O O O O O O O O O O O \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "visual_test_sentences = test_sentences[:n_samples]\n",
    "visual_test_labels = padded_test_labels[:n_samples]\n",
    "\n",
    "visual_test_predictions = char_token_embedding_rnn.predict(np.array(visual_test_sentences))\n",
    "visual_test_pred_labels = np.argmax(visual_test_predictions, axis=-1)\n",
    "\n",
    "rev_labels_map = dict(zip(labels_map.values(), labels_map.keys()))\n",
    "\n",
    "for i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n",
    "    n_tokens = len(sentence.split())\n",
    "    print(\"Sample:\\t\", \" \".join(sentence.split()))\n",
    "    print(\"True:\\t\", \" \".join([rev_labels_map[i] for i in sent_labels[:n_tokens]]))\n",
    "    print(\"Pred:\\t\", \" \".join([rev_labels_map[i] for i in sent_preds[:n_tokens]]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de9e8f-392d-40fd-8240-fe3b54274ce7",
   "metadata": {},
   "source": [
    "## Other improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2ba29-10a3-4078-9b21-4543fc32f6c1",
   "metadata": {},
   "source": [
    "- **More RNN layers** – Adding more stacked RNN layers. By adding more hidden RNN layers, we can allow the model to learn more refined latent representations, leading to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1fee02c-491b-47af-bb59-d2b85c6e2016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rnn_layer_1 = layers.SimpleRNN(units=64, activation='tanh', \n",
    "#                                use_bias=True, return_sequences=True)\n",
    "\n",
    "# rnn_out_1 = rnn_layer_1(concat_embedding_out)\n",
    "\n",
    "# rnn_layer_2 = layers.SimpleRNN(units=32, activation='tanh', \n",
    "#                                use_bias=True, return_sequences=True)\n",
    "\n",
    "# rnn_out_2 = rnn_layer_1(rnn_out_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81484c6c-eb4d-46fc-8aae-941c891ce1ec",
   "metadata": {},
   "source": [
    "- **Make the RNN layer bidirectional** – The RNN models we discussed so far are uni-directional, i.e. looks at the sequence of text from forward to backward. However a different variant known as **bi-directional RNNs looks at the sequence in both directions, i.e. forward to backward and backward to forward**. This leads to better language understanding in models and inevitably better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d26405e-6ccc-45bd-9ea9-fa7d35946409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad61d0-eec3-4103-8c4a-91d56fe0a5ee",
   "metadata": {},
   "source": [
    "- **Incorporate regularization techniques** – You can leverage L2 regularization and dropout techniques to avoid overfitting and improve generalization of the model.\n",
    "\n",
    "- **Use early stopping and learning rate reduction to reduce overfitting** – During model training, use early stopping (i.e. training the model only until the validation accuracy is improving) and learning rate reduction (i.e. gradually reducing the learning rate over the epochs).\n",
    "\n",
    "* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
