{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c46ff5-f97b-46c7-8156-a26c287845a2",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER) with RNNs\n",
    "\n",
    "- [RNNs - StatQuest](https://youtu.be/AsNTP8Kwu80)\n",
    "- [LSTM - StatQuest](https://youtu.be/YCzL96nL7j0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08e14c-b84c-462a-84bb-40056c42d0bc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883fdc50-4c74-45eb-bb58-2c5c011a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from itertools import chain\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "seed = 54321\n",
    "\n",
    "# %env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b5282-19fc-44ff-ac3b-9f714874a9ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25b6013-11bf-4ad0-8048-e5bf850bbaea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded conllpp_dev.txt\n",
      "Downloaded conllpp_test.txt\n",
      "Downloaded conllpp_train.txt\n",
      "All files downloaded and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_data(url, local_directory):\n",
    "    try:\n",
    "        # Create the local directory if it doesn't exist\n",
    "        if not os.path.exists(local_directory):\n",
    "            os.makedirs(local_directory)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            for item in data:\n",
    "                if 'download_url' in item:\n",
    "                    file_name = item['name']\n",
    "                    download_url = item['download_url']\n",
    "                    local_path = os.path.join(local_directory, file_name)\n",
    "\n",
    "                    file_response = requests.get(download_url)\n",
    "                    if file_response.status_code == 200:\n",
    "                        with open(local_path, 'wb') as file:\n",
    "                            file.write(file_response.content)\n",
    "                        print(f\"Downloaded {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to download {file_name}\")\n",
    "        else:\n",
    "            print(\"Failed to fetch file list from GitHub API\")\n",
    "\n",
    "        print(\"All files downloaded and saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# Replace with your GitHub URL and local directory\n",
    "github_url = 'https://api.github.com/repos/ZihanWangKi/CrossWeigh/contents/data'\n",
    "local_directory = 'data'\n",
    "\n",
    "download_data(github_url, local_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a4229e1-c6a6-4c33-8194-4568024084af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
    "# dir_name = 'data'\n",
    "\n",
    "# def download_data(url, download_dir):\n",
    "#     # Create directories if doesn't exist\n",
    "#     os.makedirs(download_dir, exist_ok=True)\n",
    "    \n",
    "#     file_names = ['conllpp_train.txt', 'conllpp_dev.txt', 'conllpp_test.txt']\n",
    "    \n",
    "#     for filename in file_names:\n",
    "#         if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "#             filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "#         else:\n",
    "#             filepath = os.path.join(download_dir, filename)\n",
    "#         print(filepath)\n",
    "        \n",
    "#     return\n",
    "\n",
    "# download_data(url=url, download_dir=dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566ca31-0315-40ee-b3fd-db3d9934a34a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Read the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb239239-d0fa-4828-9129-75520347c7f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc418c-b504-4597-b63e-64032e4130f5",
   "metadata": {},
   "source": [
    "The document has a single word in each line along with the associated tags of that\n",
    "word. These tags are in the following order:\n",
    "\n",
    "1. The Part-of-speech (POS) tag (e.g. noun - `NN`, verb - `VB`, determinant - `DT`, etc.)\n",
    "2. Chunk tag – A chunk is a segment of text made of one or more tokens (for example, `NP` represents a noun phrase such as “The European Commission”)\n",
    "3. Named entity tag (e.g. Location, Organization, Person, etc.)\n",
    "\n",
    "Both chunk tags and named entity tags have a B- and I- prefix (e.g. B-ORG or I-ORG). These prefixes are there to differentiate the starting token of an entity/chunk from the continuing token of an entity/chunk. \n",
    "\n",
    "There are also five types of entities in the dataset:\n",
    "- Location-based entities (`LOC`)\n",
    "- Person-based entities (`PER`)\n",
    "- Organization-based entities (`ORG`)\n",
    "- Miscellaneous entities (`MISC`)\n",
    "- Non-entities (`O`)\n",
    "\n",
    "Finally, there’s an empty line between separate sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d69da93-bd5c-4ae0-9f89-986139a10fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    '''\n",
    "    Read data from a file with given filename\n",
    "    Returns a list of sentences (each sentence a string), \n",
    "    and list of ner labels for each string\n",
    "    '''\n",
    "\n",
    "    # print(\"Reading data ...\")\n",
    "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
    "    sentences, ner_labels = [], [] \n",
    "    \n",
    "    # Open the file\n",
    "    with open(filename,'r',encoding='latin-1') as f:        \n",
    "        # Read each line\n",
    "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
    "        \n",
    "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
    "        sentence_tokens = []\n",
    "        sentence_labels = []\n",
    "        i = 0\n",
    "        for row in f:\n",
    "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                is_sos = False\n",
    "            # Otherwise keep capturing tokens and labels\n",
    "            else:\n",
    "                is_sos = True\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token.strip())\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "            \n",
    "            # When we reach the end / or reach the beginning of next\n",
    "            # add the data to the master lists, flush the temporary one\n",
    "            if not is_sos and len(sentence_tokens)>0:\n",
    "                sentences.append(' '.join(sentence_tokens))\n",
    "                ner_labels.append(sentence_labels)\n",
    "                sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    # print('\\tDone')\n",
    "    return sentences, ner_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35a3fe4-cb7f-4e2f-9b62-ae34367b1d47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14041\n",
      "Valid size: 3250\n",
      "Test size: 3452\n",
      "\n",
      "Sample data\n",
      "\n",
      "Sentence: EU rejects German call to boycott British lamb .\n",
      "Labels: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "\n",
      "Sentence: Peter Blackburn\n",
      "Labels: ['B-PER', 'I-PER']\n",
      "\n",
      "Sentence: BRUSSELS 1996-08-22\n",
      "Labels: ['B-LOC', 'O']\n",
      "\n",
      "Sentence: The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .\n",
      "Labels: ['O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Sentence: Germany 's representative to the European Union 's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\n",
      "Labels: ['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_filepath = 'data\\conllpp_train.txt'\n",
    "train_sentences, train_labels = read_data(train_filepath) \n",
    "# Validation data\n",
    "dev_filepath = 'data\\conllpp_dev.txt'\n",
    "valid_sentences, valid_labels = read_data(dev_filepath) \n",
    "# Test data\n",
    "test_filepath = 'data\\conllpp_test.txt'\n",
    "test_sentences, test_labels = read_data(test_filepath) \n",
    "\n",
    "# Print some stats\n",
    "print(f\"Train size: {len(train_labels)}\")\n",
    "print(f\"Valid size: {len(valid_labels)}\")\n",
    "print(f\"Test size: {len(test_labels)}\")\n",
    "\n",
    "# Print some data\n",
    "print('\\nSample data\\n')\n",
    "for v_sent, v_labels in zip(train_sentences[:5], train_labels[:5]):\n",
    "    print(f\"Sentence: {v_sent}\")\n",
    "    print(f\"Labels: {v_labels}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd56818d-7271-4c64-adf8-b4b5faa28cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data_with_pandas(filename):\n",
    "    sentences, ner_labels = [], []\n",
    "    sentence_tokens, sentence_labels = [], []\n",
    "    \n",
    "    with open(filename, 'r', encoding='latin-1') as f:\n",
    "        for row in f:\n",
    "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
    "                if sentence_tokens:  # Ensure we don't append empty sentences\n",
    "                    sentences.append(' '.join(sentence_tokens))\n",
    "                    ner_labels.append(sentence_labels)\n",
    "                    sentence_tokens, sentence_labels = [], []\n",
    "            else:\n",
    "                token, _, _, ner_label = row.split(' ')\n",
    "                sentence_tokens.append(token)\n",
    "                sentence_labels.append(ner_label.strip())\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({'Sentence': sentences, 'NER_Labels': ner_labels})\n",
    "    return df\n",
    "\n",
    "# df = read_data_with_pandas(train_filepath)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61b640-5572-430a-8e85-ad3b87edb5d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Checking the balance of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba5bcd-9558-4eb2-aace-8b49da3d7c04",
   "metadata": {},
   "source": [
    "One of the unique characteristics of NER tasks is the class imbalance. That is, not all classes will have a roughly equal number of samples. As you can probably guess, in a corpus, there are more non-named entities than named entities. This leads to a significant class imbalance among labels. Therefore, let’s have a look at the distribution of samples among different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f3f053-b51f-4b3f-b8cf-1d62b2a08c68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data label counts\n",
      "O         169578\n",
      "B-LOC       7140\n",
      "B-PER       6600\n",
      "B-ORG       6321\n",
      "I-PER       4528\n",
      "I-ORG       3704\n",
      "B-MISC      3438\n",
      "I-LOC       1157\n",
      "I-MISC      1155\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation data label counts\n",
      "O         42759\n",
      "B-PER      1842\n",
      "B-LOC      1837\n",
      "B-ORG      1341\n",
      "I-PER      1307\n",
      "B-MISC      922\n",
      "I-ORG       751\n",
      "I-MISC      346\n",
      "I-LOC       257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test data label counts\n",
      "O         38143\n",
      "B-ORG      1714\n",
      "B-LOC      1645\n",
      "B-PER      1617\n",
      "I-PER      1161\n",
      "I-ORG       881\n",
      "B-MISC      722\n",
      "I-LOC       259\n",
      "I-MISC      252\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# As train_labels are list of lists, so \n",
    "# To create a flat list, we can use the chain() function \n",
    "\n",
    "# Print the value count for each label\n",
    "print(\"Training data label counts\")\n",
    "print(pd.Series(chain(*train_labels)).value_counts())\n",
    "\n",
    "print(\"\\nValidation data label counts\")\n",
    "print(pd.Series(chain(*valid_labels)).value_counts())\n",
    "\n",
    "print(\"\\nTest data label counts\")\n",
    "print(pd.Series(chain(*test_labels)).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d94e8c-4ac4-4222-ad12-0d643361f7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "As we can see, `O` labels are several magnitudes higher than the volume of other labels. \n",
    "\n",
    "We need to keep this in mind when training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808213f7-59ac-4ebf-9aee-7bfda7b74eda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analysing the sequence length\n",
    "\n",
    "Let's analyze the sequence length(i.e. number of tokens) of each sentence. We need this information later to pad our sentences to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7beed3-ecb1-4636-9842-12fb26976bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14041.000000\n",
       "mean        14.501887\n",
       "std         11.602756\n",
       "min          1.000000\n",
       "5%           2.000000\n",
       "50%         10.000000\n",
       "95%         37.000000\n",
       "max        113.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05,0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b544e1-60ea-4b59-9e3b-e7ee82f59a86",
   "metadata": {},
   "source": [
    "We can see that $95\\%$ of our sentences have $37$ tokens or less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e841b8-2611-434b-afc3-9fe69f1c1340",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Processing data\n",
    "\n",
    "**Padding/Truncating sentences to create arrays**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314049ec-b319-4e1c-b6f7-f11ba126f21b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now it’s time to process the data. \n",
    "\n",
    "- We will keep the sentences in the same format, i.e. a list of strings where each string represents a sentence. This is because we will integrate text processing right into our model (as opposed to doing it externally). \n",
    "\n",
    "- For labels, we have to do several changes. Remember labels are a list of lists, where the inner lists(of diff. lengths) represent labels for all the tokens in each sentence. Specifically we will do the following:\n",
    "    - Convert the class labels to class IDs\n",
    "    - Pad the sequences of labels to a specified maximum length\n",
    "    - Generate a mask that indicates the padded labels, so that we can use this information to disregard the padded labels during model training\n",
    "    \n",
    "    \n",
    "> *\"Generate a mask that indicates the padded labels, so that we can use this information to disregard the padded labels during model training\"* ***is referring to this mask creation process. It's about creating a mask that allows you to tell your model which parts of the input sequences are actual tokens and which parts are padding tokens, so that the padding tokens don't influence the learning process.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c88b1e-8d32-49b1-be86-75edbc871667",
   "metadata": {},
   "source": [
    "- In the context of Named Entity Recognition (NER), when you're processing sequences of words to label them with named entity tags, you often have sentences of varying lengths. To process these sentences in batches for training, you need to ensure that all sequences in a batch have the same length. This is where padding comes in - you add special padding tokens to sequences that are shorter than the maximum length in the batch.\n",
    "\n",
    "- However, you don't want the padding tokens to affect the learning of your model. They're not actual tokens, so you'd want your model to ignore them during the training process. This is where the concept of a \"mask\" comes in.\n",
    "\n",
    "- A mask is a binary sequence (0s and 1s) that has the same length as your sequences. It's used to indicate which positions in a sequence are actual tokens and which are padding tokens. A value of 1 in the mask indicates an actual token, while a value of 0 indicates a padding token.\n",
    "\n",
    "- So, when you're training your model, you'll use this mask to \"mask out\" the effects of the padding tokens. For example, during the calculation of the loss function, you can use the mask to set the loss to 0 for the positions that are padded, effectively disregarding them in the learning process. This is particularly useful when sequences have variable lengths, and you want to prevent padding tokens from affecting the gradients and learned representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8561b2-a05f-484f-8d7f-97ca82a7c9a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lable_id_map(train_labels):\n",
    "    # Get the unique list of labels\n",
    "    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n",
    "    # Create a class_label --> class_ID mapping\n",
    "    label_map = dict(zip(unique_train_labels, np.arange(unique_train_labels.shape[0])))\n",
    "    \n",
    "    print(\"label_map: {}\".format(label_map))\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e905480f-6c00-4cb8-8788-d4dd4a231862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
     ]
    }
   ],
   "source": [
    "labels_map = get_lable_id_map(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89fb3a4-c2a3-4548-b92c-ce4a97a43596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_padded_int_labels(labels:list[list[str]], labels_map:dict[str, int], \n",
    "                          max_seq_length:int, return_mask:bool = True):\n",
    "    \"\"\"\n",
    "    This function takes sequences of class labels and return sequences of padded \n",
    "    class IDs, with the option to return a mask indicating padded labels.\n",
    "    \n",
    "    This function takes the following arguments:\n",
    "        * labels (List[List[str]]) – A list of lists of strings, where each string is \n",
    "                                     a class label of the string type\n",
    "        \n",
    "        * labels_map (Dict[str, int]) – A dictionary mapping a string label to a \n",
    "                                        class ID of type integer\n",
    "        \n",
    "        * max_seq_length (int) – A maximum length to be padded to (longer sequences \n",
    "                                 will be truncated at this length)\n",
    "        \n",
    "        * return_mask (bool) – Whether to return the mask showing padded labels or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert string labels to integers\n",
    "    int_labels = [[labels_map[x] for x in one_seq] for one_seq in labels]\n",
    "    \n",
    "    # Pad sequences\n",
    "    if return_mask:\n",
    "        # If we return mask, we first pad with a special value (-1) and\n",
    "        # use that to create the mask and later replace -1 with 'O'\n",
    "        padded_labels = np.array(\n",
    "                           tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                                int_labels, maxlen=max_seq_length, padding='post',\n",
    "                                truncating='post', value=-1\n",
    "                           )\n",
    "                        )\n",
    "        # mask filter\n",
    "        mask_filter = (padded_labels != -1)\n",
    "        \n",
    "        # replace -1 with 'O' s ID\n",
    "        padded_labels[~mask_filter] = labels_map['O']\n",
    "        \n",
    "        return padded_labels, mask_filter.astype('int')\n",
    "    else:\n",
    "        # padded_labels = np.array(ner_pad_sequence_func(int_labels, \n",
    "        #                                                value=labels_map['O'])\n",
    "        #                         )\n",
    "        # return padded_labels\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fccd5-3f2b-4d37-bbda-bf06c8c8a722",
   "metadata": {},
   "source": [
    "**Explaining what happens after generating the padded_labels in the above code:**\n",
    "\n",
    "After getting the `padded_labels`, we can simply generate the mask as a boolean filter where padded_labels is not equal to -1. \n",
    "\n",
    "Thus, the positions where original labels exist will have a value of 1 and the rest will have 0 in the mask. \n",
    "\n",
    "However, we have to convert the -1 values to a class ID found in the `labels_map`. We will give them the class ID of the label `O` (i.e. others)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b21d8-3cdd-46d6-8f39-6b753c3905d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Processing the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05ff41-157d-4b18-bce3-d0b14c9880cd",
   "metadata": {},
   "source": [
    "Remember that the 95% percentile fell at the length of 37 words. So, let's set `max_seq_length = 40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff95469-1c94-4639-a29c-e09affe538a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_seq_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90aecf96-2ec1-4553-a4bf-6666238857e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14041, 40) (14041, 40)\n",
      "\n",
      "Lable Map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "\n",
      "Padded Label: [0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "Mask:\t [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert string labels to integers for all train/validation/test data\n",
    "# Pad train/validation/test data\n",
    "padded_train_labels, train_mask = get_padded_int_labels(train_labels, labels_map, \n",
    "                                                        max_seq_length, return_mask=True)\n",
    "\n",
    "padded_valid_labels, valid_mask = get_padded_int_labels(valid_labels, labels_map, \n",
    "                                                        max_seq_length, return_mask=True)\n",
    "\n",
    "padded_test_labels, test_mask  = get_padded_int_labels(test_labels, labels_map, \n",
    "                                                       max_seq_length, return_mask=True)\n",
    "\n",
    "print(padded_train_labels.shape, train_mask.shape)\n",
    "print(\"\\nLable Map:\", labels_map)\n",
    "print(\"\\nPadded Label:\",padded_train_labels[0])\n",
    "print(\"Mask:\\t\", train_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ee2af-5647-46a8-9a26-e2aa9ac518c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can see that the mask is indicating the true labels and padded ones clearly. `1` in the mask represents that in that position we have actual labels, while `0` in the mask represents that in those postions have been padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c352e-e738-41e3-b942-d2d27ced9034",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efa454-7e41-4e4f-95e8-8b60a781e4ed",
   "metadata": {},
   "source": [
    "- `max_seq_length` -  Denotes the maximum length for a sequence. We infer this from our training data during data exploration. It is important to have a reasonable length for sequences, as otherwise, memory can explode, due to the unrolling of the RNN.\n",
    "\n",
    "- `emedding_size` -  The dimensionality of token embeddings. Since we have a small corpus, a value < 100 will suffice.\n",
    "\n",
    "- `rnn_hidden_size` - The dimensionality of hidden layers in the RNN. Increasing dimensionality of the hidden layer usually leads to better performance. However, note that increasing the size of the hidden layer causes all three sets of internal weights (that is, U, W, and V) to increase as well, thus resulting in a high computational footprint.\n",
    "\n",
    "- `n_classes` -  Number of unique output classes present.\n",
    "\n",
    "- `batch_size` -  The batch size for training data, validation data, and test data. A higher batch size often leads to better results as we are seeing more data during each optimization step, but just like unrolling, this causes a higher memory requirement.\n",
    "\n",
    "- `epochs` -  The number of epochs to train the model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63cfcf1-09bd-4554-b733-cc3837ca69aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The maximum length of sequences\n",
    "max_seq_length = 40\n",
    "\n",
    "# Size of token embeddings\n",
    "embedding_size = 64\n",
    "\n",
    "# Number of hidden units in the RNN layer\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Number of output nodes in the last layer\n",
    "n_classes = len(labels_map)\n",
    "\n",
    "# Number of samples in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs to train\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093e46b-2b67-4eff-8573-0c528db969ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defining a Simple RNN Model\n",
    "\n",
    "Our model will have an embedding layer, followed by a simple RNN layer, and finally a dense prediction layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411120c-1a0d-4e68-9a74-e4527b701963",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/model_architecture.png' title='Model Architecture'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d38ad6-b46f-4141-89ae-95e31f5fc2ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Introduction to the [`TextVectorization` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)\n",
    "\n",
    "A preprocessing layer which maps text features to integer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b874b3-a475-4a43-a9b0-5dce9f1d35fc",
   "metadata": {},
   "source": [
    "#### Why not use an external `Tokenizer` !\n",
    "\n",
    "- One thing to note in the work we have done so far is that, unlike in previous chapters, we haven’t yet defined a Tokenizer object. \n",
    "\n",
    "- Although the `Tokenizer` has been an important part of our NLP pipeline to convert each token (or word) into an ID, there's a big downside to using an external tokenizer. \n",
    "\n",
    "- After training the model, if you forget to save the tokenizer along with the model, your machine learning model becomes useless: to combat this, during inference, you would need to map each word to the exact ID it was mapped to during training. \n",
    "\n",
    "- This is a significant risk the tokenizer poses. \n",
    "\n",
    "The `TextVectorization` layer can be thought of as a modernized tokenizer that can be plugged into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a799254-52a0-4ddb-8e89-f2879dd21e9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With default arguments\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2  3  8  7]\n",
      " [ 2  3  5 10  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n",
      "\n",
      "With limited vocabulary\n",
      "\n",
      "Data: \n",
      "[[1 4 1 2 3 1 1]\n",
      " [2 3 1 1 0 0 0]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went']\n",
      "--------------------------------------------------\n",
      "\n",
      "With preprocessing disabled\n",
      "\n",
      "Data: \n",
      "[[12  2  4  5  7  6 10]\n",
      " [ 9 11  3  8  0  0  0]]\n",
      "Vocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', 'empty.', 'The', 'Sunday', 'Market', 'I']\n",
      "--------------------------------------------------\n",
      "\n",
      "With a maximum sequence length\n",
      "\n",
      "Data: \n",
      "[[ 9  4  6  2]\n",
      " [ 2  3  5 10]]\n",
      "Vocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "toy_corpus = [\"I went to the market on Sunday\", \"The Market was empty.\"]\n",
    "toy_vectorization_layer = TextVectorization()\n",
    "\n",
    "# Fit it on a corpus of data\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "toy_vectorized_output = toy_vectorization_layer(toy_corpus)\n",
    "toy_vocabulary = toy_vectorization_layer.get_vocabulary()\n",
    "\n",
    "print(\"With default arguments\\n\")\n",
    "print(f\"Data: \\n{toy_vectorized_output}\")\n",
    "print(f\"Vocabulary: {toy_vocabulary}\")\n",
    "print('-'*50)\n",
    "\n",
    "# limit the size of the vocabulary\n",
    "toy_vectorization_layer = TextVectorization(max_tokens=5)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith limited vocabulary\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "#  skip the text pre-processing that happens within the layer, standardize=None\n",
    "toy_vectorization_layer = TextVectorization(standardize=None)\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith preprocessing disabled\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "# we can also control the padding/truncation of sequences with the output_sequence_length\n",
    "toy_vectorization_layer = TextVectorization(output_sequence_length=4) # pad/truncate sequences at length 4\n",
    "toy_vectorization_layer.adapt(toy_corpus)\n",
    "\n",
    "print(\"\\nWith a maximum sequence length\\n\")\n",
    "print(f\"Data: \\n{toy_vectorization_layer(toy_corpus)}\")\n",
    "print(f\"Vocabulary: {toy_vectorization_layer.get_vocabulary()}\")\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12e303-07d7-4acc-b3f5-eb7c34157558",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41e6af1-f365-428b-8996-3607947ce026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n",
    "    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n",
    "    \n",
    "    # Define the layer\n",
    "    vectorization_layer = TextVectorization(max_tokens=vocabulary_size, standardize=None,\n",
    "                                           output_sequence_length=max_seq_length)\n",
    "    \n",
    "    # Fit on the text corpus\n",
    "    vectorization_layer.adapt(corpus)\n",
    "    \n",
    "    # Get the vocabulary_size\n",
    "    n_vocab = len(vectorization_layer.get_vocabulary())\n",
    "    \n",
    "    return vectorization_layer, n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62c493-d5a0-4e3e-be59-19a942fd99c3",
   "metadata": {},
   "source": [
    ">✒️**NOTE:** Pay attention to the various arguments we have set for the vectorization layer. We are passing the vocabulary size as `max_tokens`; we are setting the `standardize` to None. **This is an important setting.** \n",
    "\n",
    "**When performing NER, keeping the case of characters is very important. Typically, an entity starts with an uppercase letter (e.g. the name of a person or organization). Therefore, we should preserve the case in the text.**\n",
    "\n",
    "Finally, we also set the `output_sequence_length` to the sequence length we found during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44a0b842-d14b-436d-aa52-9710e4d7bf70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000001E57F9BBC70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Input layer - that has a single column (i.e. each sentence represented as a single unit), thus shape=(1,)\n",
    "word_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text Vectorization layer\n",
    "vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(corpus=train_sentences, \n",
    "                                                                max_seq_length=max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "vectorized_out = vectorize_layer(word_input)\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=n_vocab, \n",
    "                                   output_dim=embedding_size,\n",
    "                                   mask_zero=True)(vectorized_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252d806-f79d-46b5-8aac-7fed7d8c6fe9",
   "metadata": {},
   "source": [
    "- `mask_true` argument in `layers.Embedding`: **Masking is used to mask uninformative words added to sequences (e.g. the padding token added to make sentences a fixed length), as they do not contribute to the final outcome.** \n",
    "\n",
    "- In the embedding layer `mask_true=True` is set, to ignore padded values (which will be zeros).\n",
    "\n",
    "* **\n",
    "From documentation:\n",
    "- `mask_zero` Boolean: \n",
    "    - Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is `True`, then all subsequent layers in the model need to support masking or an exception will be raised. \n",
    "    \n",
    "    - If `mask_zero` is set to `True`, as a consequence, index `0` cannot be used in the vocabulary (`input_dim` should equal size of `vocabulary + 1`).\n",
    "    \n",
    "\n",
    "        - In TensorFlow, most layers support masking.\n",
    "        \n",
    "* **\n",
    "\n",
    "- When you enable masking in a layer, it will propagate the mask to the downstream layers, flowing down until the loss computations.\n",
    "\n",
    "    - **In other words, you only need to enable masking at the start of the model (as we have done at the embedding layer) and the rest is taken care of by TensorFlow.**\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91725d2f-7296-4dff-9d93-550918a576f1",
   "metadata": {},
   "source": [
    "#### Time-Dimension in Sequences\n",
    "\n",
    "Until now, we dealt with feed-forward networks. Outputs of feed-forward networks did not have a time dimension. But if you look at the output from the `TextVectorization` layer, it will be a `[batch_size, sequence length]` - sized output. When this output goes through an `embedding_layer`, the output would be a `[batch size, sequence length, embedding size]`- shaped tensor. In other words, there is an additional time dimension included in the output of the embedding layer.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60ab83-d915-4219-abd6-21d6b226fbd6",
   "metadata": {},
   "source": [
    "#### Masking in Sequence Learning\n",
    "\n",
    "Masking is a commonly used technique in sequence learning.\n",
    "\n",
    "- Naturally, text has arbitrary lengths. For example, sentences in a corpus would have a wide variety of token lengths. But neural networks process tensors with fixed dimensions. \n",
    "\n",
    "- To bring arbitrary-length sentences to constant length, we pad these sequences with some special value (e.g. 0). \n",
    "\n",
    "- However, these padded values are synthetic, and only serve as a way to ensure the correct input shape. They should not contribute to the final loss or evaluation metrics. To ignore them during loss calculation and evaluation, \"masking\" is used. \n",
    "\n",
    "- The idea is to multiply the loss resulting from padded timesteps with a zero, essentially cutting them off from the final loss.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "532269aa-14ab-4d5f-bec3-de36353dfe0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Define a simple RNN layer\n",
    "rnn_layer = layers.SimpleRNN(units=rnn_hidden_size, \n",
    "                             return_sequences=True)\n",
    "\n",
    "rnn_out = rnn_layer(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf525b53-f8a4-4250-a466-e2434245c1b5",
   "metadata": {},
   "source": [
    "Here we pass two important arguments:\n",
    "- `units` (int) – This defines the hidden output size of the RNN model. The larger this is, the more representational power the model will have.\n",
    "\n",
    "- `return_sequences` (bool) – Whether to return outputs from all the timesteps, or to return only the last output. **For NER tasks, we need to label every single token. Therefore we need to return outputs for all the time steps.**\n",
    "\n",
    "The rnn_layer takes a `[batch size, sequence length, embedding size]` - sized tensor and returns a `[batch size, sequence length, rnn hidden size]` - sized tensor.\n",
    "\n",
    "Finally, the time-distributed output from the RNN will go to a Dense layer with `n_classes` output nodes and a `softmax` activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1547853-f1f8-4920-9a2f-2f12be945157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out)\n",
    "\n",
    "# Next define the model\n",
    "model = tf.keras.Model(inputs=word_input, outputs=dense_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5600dbcf-1c42-4c5b-b33a-3e9edeeffb5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation metrics & Loss function\n",
    "\n",
    "**Dealing with high class-imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55fe2b-df7c-44e9-9235-28862a4e5281",
   "metadata": {},
   "source": [
    "- As we saw in the following cell [Checking the balance of labels](#Checking-the-balance-of-labels), that **NER tasks carry high class imbalance.**\n",
    "\n",
    "- It is quite normal for text to have more non-entity-related tokens than entity-related tokens. This leads to large amounts of other (`O`) type labels and fewer of the remaining types. \n",
    "\n",
    "- We need to take this into consideration when training the model and evaluating the model. We can address the class imbalance in two ways:\n",
    "     1. We can create a new evaluation metric that is resilient to class imbalance.\n",
    "     \n",
    "     2. We can use sample weights to penalize more frequent classes and boost the importance of rare classes.\n",
    "\n",
    "\n",
    "- Let's look at the first one, a new evaluation metric\n",
    "\n",
    "    - We will define a modified version of the accuracy. This is called a **macro-averaged accuracy**. \n",
    "\n",
    "    - **In macro averaging, we compute accuracies for each class separately, and then average it. Therefore, the class imbalance is ignored when computing the accuracy.** \n",
    "    \n",
    "\n",
    "Below we define the function to compute `macro_accuracy` using a batch of true targets (y_true) and predictions (y_pred). `y_true.shape = [batch_size, sequence length]` and `y_pred.shape = [batch size, sequence length, n_classes]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3cdb33d-e5db-4289-8c08-307943e72134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def macro_accuracy(y_true, y_pred):\n",
    "    \n",
    "    #  [batch size, time] => [batch size * time]\n",
    "    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n",
    "    \n",
    "    # [batch size, sequence length, n_classes] => [batch size * time]\n",
    "    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), 'int32')\n",
    "    \n",
    "    sorted_y_true = tf.sort(y_true)\n",
    "    sorted_inds = tf.argsort(y_true)\n",
    "    \n",
    "    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n",
    "    \n",
    "    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, sorted_y_pred), 'int32')\n",
    "    \n",
    "    # We are adding one to make sure there are no division by zero\n",
    "    correct_for_each_label = tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), 'float32') + 1\n",
    "    all_for_each_label = tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), sorted_y_true), 'float32') + 1\n",
    "    \n",
    "    mean_accuracy = tf.reduce_mean(correct_for_each_label/all_for_each_label)\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "\n",
    "# mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy,\n",
    "#                                                           name='macro_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1af6318-6345-4e21-824f-342df5665c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 1, 1, 0])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.math.equal([1,1,0,0,2], [3,1,0,0,3]), 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b63a367-722d-474d-9a9f-a0045484ee83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7,  5, 13])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.segment_sum(data=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "            segment_ids= [0, 0, 0, 1, 1, 2, 3, 3]).numpy()\n",
    "# then the segment sum would be [0+1+2, 3+4, 5, 6+7] = [3, 7, 5, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e225ae-7f63-4f5d-89bc-f56a7e74c7ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align='center'><b>Explanation of macro_accuracy code from the book</b></div>\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/macro_acc.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72966206-0cf7-48d3-b73a-a67a59e21bb3",
   "metadata": {},
   "source": [
    "## Compile the Model & Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd9b72-6665-4496-b5f4-38b6f0e7ba72",
   "metadata": {},
   "source": [
    "**COMPLETE CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc5a39b2-8180-4a3f-a7e2-d7f9c14b3376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 40)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 40, 64)            1512000   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 40, 64)            8256      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40, 9)             585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,520,841\n",
      "Trainable params: 1,520,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Input layer - that has a single column (i.e. each sentence represented as a single unit), thus shape=(1,)\n",
    "word_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Text Vectorization layer\n",
    "vectorize_layer, n_vocab = get_fitted_token_vectorization_layer(corpus=train_sentences, \n",
    "                                                                max_seq_length=max_seq_length)\n",
    "\n",
    "# Vectorized output (each word mapped to an int ID)\n",
    "vectorized_out = vectorize_layer(word_input)\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=n_vocab, \n",
    "                                   output_dim=embedding_size,\n",
    "                                   mask_zero=True)(vectorized_out)\n",
    "\n",
    "#  Define a simple RNN layer\n",
    "rnn_layer = layers.SimpleRNN(units=rnn_hidden_size, \n",
    "                             return_sequences=True)\n",
    "\n",
    "rnn_out = rnn_layer(embedding_layer)\n",
    "\n",
    "dense_layer = layers.Dense(n_classes, activation='softmax')\n",
    "dense_out = dense_layer(rnn_out)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model(inputs=word_input, outputs=dense_out)\n",
    "\n",
    "# Defining the custom metric\n",
    "mean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, \n",
    "                                                          name='macro_accuracy')\n",
    "\n",
    "# Complie the model\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer='adam', metrics=[mean_accuracy_metric])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9d20f-902a-48c1-b9d5-bb39b2a4a187",
   "metadata": {},
   "source": [
    "## Training and evaluating RNN on NER task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826d243-3753-4e42-a706-5faafd6ab1ea",
   "metadata": {},
   "source": [
    "### Tackling Class Imbalance\n",
    "When training the model we will use `sample_weight` to counteract class-imbalance.\n",
    "\n",
    "To compute sample weights, we will first define a function called `get_class_weights()` that computes class_weights for each class. Next we will pass the class weights to another function, `get_sample_weights_from_class_weights()`, which will generate sample weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2dfb221-c43c-4a78-b552-ac7cb21fef89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Count of train labels\")\n",
    "# temp = pd.Series(chain(*train_labels)).value_counts()\n",
    "# print(temp)\n",
    "# print('-'*30, '\\n')\n",
    "\n",
    "# print('Getting class weights by dividing the\\nmin. label count with respective label count')\n",
    "# temp = temp.min()/temp\n",
    "# print(temp)\n",
    "# print('-'*30, '\\n')\n",
    "\n",
    "# print(f\"Lable Map: {labels_map}\")\n",
    "# print('-'*30, '\\n')\n",
    "\n",
    "# print(\"Mapping the label names using above label map\")\n",
    "# temp.index = temp.index.map(labels_map)\n",
    "# print(temp)\n",
    "\n",
    "# del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1fc1574-1141-4e5f-824b-b074c2c13024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n",
      "Class weights: {1: 0.006811025015037328, 5: 0.16176470588235295, 3: 0.175, 0: 0.18272425249169436, 4: 0.25507950530035334, 6: 0.31182505399568033, 2: 0.33595113438045376, 8: 0.9982713915298185, 7: 1.0}\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(train_labels):\n",
    "    \"\"\"\n",
    "    Class weight is calculated by min_label_count/label_count_of_that_class.\n",
    "    This way the minority class gets more weightage, thus tackling class imabalance\n",
    "    \"\"\"\n",
    "    label_count = pd.Series(chain(*train_labels)).value_counts()\n",
    "    label_count = label_count.min()/label_count\n",
    "    \n",
    "    label_id_map = get_lable_id_map(train_labels)\n",
    "    label_count.index = label_count.index.map(label_id_map)\n",
    "    return label_count.to_dict()\n",
    "\n",
    "def get_sample_weights_from_class_weights(labels, class_weights):\n",
    "    \"\"\" \n",
    "    - From the class weights generate sample weights.\n",
    "    \n",
    "    - This is simply mapping the class_weights with each training data sample.\n",
    "    \n",
    "    - The sample_weights will be the same shape as the train_labels as there’s \n",
    "      one weight for each sample.\n",
    "    \"\"\"\n",
    "    return np.vectorize(class_weights.get)(labels)\n",
    "\n",
    "\n",
    "train_class_weights = get_class_weights(train_labels)\n",
    "print(f\"Class weights: {train_class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e103cacf-e067-4cb8-a147-35a75d5c971b",
   "metadata": {},
   "source": [
    "You can see the class `Other(O)` has the lowest weight (because it’s the most frequent), and the class `I-MISC` has the highest as it’s the least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11deffd2-11d1-4eb8-9ba6-155062a0306c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "220/220 [==============================] - 24s 95ms/step - loss: 0.0298 - macro_accuracy: 0.5693 - val_loss: 0.4026 - val_macro_accuracy: 0.7124\n",
      "Epoch 2/3\n",
      "220/220 [==============================] - 20s 91ms/step - loss: 0.0090 - macro_accuracy: 0.8874 - val_loss: 0.1934 - val_macro_accuracy: 0.7996\n",
      "Epoch 3/3\n",
      "220/220 [==============================] - 20s 91ms/step - loss: 0.0030 - macro_accuracy: 0.9606 - val_loss: 0.1393 - val_macro_accuracy: 0.8108\n"
     ]
    }
   ],
   "source": [
    "# Make train_sequences an array\n",
    "train_sentences = np.array(train_sentences)\n",
    "\n",
    "# Get sample weights (we cannot use class_weight with TextVectorization layer)\n",
    "train_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, \n",
    "                                                             train_class_weights)\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(train_sentences, padded_train_labels,\n",
    "                    sample_weight=train_sample_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(np.array(valid_sentences), padded_valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2caef6c6-9100-4368-af19-b1d50f2faa0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 2s 17ms/step - loss: 0.1487 - macro_accuracy: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14868271350860596, 0.7812392115592957]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "model.evaluate(np.array(test_sentences), padded_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b4749-62d9-403a-a3e3-73b37d1ce646",
   "metadata": {},
   "source": [
    "We get an accuracy of around $96\\%$ on training data, $81\\%$ on validation data, $78\\%$ on testing data. Since the validation accuracy and test accuracy are on par, we can say that the model has generalized well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd62a1-e0f9-4954-a6b2-f34d9673e00a",
   "metadata": {},
   "source": [
    "## Visually analysing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8af67dc-cc1b-4fc6-89c5-d9e7666f55f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "Sample:\t SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .\n",
      "True:\t O O B-LOC O O O O B-LOC O O O O\n",
      "Pred:\t O O B-MISC I-MISC I-MISC O O B-ORG O B-MISC I-LOC O \n",
      "\n",
      "Sample:\t Nadim Ladki\n",
      "True:\t B-PER I-PER\n",
      "Pred:\t B-ORG I-ORG \n",
      "\n",
      "Sample:\t AL-AIN , United Arab Emirates 1996-12-06\n",
      "True:\t B-LOC O B-LOC I-LOC I-LOC O\n",
      "Pred:\t B-ORG O B-LOC I-LOC I-LOC I-LOC \n",
      "\n",
      "Sample:\t Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
      "True:\t B-LOC O O O O O B-MISC I-MISC O O O O O O O B-LOC O O O O O O O O O\n",
      "Pred:\t B-LOC I-LOC O O O O B-MISC I-MISC I-MISC O O O O O O B-LOC O O B-MISC O O O O O O \n",
      "\n",
      "Sample:\t But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
      "True:\t O B-LOC O O O O O O O O O O O O O O O O O O O O O B-LOC O\n",
      "Pred:\t O B-LOC O O O B-MISC O O O O O O O O O O O O O O O O O B-ORG O \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "visual_test_sentences = test_sentences[:n_samples]\n",
    "visual_test_labels = padded_test_labels[:n_samples]\n",
    "\n",
    "visual_test_predictions = model.predict(np.array(visual_test_sentences))\n",
    "visual_test_pred_labels = np.argmax(visual_test_predictions, axis=-1)\n",
    "\n",
    "rev_labels_map = dict(zip(labels_map.values(), labels_map.keys()))\n",
    "\n",
    "for i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n",
    "    n_tokens = len(sentence.split())\n",
    "    print(\"Sample:\\t\", \" \".join(sentence.split()))\n",
    "    print(\"True:\\t\", \" \".join([rev_labels_map[i] for i in sent_labels[:n_tokens]]))\n",
    "    print(\"Pred:\\t\", \" \".join([rev_labels_map[i] for i in sent_preds[:n_tokens]]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110fa044-aded-4b6a-a951-8bff8dc5a015",
   "metadata": {},
   "source": [
    "**It can be seen that our model is doing a decent job. It is good at identifying locations but is struggling at identifying the names of people.**\n",
    "\n",
    "* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
