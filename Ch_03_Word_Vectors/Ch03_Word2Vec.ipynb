{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd3e361-b5ca-4407-b4ff-2212ae28d27a",
   "metadata": {},
   "source": [
    "***Reference:***\n",
    "\n",
    "***Ganegedara, Thushan. Natural Language Processing with TensorFlow: The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition. Packt Publishing.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2316b-202e-45cb-a7a9-d279d595b085",
   "metadata": {},
   "source": [
    "# **Chapter 3: Word2Vec - Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214607b-9432-4425-8f3e-55828e700021",
   "metadata": {},
   "source": [
    "Word2Vec is a technique for numerical representation(vectors)  of words/tokens in a corpus of text. It captures the semantic and contextual information that the word carries.\n",
    "\n",
    "For e.g., the word *forest* and *oven* have very diff. vector representation as they are rarely used in similar contexts, while the words *forest* and *jungle* should be very similar.\n",
    "\n",
    "This chapter covers this information through the\n",
    "following main topics:\n",
    "- What is a word representation or meaning?\n",
    "- Classical approaches to learning word representations\n",
    "- Word2vec â€” a neural network-based approach to learning word representation\n",
    "- The skip-gram algorithm\n",
    "- The Continuous Bag-of-Words algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2bffc-c6e1-48a3-9d2e-a9cd037aba45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. What is a word representation or meaning?\n",
    "\n",
    "What is *word meanining*? : *meaning* is the idea conveyed by or some representation associated with the word.\n",
    "\n",
    "To achieve this, we will use algorithms that can analyze a given text corpus and come up with good numerical representations of words (that is, word embeddings) such that words that fall within similar contexts (for example, one and two, I and we) will have similar numerical representations compared to words that are unrelated (for example, cat and volcano)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e104d2-3a21-453f-8c87-3854ad26794e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Classical approaches to learning word representations\n",
    "\n",
    "- One-Hot Encoding \n",
    "- Term frequency-Inverse Document Frequency(TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8e08-a688-4123-ad8b-cc2302a2de55",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "One-hot encoding is also known as a localist representation (the opposite to the distributed representation), as the feature representation is decided by the activation of a single element in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea009e-f6cb-4b3c-93ef-151a9e3a51b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.3 TF-IDF Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b3e47-b013-4f99-8229-b777a164cc65",
   "metadata": {},
   "source": [
    "- **TF-IDF is a frequency-based method that takes into account the frequency with which a word appears in a corpus. This is a word representation in the sense that it represents the importance of a specific word in a given document. Intuitively, the higher the frequency of the word, the more important that word is in the document.**\n",
    "\n",
    "    - For example, in a document about cats, the word cats will appear more often than in a document that isn't about cats. \n",
    "    \n",
    "    - However, just calculating the equency woul not work because words such as this and is are very frequent in documents but do not contribute much information. TF- IDF takes this into consideration and gives values of near- zero for such common words.\n",
    "    \n",
    "\n",
    "- Again, **TF** stands for **term frequency** and **IDF** stands for **inverse document frequency:**\n",
    "\n",
    "    - $TF(w_i) = \\large{\\frac{\\text{No. of times } w_i \\text{ apear}}{\\text{Total No. of words}}}$\n",
    "    \n",
    "    - $IDF(w_i) = \\large{\\frac{\\text{Total No. of documents}}{\\text{No. of docs.  with }w_i \\text{ in it}}}$\n",
    "    \n",
    "    - $TF-IDF(w_i) = TF(w_i) \\times IDF(w_i)$\n",
    "    \n",
    "- E.g.: Therefore, the word **\"cats\"** is informative, while **\"this\"** is not. This is the desired behavior we needed in terms of measuring the importance of words.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/tfidf.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10facd-b15b-4c71-ad33-907f503946b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.4 Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b02f1e-51de-49b6-be9a-dab7fc492dab",
   "metadata": {},
   "source": [
    "Co-occurance matrix, unlike one-hot encoded representation, encode the context info. of words, but require a maintaining a $V \\times V$ matrix, where $V = \\text{vocaubalry size}$. \n",
    "\n",
    "To understand the co-occurance matrix, let's take two sentences:\n",
    "- *Jerry and Mary are friends.*\n",
    "- *Jerry buys flowers for Mary.*\n",
    "\n",
    "The co-occ. matrix will look like the foll. It's symmetrical:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/co_occ_matrix.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb3593-8580-433d-a0fb-eba97f7077f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Word2Vec - Intution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb05bff-b408-4cf6-9e91-2af21d47d926",
   "metadata": {},
   "source": [
    "**Syntax is the grammatical structure of the text, whereas Semantics is the meaning being conveyed.**\n",
    "\n",
    "To understand:\n",
    "- [Semantic & Syntactic Analysis - Blog-1](https://www.gnani.ai/resources/blogs/semantic-analysis-v-s-syntactic-analysis-in-nlp/)\n",
    "- [Syntactic & Semantic Analysis - Blog-2](https://builtin.com/data-science/introduction-nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0da0a4-f653-4ecd-89f0-9567b4aadab0",
   "metadata": {},
   "source": [
    "- **Word2vec is a groundbreaking approach that allows computers to learn the meaning of words without any human intervention. Also, Word2vec learns numerical representations of words by looking at the words surrounding a given word.**\n",
    "\n",
    "    - Above quote can be understood by the foll. e.g.: \"Mary is a very stubborn child. Her *previcacious* nature always gets her in trouble.\"\n",
    "\n",
    "    - We might not know what *previcacious* means, but by looking at the words that surround it like *stubborn, nature, trouble*, we can understand *previcacious* in fact means the state of being stubborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3825927-c79d-4e0a-9c5d-88b11cd943ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1 Basics of Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb0a5b-c204-4b71-a5bf-8dc8a63ea1c3",
   "metadata": {},
   "source": [
    "- As already mentioned, **Word2vec learns the meaning of a given word by looking at its context and representing it numerically.**\n",
    "\n",
    "    - **context** means fixed number of words in fornt of and behind the word of interest.\n",
    "    \n",
    "\n",
    "- Now, if we want to find a good algorithm that is capable of learning word meanings, **given a word, our algorithm should be able to predict the context words correctly.** \n",
    "\n",
    "    - This means that given a word $w_i$ the probability of *surrounding/context* words should be **high**: $$\\large{P(w_{i-m}, \\cdots, w_{i-1}, w_{i+1}, \\cdots, w_{i+m}|w_i) = \\prod_{j \\neq i \\wedge j=i-m}^{i+m} P(w_j|w_i)}$$\n",
    "    \n",
    "    - To arrive at the right-hand side of the equation, we need to assume that given the target word $(w_i)$, the context words are independent of each other (for example, $w_{i-2}$ and $w_{i-1}$ are independent). Though not entirely true, this approximation makes the learning problem practical and works well in practice. \n",
    "    \n",
    "Let's go through an example to understand the computations.\n",
    "\n",
    "**Exercise: does \"queen = king - he + she\"?** : See the book for explanation\n",
    "\n",
    "\n",
    "**In short maximizing the about probability leads to finding good meaning(or representation) of words, i.e. the Semantic structure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce6015-38c0-4ae4-8f06-55c860ef767a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. the Skip-gram Algorithm\n",
    "\n",
    "**The skip-gram algorithm, is an algorithm that exploit the context of the words in a written text to learn good word embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93dd0dd-2b88-4b9b-8da0-f3002fb1e0da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.1 Data Prep.: From raw text to semi-structured text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31e85e-5d48-4d27-9aa8-2e0bd1889dd3",
   "metadata": {},
   "source": [
    "First, we need to design a mechanism to extract a dataset that can be fed to our learning model. **Such a dataset should be a set of tuples of the format (target, context)**. Moreover, this needs to be created in an unsupervised manner. \n",
    "\n",
    "In summary, the data prep. process should do the foll:\n",
    "- Capture the surrounding words(context) of given word\n",
    "- Run in an unsupervised manner\n",
    "\n",
    "The skip-gram model uses the foll. approach to design a dataset:\n",
    "1. For a given word $w_i$, a context window of $m$ is assumed.\n",
    "\n",
    "    - By **context window size**, we mean # of words considered as context on either side of the target word.\n",
    "    \n",
    "    - So, for a word $w_i$, the context window(including the target word $w_i$) will be of size $2m+1$; $[w_{i-m}, \\cdots, w_{i-1}, w_i, w_{i+1}, \\cdots, w_{i+m}]$.<br></br>\n",
    "    \n",
    "2. Next, **(traget, context)** tuples are formed as: $[\\cdots, (w_i, w_{i-m}), \\cdots, (w_i, w_{i-1}), (w_i, w_{i+1}), \\cdots, (w_i, w_{i+m}), \\cdots]$; here, $m+1 \\leq i \\leq N-m$, and $N = \\text{# words in text corpus}$.\n",
    "\n",
    "E.g. : context window size(m) = 1\n",
    "> The dog barked at the mailman.\n",
    "\n",
    "For this example, the dataset would be as follows:\n",
    "> [(dog, The), (dog, barked), (barked, dog), (barked, at), ..., (the, at), (the, mailman)]\n",
    "\n",
    "Once the data is in the (target, context) format, we can use a\n",
    "neural network to learn the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e58fb-1eca-4672-8169-fae6950e4a5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.2 Understanding Skip-Gram Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8b72f-22c5-4f29-9f30-084ab043f1d8",
   "metadata": {},
   "source": [
    "#### Variables and Notations to learn the word embeddings\n",
    "\n",
    "\n",
    "- To store the embeddings, we need two $V \\times D$ matrices, $V = \\text{vocabulary size, } D = \\text{dimentionality of the word embeddings}$(i.e., the No. of elements in the vector that represents a single word).\n",
    "\n",
    "- **D** is a hyperparameter. The higher **D** is, the more expressive the word embeddings learned will be. \n",
    "\n",
    "- **We need two matrices, one to represent the context words and one to represent the target words.** \n",
    "    - These matrices will be referred to as the **context embedding space (or context embedding layer)** and,\n",
    "    - the **target embedding space (or target embedding layer)**, or in general as the embedding space (or the embedding layer).\n",
    "\n",
    "\n",
    "Each word will be represented with a unique ID in the range [1, V+ 1]. These IDs are passed to the embedding layer to look up corresponding vectors. To generate these IDs, we will use a special object called a Tokenizer that's available in TensorFlow.\n",
    "\n",
    "- Let's refer to an example target-context tuple $(w_i, w_j)$, where the target word ID is $w_i$, and one of the context words is $w_j$.\n",
    "\n",
    "- The corresponding target embedding of $w_i$ is $t_i$, and the corresponding context embedding of $w_i$ is $c_j$. \n",
    "\n",
    "- Each target-context tuple is accompanied by a label (O or 1), denoted by $y_i$, \n",
    "    - where true target-context pairs will get a label of 1, and\n",
    "    \n",
    "    - negative (or false) target-context candidates will get a label of O. \n",
    "    - It is easy to generate negative target-context candidates by sampling a word that does not appear in the context of a given target as the context word. We will talk about this in more detail later.\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c525e-7ddb-4281-b601-cfb7f6ad75d7",
   "metadata": {},
   "source": [
    "At this point, we have defined the necessary variables. \n",
    "\n",
    "- Next, for each input $w_i$, we will look up the embedding vectors from the context-embedding layer corresponding to the input. This operation provides us with $c_i$, which is a D-sized vector(i.e., a D-long embedding vector). \n",
    "\n",
    "- We do the same for the input $w_j$, using the context embedding space to retrieve $c_j.$\n",
    "\n",
    "- Afterward, we calculate the prediction output for $(w_i, w_j)$ using the following transformation:\n",
    "$$\\large{logit(w_i, w_j) = c_i \\cdot t_j}$$\n",
    "\n",
    "$$\\large{\\hat{y}_{ij} = sigmoid(logit(w_i, w_j))}$$\n",
    "\n",
    "- Here, $logit(w_i, w_j)$ represent the unnormalized scores(i.e., logits),\n",
    "\n",
    "- $\\hat{y}_i$ is a singled valued predicted output(representing the probability of context word belonging in the context of the target word).\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_1.png\"/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "- Using both the existing and derived entities, we can now use the cross-entropy loss function to calculate the loss for a given data point $[(w_i, w_j), y_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d1eb9-849a-44da-8db8-cd947b0a0e06",
   "metadata": {},
   "source": [
    "- **The Comceptual Skip-gram Model**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_2.png\"/>\n",
    "</div>\n",
    "\n",
    "- **The implementation of the skip-gram model**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_3.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffd149-8c21-427b-907c-12377f840382",
   "metadata": {},
   "source": [
    "## 5. Implementing Skip-gram with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40f25b-e763-4547-ac80-b8eefe31d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
