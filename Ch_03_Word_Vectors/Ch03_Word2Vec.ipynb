{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd3e361-b5ca-4407-b4ff-2212ae28d27a",
   "metadata": {},
   "source": [
    "***Reference:***\n",
    "\n",
    "***Ganegedara, Thushan. Natural Language Processing with TensorFlow: The definitive NLP book to implement the most sought-after machine learning models and tasks, 2nd Edition. Packt Publishing.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2316b-202e-45cb-a7a9-d279d595b085",
   "metadata": {},
   "source": [
    "# **Chapter 3: Word2Vec - Word Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214607b-9432-4425-8f3e-55828e700021",
   "metadata": {},
   "source": [
    "Word2Vec is a technique for numerical representation(vectors)  of words/tokens in a corpus of text. It captures the semantic and contextual information that the word carries.\n",
    "\n",
    "For e.g., the word *forest* and *oven* have very diff. vector representation as they are rarely used in similar contexts, while the words *forest* and *jungle* should be very similar.\n",
    "\n",
    "This chapter covers this information through the\n",
    "following main topics:\n",
    "- What is a word representation or meaning?\n",
    "- Classical approaches to learning word representations\n",
    "- Word2vec â€” a neural network-based approach to learning word representation\n",
    "- The skip-gram algorithm\n",
    "- The Continuous Bag-of-Words algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2bffc-c6e1-48a3-9d2e-a9cd037aba45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. What is a word representation or meaning?\n",
    "\n",
    "What is *word meanining*? : *meaning* is the idea conveyed by or some representation associated with the word.\n",
    "\n",
    "To achieve this, we will use algorithms that can analyze a given text corpus and come up with good numerical representations of words (that is, word embeddings) such that words that fall within similar contexts (for example, one and two, I and we) will have similar numerical representations compared to words that are unrelated (for example, cat and volcano)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e104d2-3a21-453f-8c87-3854ad26794e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Classical approaches to learning word representations\n",
    "\n",
    "- One-Hot Encoding \n",
    "- Term frequency-Inverse Document Frequency(TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db8e08-a688-4123-ad8b-cc2302a2de55",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "One-hot encoding is also known as a localist representation (the opposite to the distributed representation), as the feature representation is decided by the activation of a single element in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea009e-f6cb-4b3c-93ef-151a9e3a51b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.3 TF-IDF Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5b3e47-b013-4f99-8229-b777a164cc65",
   "metadata": {},
   "source": [
    "- **TF-IDF is a frequency-based method that takes into account the frequency with which a word appears in a corpus. This is a word representation in the sense that it represents the importance of a specific word in a given document. Intuitively, the higher the frequency of the word, the more important that word is in the document.**\n",
    "\n",
    "    - For example, in a document about cats, the word cats will appear more often than in a document that isn't about cats. \n",
    "    \n",
    "    - However, just calculating the equency woul not work because words such as this and is are very frequent in documents but do not contribute much information. TF- IDF takes this into consideration and gives values of near- zero for such common words.\n",
    "    \n",
    "\n",
    "- Again, **TF** stands for **term frequency** and **IDF** stands for **inverse document frequency:**\n",
    "\n",
    "    - $TF(w_i) = \\large{\\frac{\\text{No. of times } w_i \\text{ apear}}{\\text{Total No. of words}}}$\n",
    "    \n",
    "    - $IDF(w_i) = \\large{\\frac{\\text{Total No. of documents}}{\\text{No. of docs.  with }w_i \\text{ in it}}}$\n",
    "    \n",
    "    - $TF-IDF(w_i) = TF(w_i) \\times IDF(w_i)$\n",
    "    \n",
    "- E.g.: Therefore, the word **\"cats\"** is informative, while **\"this\"** is not. This is the desired behavior we needed in terms of measuring the importance of words.\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/tfidf.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10facd-b15b-4c71-ad33-907f503946b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.4 Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b02f1e-51de-49b6-be9a-dab7fc492dab",
   "metadata": {},
   "source": [
    "Co-occurance matrix, unlike one-hot encoded representation, encode the context info. of words, but require a maintaining a $V \\times V$ matrix, where $V = \\text{vocaubalry size}$. \n",
    "\n",
    "To understand the co-occurance matrix, let's take two sentences:\n",
    "- *Jerry and Mary are friends.*\n",
    "- *Jerry buys flowers for Mary.*\n",
    "\n",
    "The co-occ. matrix will look like the foll. It's symmetrical:\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/co_occ_matrix.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bb3593-8580-433d-a0fb-eba97f7077f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Word2Vec - Intution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb05bff-b408-4cf6-9e91-2af21d47d926",
   "metadata": {},
   "source": [
    "**Syntax is the grammatical structure of the text, whereas Semantics is the meaning being conveyed.**\n",
    "\n",
    "To understand:\n",
    "- [Semantic & Syntactic Analysis - Blog-1](https://www.gnani.ai/resources/blogs/semantic-analysis-v-s-syntactic-analysis-in-nlp/)\n",
    "- [Syntactic & Semantic Analysis - Blog-2](https://builtin.com/data-science/introduction-nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0da0a4-f653-4ecd-89f0-9567b4aadab0",
   "metadata": {},
   "source": [
    "- **Word2vec is a groundbreaking approach that allows computers to learn the meaning of words without any human intervention. Also, Word2vec learns numerical representations of words by looking at the words surrounding a given word.**\n",
    "\n",
    "    - Above quote can be understood by the foll. e.g.: \"Mary is a very stubborn child. Her *previcacious* nature always gets her in trouble.\"\n",
    "\n",
    "    - We might not know what *previcacious* means, but by looking at the words that surround it like *stubborn, nature, trouble*, we can understand *previcacious* in fact means the state of being stubborn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3825927-c79d-4e0a-9c5d-88b11cd943ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.1 Basics of Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb0a5b-c204-4b71-a5bf-8dc8a63ea1c3",
   "metadata": {},
   "source": [
    "- As already mentioned, **Word2vec learns the meaning of a given word by looking at its context and representing it numerically.**\n",
    "\n",
    "    - **context** means fixed number of words in fornt of and behind the word of interest.<br></br>\n",
    "\n",
    "- Now, if we want to find a good algorithm that is capable of learning word meanings, **given a word, our algorithm should be able to predict the context words correctly.** \n",
    "\n",
    "    - This means that given a word $w_i$ the probability of *surrounding/context* words should be **high**: $$\\large{P(w_{i-m}, \\cdots, w_{i-1}, w_{i+1}, \\cdots, w_{i+m}|w_i) = \\prod_{j \\neq i \\wedge j=i-m}^{i+m} P(w_j|w_i)}$$\n",
    "    \n",
    "    - To arrive at the right-hand side of the equation, we need to assume that given the target word $(w_i)$, the context words are independent of each other (for example, $w_{i-2}$ and $w_{i-1}$ are independent). Though not entirely true, this approximation makes the learning problem practical and works well in practice. \n",
    "    \n",
    "Let's go through an example to understand the computations.\n",
    "\n",
    "**Exercise: does \"queen = king - he + she\"?** : See the book for explanation\n",
    "\n",
    "\n",
    "**In short maximizing the about probability leads to finding good meaning(or representation) of words, i.e. the Semantic structure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce6015-38c0-4ae4-8f06-55c860ef767a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. the Skip-gram Algorithm\n",
    "\n",
    "**The skip-gram algorithm, is an algorithm that exploit the context of the words in a written text to learn good word embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93dd0dd-2b88-4b9b-8da0-f3002fb1e0da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.1 Data Prep.: From raw text to semi-structured text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31e85e-5d48-4d27-9aa8-2e0bd1889dd3",
   "metadata": {},
   "source": [
    "First, we need to design a mechanism to extract a dataset that can be fed to our learning model. **Such a dataset should be a set of tuples of the format (target, context)**. Moreover, this needs to be created in an unsupervised manner. \n",
    "\n",
    "In summary, the data prep. process should do the foll:\n",
    "- Capture the surrounding words(context) of given word\n",
    "- Run in an unsupervised manner\n",
    "\n",
    "The skip-gram model uses the foll. approach to design a dataset:\n",
    "1. For a given word $w_i$, a context window of $m$ is assumed.\n",
    "\n",
    "    - By **context window size**, we mean # of words considered as context on either side of the target word.\n",
    "    \n",
    "    - So, for a word $w_i$, the context window(including the target word $w_i$) will be of size $2m+1$; $[w_{i-m}, \\cdots, w_{i-1}, w_i, w_{i+1}, \\cdots, w_{i+m}]$.<br></br>\n",
    "    \n",
    "2. Next, **(traget, context)** tuples are formed as: $[\\cdots, (w_i, w_{i-m}), \\cdots, (w_i, w_{i-1}), (w_i, w_{i+1}), \\cdots, (w_i, w_{i+m}), \\cdots]$; here, $m+1 \\leq i \\leq N-m$, and $N = \\text{# words in text corpus}$.\n",
    "\n",
    "E.g. : context window size(m) = 1\n",
    "> The dog barked at the mailman.\n",
    "\n",
    "For this example, the dataset would be as follows:\n",
    "> [(dog, The), (dog, barked), (barked, dog), (barked, at), ..., (the, at), (the, mailman)]\n",
    "\n",
    "Once the data is in the (target, context) format, we can use a\n",
    "neural network to learn the word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e58fb-1eca-4672-8169-fae6950e4a5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.2 Understanding Skip-Gram Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8b72f-22c5-4f29-9f30-084ab043f1d8",
   "metadata": {},
   "source": [
    "#### ******* Variables and Notations to learn the word embeddings *******\n",
    "\n",
    "\n",
    "- To store the embeddings, we need two $V \\times D$ matrices, \n",
    "    - $V = \\text{vocabulary size, } D = \\text{dimentionality of the word embeddings}$(i.e., the No. of elements in the vector that represents a single word).<br></br>\n",
    "\n",
    "- **D** is a hyperparameter. The higher **D** is, the more expressive the word embeddings learned will be. \n",
    "\n",
    "- **We need two matrices, one to represent the context words and one to represent the target words.** \n",
    "    - These matrices will be referred to as the **context embedding space (or context embedding layer)** and,\n",
    "    - the **target embedding space (or target embedding layer)**, or in general as the embedding space (or the embedding layer).\n",
    "\n",
    "\n",
    "Each word will be represented with a unique ID in the range [1, V+1]. These IDs are passed to the embedding layer to look up corresponding vectors. To generate these IDs, we will use a special object called a Tokenizer that's available in TensorFlow.\n",
    "\n",
    "- Let's refer to an example target-context tuple $(w_i, w_j)$, where the target word ID is $w_i$, and one of the context words is $w_j$.\n",
    "\n",
    "- The corresponding target embedding of $w_i$ is $t_i$, and the corresponding context embedding of $w_i$ is $c_j$. \n",
    "\n",
    "- Each target-context tuple is accompanied by a label (O or 1), denoted by $y_i$, \n",
    "    - where true target-context pairs will get a label of 1, and\n",
    "    \n",
    "    - negative (or false) target-context candidates will get a label of O. \n",
    "    - It is easy to generate negative target-context candidates by sampling a word that does not appear in the context of a given target as the context word. We will talk about this in more detail later.\n",
    "    \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c525e-7ddb-4281-b601-cfb7f6ad75d7",
   "metadata": {},
   "source": [
    "At this point, we have defined the necessary variables. \n",
    "\n",
    "- Next, for each input $w_i$, we will look up the embedding vectors from the context-embedding layer corresponding to the input. This operation provides us with $c_i$, which is a D-sized vector(i.e., a D-long embedding vector). \n",
    "\n",
    "- We do the same for the input $w_j$, using the context embedding space to retrieve $c_j.$\n",
    "\n",
    "- Afterward, we calculate the prediction output for $(w_i, w_j)$ using the following transformation:\n",
    "$$\\large{logit(w_i, w_j) = c_i \\cdot t_j}$$\n",
    "\n",
    "$$\\large{\\hat{y}_{ij} = sigmoid(logit(w_i, w_j))}$$\n",
    "\n",
    "- Here, $logit(w_i, w_j)$ represent the unnormalized scores(i.e., logits),\n",
    "\n",
    "- $\\hat{y}_i$ is a singled valued predicted output(representing the probability of context word belonging in the context of the target word).\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_1.png\"/>\n",
    "</div>\n",
    "\n",
    "* **\n",
    "\n",
    "- Using both the existing and derived entities, we can now use the cross-entropy loss function to calculate the loss for a given data point $[(w_i, w_j), y_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d1eb9-849a-44da-8db8-cd947b0a0e06",
   "metadata": {},
   "source": [
    "- **The Comceptual Skip-gram Model**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_2.png\"/>\n",
    "</div>\n",
    "\n",
    "- **The implementation of the skip-gram model**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_3.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffd149-8c21-427b-907c-12377f840382",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. Implementing Data Generators with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c6eb6-c28a-4686-bd95-09df95e75f07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47555fac-98c4-40b7-9545-5717529a65ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:07.242171Z",
     "iopub.status.busy": "2023-06-25T08:29:07.241170Z",
     "iopub.status.idle": "2023-06-25T08:29:14.440388Z",
     "shell.execute_reply": "2023-06-25T08:29:14.440388Z",
     "shell.execute_reply.started": "2023-06-25T08:29:07.242171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import zipfile\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# !pip install adjustText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b306dad-ce52-4397-b116-f8210b4be815",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b655838-0d65-4497-81a2-465ccb7657d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:14.443015Z",
     "iopub.status.busy": "2023-06-25T08:29:14.442010Z",
     "iopub.status.idle": "2023-06-25T08:29:14.457366Z",
     "shell.execute_reply": "2023-06-25T08:29:14.456355Z",
     "shell.execute_reply.started": "2023-06-25T08:29:14.443015Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "  \n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    if not os.path.exists(extract_path):\n",
    "        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "  \n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76439e15-1837-4300-83d6-8ca49774d474",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Read Data without Preprocessing \n",
    "Reads data as it is to a string and tokenize it using spaces and returns a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3051b16-105a-4f98-9a09-ef1563590988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:14.458374Z",
     "iopub.status.busy": "2023-06-25T08:29:14.458374Z",
     "iopub.status.idle": "2023-06-25T08:29:25.322117Z",
     "shell.execute_reply": "2023-06-25T08:29:25.321108Z",
     "shell.execute_reply.started": "2023-06-25T08:29:14.458374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "\n",
      "Detected 2226 stories\n",
      "865589 words found in the total news set\n",
      "Example words (start):  Ad sales boost Time Warner profit  Quarterly profi\n",
      "Example words (end):  Online was the game, ahhhh them was the days ! LOL\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []\n",
    "    \n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            # We don't read the readme file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            \n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            #print(\".\"*i, f, end='\\r')\n",
    "            \n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
    "                \n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in f:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)                        \n",
    "                # Add that to the list\n",
    "                news_stories.append(story)  \n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories\n",
    "                \n",
    "\n",
    "news_stories = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f40bc2-5779-4a90-a11e-1ad342cc1e27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9510e2f2-9138-412c-89f2-450b819ff435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:25.324120Z",
     "iopub.status.busy": "2023-06-25T08:29:25.324120Z",
     "iopub.status.idle": "2023-06-25T08:29:25.839809Z",
     "shell.execute_reply": "2023-06-25T08:29:25.838792Z",
     "shell.execute_reply.started": "2023-06-25T08:29:25.324120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None, # None means all the words will be used for the vocabulary \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # default\n",
    "                      lower=True,\n",
    "                      split=' ',)\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa311ab7-3b9c-46ec-9dfb-099925763102",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Exploring the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0c854-232b-4c6b-8836-5439273f7d64",
   "metadata": {},
   "source": [
    "Once it has been fitted, the Tokenizer will have two important attributes populated: `word_index` and `index_word`. \n",
    "- `word_index` is a dictionary that maps each word to a unique ID. \n",
    "- `index_word` attribute is the opposite of `word_index`, that is, a dictionary that maps each unique word ID to the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31f92e9-6490-4f0e-bffd-800e63c8d56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:25.840820Z",
     "iopub.status.busy": "2023-06-25T08:29:25.840820Z",
     "iopub.status.idle": "2023-06-25T08:29:25.870582Z",
     "shell.execute_reply": "2023-06-25T08:29:25.870582Z",
     "shell.execute_reply.started": "2023-06-25T08:29:25.840820Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad': 21,\n",
       " 'sales': 498,\n",
       " 'boost': 158,\n",
       " 'time': 1324,\n",
       " 'warner': 28,\n",
       " 'profit': 114,\n",
       " 'quarterly': 23,\n",
       " 'profits': 182,\n",
       " 'at': 4640,\n",
       " 'us': 1954}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "word_count = tokenizer.word_counts # count of all words in corpus\n",
    "dict(itertools.islice(dict(word_count).items(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5af72-2f4d-4628-bb7e-bb302f25aabe",
   "metadata": {},
   "source": [
    "> **NOTE:** For vocab. size we are using `word_index` dict. We need to add additional 1 as the ID 0 is a reserved ID and will not be used for any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afcb9ba3-ee3d-467d-9d33-2548b5389ef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:25.872021Z",
     "iopub.status.busy": "2023-06-25T08:29:25.872021Z",
     "iopub.status.idle": "2023-06-25T08:29:25.885871Z",
     "shell.execute_reply": "2023-06-25T08:29:25.885871Z",
     "shell.execute_reply.started": "2023-06-25T08:29:25.872021Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32360\n",
      "\n",
      "Words at the top\n",
      "{'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
      "\n",
      "Words at the bottom\n",
      "{'counsellor': 32350, \"'frag'\": 32351, 'relasing': 32352, \"'real'\": 32353, 'hrs': 32354, 'enviroment': 32355, 'trifling': 32356, '24hours': 32357, 'ahhhh': 32358, 'lol': 32359}\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(tokenizer.word_index.items()) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")\n",
    "\n",
    "print(\"\\nWords at the top\")\n",
    "print(dict(list(tokenizer.word_index.items())[:10]))\n",
    "print(\"\\nWords at the bottom\")\n",
    "print(dict(list(tokenizer.word_index.items())[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af5ffe-4640-4a1d-a8a1-14252424957c",
   "metadata": {},
   "source": [
    "The more frequent a word is in the corpus, the lower the ID will be. Words such as \"the\", \"to\" and \"of\" which tend to be common (and are called **stop words**) are in fact the most common words. As the next step, we are going to refine our Tokenizer object to have a limited-sized vocabulary. Because we are working with a relatively small corpus, we have to make sure the vocabulary is not too large, as it can lead to poorly learned word vectors due to the lack of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdc5c76-e840-4e38-9ac3-bff114050c7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build a Tokenizer (Refined)\n",
    "Here, we will restrict the vocabulary to 15,000 words and eleminate words except the first most common 15000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902586d6-508b-4820-82f7-29169f9850e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:25.888486Z",
     "iopub.status.busy": "2023-06-25T08:29:25.887366Z",
     "iopub.status.idle": "2023-06-25T08:29:26.402317Z",
     "shell.execute_reply": "2023-06-25T08:29:26.402317Z",
     "shell.execute_reply.started": "2023-06-25T08:29:25.888486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fitted on the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n_vocab = 15000 + 1\n",
    "tokenizer = Tokenizer(num_words=n_vocab - 1, \n",
    "                      oov_token='')\n",
    "\n",
    "tokenizer.fit_on_texts(news_stories)\n",
    "print(\"Data fitted on the tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cd32c-af5c-41aa-8bc6-1d29557f5624",
   "metadata": {},
   "source": [
    "```\n",
    "oov_token: \n",
    "if given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "```\n",
    "\n",
    "Since we have a total vocabulary of more than 30,000 words, we'll restrict the size of the vocabulary to 15,000. This means the Tokenizer will only keep the most common 15,OOO words as the vocabulary. \n",
    "\n",
    ">When we restrict a vocabulary this way, a new problem arises. As the Tokenizer's vocabulary does not encompass all possible words in the true vocabulary, out-of-vocabulary words (or OOV words) can rear their heads. \n",
    "\n",
    "Some solutions are to replace OOV words with a special token (such as < UNK >) or remove them from the corpus. This is possible by passing the string you want to replace OOV tokens with to the oov_token argument in the Tokenizer. \n",
    "\n",
    "In this case, we are removing OOV words.\n",
    ">If we are careful when setting the size of the vocabulary, omitting some of the rare words would not harm learning the context of words accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5524fc-5546-4441-8371-df6a6b5e211e",
   "metadata": {},
   "source": [
    "#### Checking the results of the tokenizer\n",
    "\n",
    "We can call the `tokenizer`'s `texts_to_sequences()` **method to convert a list of documents(where each document is a string) to a list of list of word IDs(i.e., each document is converted to a list of word IDs).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca489af-bb94-432b-a77e-e17c94c440b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.405098Z",
     "iopub.status.busy": "2023-06-25T08:29:26.404088Z",
     "iopub.status.idle": "2023-06-25T08:29:26.418945Z",
     "shell.execute_reply": "2023-06-25T08:29:26.417859Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.405098Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.1\n",
      "\n",
      "Sequence IDs:\n",
      "[4092, 186, 712, 66, 3303, 997, 3827, 607, 21, 49, 302, 713, 5322, 2911, 5093, 3, 108, 108]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original:\\n{news_stories[0][:100]}\")\n",
    "print(f\"\\nSequence IDs:\\n{tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897a995-111e-4c09-bdc5-a5578a3f4ff4",
   "metadata": {},
   "source": [
    "#### Converting all articles to word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33166618-9a92-4f6e-be3e-d6603f7dae72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.420232Z",
     "iopub.status.busy": "2023-06-25T08:29:26.418945Z",
     "iopub.status.idle": "2023-06-25T08:29:26.871112Z",
     "shell.execute_reply": "2023-06-25T08:29:26.869935Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.420232Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_sequences = tokenizer.texts_to_sequences(news_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa8e4ce-ed40-4618-af99-ea9a08294243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.873623Z",
     "iopub.status.busy": "2023-06-25T08:29:26.873623Z",
     "iopub.status.idle": "2023-06-25T08:29:26.885279Z",
     "shell.execute_reply": "2023-06-25T08:29:26.885279Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.873623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2226, 437, 388)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_sequences), len(news_sequences[0]), len(news_sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a2c7e-f341-47bc-b741-05728ada7c0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generating skip-grams word pairs from the corpus\n",
    "In TensorFlow you have the convenient `tf.keras.preprocessing.sequence.skipgrams()` function to generate skipgrams word pairs.\n",
    "\n",
    "This function transforms a sequence of word indexes (list of integers) into tuples of words of the form:\n",
    "\n",
    "- (word, word in the same window), with label 1 (positive samples).\n",
    "- (word, random word from the vocabulary), with label 0 (negative samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9593f129-d787-40f3-a1ce-bdce6d5ef960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.886453Z",
     "iopub.status.busy": "2023-06-25T08:29:26.886453Z",
     "iopub.status.idle": "2023-06-25T08:29:26.901983Z",
     "shell.execute_reply": "2023-06-25T08:29:26.900979Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.886453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Phrase: ad sales boost time warner\n",
      "Sample Word IDs: [4092, 186, 712, 66, 3303]\n"
     ]
    }
   ],
   "source": [
    "# Using first 5 words of first article in the dataset as a sample phrase\n",
    "\n",
    "sample_word_ids = news_sequences[0][:5]\n",
    "sample_phrase = \" \".join(tokenizer.index_word[w_id] for w_id in sample_word_ids)\n",
    "\n",
    "print(\"Sample Phrase: %s\"%sample_phrase)\n",
    "print(\"Sample Word IDs: %s\" %sample_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d09ee19-8c09-40b1-859b-06c9744f6d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.903981Z",
     "iopub.status.busy": "2023-06-25T08:29:26.902982Z",
     "iopub.status.idle": "2023-06-25T08:29:26.917247Z",
     "shell.execute_reply": "2023-06-25T08:29:26.916188Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.903981Z"
    }
   },
   "outputs": [],
   "source": [
    "window_size = 1 # How many words to consider left and right.\n",
    "\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                    sequence=sample_word_ids,\n",
    "                    vocabulary_size=n_vocab,\n",
    "                    window_size=window_size,\n",
    "                    negative_samples=1.0, \n",
    "                    shuffle=False,\n",
    "                    categorical=False,\n",
    "                    sampling_table=None,\n",
    "                    seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20113799-2937-4d25-a5bb-3f341972a024",
   "metadata": {},
   "source": [
    "> **When run, above function will output data in the exact format we need the data in, i.e., (target-context) tuples as inputs and corresponding labels (O or 1) as outputs.**\n",
    "\n",
    "* **\n",
    "\n",
    "- `negative_samples` (`int`),â€” Fraction of negative candidates to generate. For example, a value of 1 means there will be an equal number of positive and negative skipgram candidates. A value of O means there will not be any negative candidates.\n",
    "\n",
    "- `shuffle` (`bool`) â€” Whether to shuffle the generated inputs or not.\n",
    "\n",
    "- `categorical` (`bool`) â€” Whether to produce labels as categorical (that is, one-hot encoded) or integers.\n",
    "\n",
    "- `sampling_table` (`np.ndarray`) - An array of the same size as the vocabulary. An element in a given position in the array represents the probability of sampling the word indexed by that position in the Tokenizer's word ID to word mapping. As we will see soon, this is a handy way to avoid common uninformative words being over- sampled much.\n",
    "\n",
    "- `seed` â€” If shuffling is enabled, this is the random seed to be used for shuffling.\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f15bca-ee37-481b-bd93-103ee714e3ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.918476Z",
     "iopub.status.busy": "2023-06-25T08:29:26.917247Z",
     "iopub.status.idle": "2023-06-25T08:29:26.933265Z",
     "shell.execute_reply": "2023-06-25T08:29:26.932194Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.917247Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4092, 186], 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (target-context) tuples and labels\n",
    "inputs[0], labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941eada7-7c49-4ef1-a4fc-adfabc46543e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.934466Z",
     "iopub.status.busy": "2023-06-25T08:29:26.933265Z",
     "iopub.status.idle": "2023-06-25T08:29:26.946931Z",
     "shell.execute_reply": "2023-06-25T08:29:26.946931Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.934466Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Phrase: ad sales boost time warner\n",
      "\n",
      "Sample skip-grams: (target-context) tuples\n",
      "\n",
      "Input: [4092, 186] (['ad', 'sales']) / Label: 1\n",
      "Input: [186, 4092] (['sales', 'ad']) / Label: 1\n",
      "Input: [186, 712] (['sales', 'boost']) / Label: 1\n",
      "Input: [712, 186] (['boost', 'sales']) / Label: 1\n",
      "Input: [712, 66] (['boost', 'time']) / Label: 1\n",
      "Input: [66, 712] (['time', 'boost']) / Label: 1\n",
      "Input: [66, 3303] (['time', 'warner']) / Label: 1\n",
      "Input: [3303, 66] (['warner', 'time']) / Label: 1\n",
      "Input: [186, 13277] (['sales', 'empower']) / Label: 0\n",
      "Input: [712, 3331] (['boost', 'rely']) / Label: 0\n",
      "Input: [3303, 12550] (['warner', 'stride']) / Label: 0\n",
      "Input: [712, 13684] (['boost', 'ipc']) / Label: 0\n",
      "Input: [4092, 589] (['ad', '2001']) / Label: 0\n",
      "Input: [186, 2906] (['sales', 'cd']) / Label: 0\n",
      "Input: [66, 2816] (['time', 'bmw']) / Label: 0\n",
      "Input: [66, 12746] (['time', 'matfield']) / Label: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Phrase: %s\\n\"%sample_phrase)\n",
    "print(\"Sample skip-grams: (target-context) tuples\\n\")\n",
    "\n",
    "for inp, lbl in zip(inputs, labels):\n",
    "    print(f\"Input: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52838c-4385-436b-803b-370f4c20f026",
   "metadata": {},
   "source": [
    "For example, \n",
    "\n",
    "since the word `\"sales\"` appears in the context of the word `\"ad\"`, it is considered a `positive candidate`. \n",
    "\n",
    "On the other hand, since the word `\"raul\"`(**randomly sampled from the vocabulary**) does not appear in the context of the word `\"ad\"`, it is added as a `negative candidate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91ca9f-105a-43de-843b-0cdcff28b49d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generating negative candidates\n",
    "\n",
    "**Word2vec algorithms rely on negative candidates to understand words that do not appear in the context of a given target word.**\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1947e-333c-4f63-8778-5c2d9d5d3473",
   "metadata": {},
   "source": [
    "- **When selecting negative candidates, the `skipgrams()` function selects them randomly, giving uniform weights to all the words in the vocabulary.** \n",
    "\n",
    "- **However, the original paper explains that this can lead to poor performance.** A better strategy is to use the **unigram distribution** as a prior for selecting negative context words. \n",
    "\n",
    "- **Unigram distribution:** *It represents the frequency counts of unigrams(or tokens) found in the text. Then the frequency counts are easily converted to probabilities(or normalized frequencies) by dividing them by the sum of all frequencies.* \n",
    "\n",
    "<!-- - The most amazing thing is that you don't have to compute this by hand for every corpus of text! **It turns out that if you take any suffciently large corpus of text, compute the normalized frequencies of unigrams, and order them from high to low, you'll see that the corpus approximately follows a certain constant distribution.** \n",
    "\n",
    "For the word with rank `math` in a corpus of `math` unigrams, the normalized frequency $f_k$ is given by: $$f_k = \\frac{k^{-s}}{\\sum_{i=1}^{N}n^{-s}}$$ -->\n",
    "\n",
    "\n",
    "- To generate negative candidates we can use `tf.random.log_uniform_candidate_sampler()` function. \n",
    "\n",
    "    - This function takes a batch of +ve context candidates of shape [b, num_true], where `b = batch_size` and `num_true = # of true candidates per example(1 for skip-gram model)`.\n",
    "    \n",
    "    - It outputs a [`num_sampled`] sized array, where `num_sampled = # of -ve samples we need`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13f21be-3c68-474e-9a10-0b76d72f581c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:26.949536Z",
     "iopub.status.busy": "2023-06-25T08:29:26.948436Z",
     "iopub.status.idle": "2023-06-25T08:29:30.903608Z",
     "shell.execute_reply": "2023-06-25T08:29:30.903608Z",
     "shell.execute_reply.started": "2023-06-25T08:29:26.949536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sample: [[186]]\n",
      "Negative samples:\n",
      "[ 176 1529  689    0   25 1362 1329    5   89 6569]\n",
      "true_expected_count:\n",
      "[[0.0060841]]\n",
      "sampled_expected_count:\n",
      "[6.42586965e-03 7.47171289e-04 1.65542832e-03 5.60863376e-01\n",
      " 4.23351340e-02 8.38648994e-04 8.59441701e-04 1.62862480e-01\n",
      " 1.25679085e-02 1.74087749e-04]\n"
     ]
    }
   ],
   "source": [
    "# generating positive candidates\n",
    "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
    "    sequence=sample_word_ids,\n",
    "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
    "    window_size=window_size,\n",
    "    negative_samples=0, # Only +ve samples\n",
    "    shuffle=False)\n",
    "\n",
    "inputs, labels = np.array(inputs), np.array(labels)\n",
    "\n",
    "\n",
    "# generate negative candidates [for a single word]\n",
    "negative_sampling_candidates,true_expected_count,sampled_expected_count =\\\n",
    "tf.random.log_uniform_candidate_sampler(\n",
    "    # A true context word that appears in the context of the target\n",
    "    true_classes=inputs[:1, 1:], # [b, 1] sized tensor\n",
    "    num_true=1, # number of true words per example\n",
    "    num_sampled=10,\n",
    "    unique=True,\n",
    "    range_max=n_vocab,\n",
    "    name=\"negative_sampling\"\n",
    ")\n",
    "\n",
    "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
    "print(f\"Negative samples:\\n{negative_sampling_candidates}\")\n",
    "print(f\"true_expected_count:\\n{true_expected_count}\")\n",
    "print(f\"sampled_expected_count:\\n{sampled_expected_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f997f29-d5ba-43bb-bf5c-b34318ea2f88",
   "metadata": {},
   "source": [
    "`tf.random.log_uniform_candidate_sampler()` takes:\n",
    "\n",
    "- `true_classes` â€” A tensor containing true target words. This needs to be a `[b, num_true]` sized array, where `num_true` denotes the number of true context candidates per example. Since we have one context word per example, this is 1.\n",
    "\n",
    "- `num_true` â€” The number of true context terms per example.\n",
    "\n",
    "- `num_sampled` ( int â€” The number of negative samples to generate.\n",
    "\n",
    "- `unique` â€” Whether to generate unique samples or with replacement.\n",
    "\n",
    "- `range_max` â€” The size of the vocabulary.\n",
    "\n",
    "It returns:\n",
    "\n",
    "- `sampled_candidates` â€” A tensor of size [num sampled] containing negative candidates\n",
    "\n",
    "- `true_expected_count` â€” A tensor of size [b, num_true]; the probability of each true candidate being sampled (according to Zipf's law)\n",
    "\n",
    "- `sampled_expected_count` â€” A tensor of size [`num_sampled`]; the probabilities of each negative sample occurring along with true candidates, if sampled from the corpus.\n",
    "\n",
    "\n",
    "We will not worry too much about the latter two entities. **The most important to us is `sampled_candidates`. When calling the function, we have to make sure `true_classes` has the shape `[b, num_true]`.**\n",
    "\n",
    "In our case, we will run this for a single word_ID, which will have shape [1,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbdc696-1385-4cfc-a106-79dad19d222d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generating data (positive + negative candidates)\n",
    "\n",
    "Now, putting everything together, let's write a data generator function that generates batches of data for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e932a-4b59-4888-b010-b89818c21eb7",
   "metadata": {},
   "source": [
    "This function, named `skip_gram_data_generator()`, takes the following arguments:\n",
    "\n",
    "- `sequences`: A List of Word IDs. THis is the o/p generated by Tokenizer's `texts_to_sequences()` function.\n",
    "\n",
    "- `window_size`: The window_size for the context.\n",
    "\n",
    "- `batch_size`\n",
    "- `negative_samples`: # of -ve samples per example to generate\n",
    "\n",
    "- `vocab_size`: The vocabulary size.\n",
    "- `seed`\n",
    "\n",
    "It will return a batch of data containing:\n",
    "- A batch of target word IDs\n",
    "- A batch of corresponding context word IDs (both positive and negative)\n",
    "- A batch of labels (O and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e10a6aa-4930-48c0-9ef5-8d450a8f3364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:30.905618Z",
     "iopub.status.busy": "2023-06-25T08:29:30.904609Z",
     "iopub.status.idle": "2023-06-25T08:29:30.920034Z",
     "shell.execute_reply": "2023-06-25T08:29:30.918932Z",
     "shell.execute_reply.started": "2023-06-25T08:29:30.905618Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
    "\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d4d95-8edf-44ec-a771-bd334439286d",
   "metadata": {},
   "source": [
    "> Note that we are passing a `sampling_table` argument in below function while generating `positive_skip_grams`. This is another strategy to enhance the performance of Word2vec models. `sampling_table` is simply an array that is the same size as your vocabulary and specifies a probability at each index of the array with which the word indexed by that index will be sampled during skip gram generation. This technique is known as subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfd6f1ac-ccff-4fc9-acfa-dc54380bd97f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:30.921071Z",
     "iopub.status.busy": "2023-06-25T08:29:30.921071Z",
     "iopub.status.idle": "2023-06-25T08:29:30.934118Z",
     "shell.execute_reply": "2023-06-25T08:29:30.934118Z",
     "shell.execute_reply.started": "2023-06-25T08:29:30.921071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def skip_gram_data_generator(sequences, window_size, \n",
    "                             batch_size, negative_samples, \n",
    "                             vocab_size, seed=None):\n",
    "    # shuffle the data - word IDs\n",
    "    rand_sequence_ids = np.arange(len(sequences))\n",
    "    np.random.shuffle(rand_sequence_ids)\n",
    "    \n",
    "    # generating +ve skip-gram word-pairs\n",
    "    for si in rand_sequence_ids:\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "                                    sequence=sequences[si],\n",
    "                                    vocabulary_size=vocab_size,\n",
    "                                    window_size=window_size,\n",
    "                                    negative_samples=0.0,\n",
    "                                    shuffle=False,\n",
    "                                    sampling_table=sampling_table,\n",
    "                                    seed=seed)\n",
    "        \n",
    "    targets, contexts, labels = [], [], []\n",
    "    \n",
    "    # For each tuple contained in `positive_skip_grams`, we generate \n",
    "    # negative samples number of negative candidates. \n",
    "    # We then populate targets, contexts, and label lists with both \n",
    "    # positive and negative candidates:\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "        context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), \n",
    "                                       axis=1)\n",
    "        \n",
    "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                                                    true_classes=context_class,\n",
    "                                                    num_true=1,\n",
    "                                                    num_sampled=negative_samples,\n",
    "                                                    unique=True,\n",
    "                                                    range_max=vocab_size,\n",
    "                                                    name=\"negative_sampling\")\n",
    "        \n",
    "        # Build context and label vectors (for one target word)\n",
    "        context = tf.concat([tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
    "                            axis=0)\n",
    "\n",
    "        label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
    "\n",
    "        # Append each element from the training example to global lists.\n",
    "        targets.extend([target_word]*(negative_samples+1))\n",
    "        contexts.append(context)\n",
    "        labels.append(label)\n",
    "        \n",
    "    contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
    "        \n",
    "    assert contexts.shape[0] == targets.shape[0]\n",
    "    assert contexts.shape[0] == labels.shape[0]\n",
    "\n",
    "    # If seed is not provided generate a random one\n",
    "    if not seed:\n",
    "        seed = random.randint(0, 10e6)\n",
    "    \n",
    "    # Shuffling the data, while mainting the corresponding labels\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(contexts)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(targets)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(labels)\n",
    "    \n",
    "    # Generating batches\n",
    "    for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
    "        yield (targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])], \n",
    "               contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
    "              ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c7b59b0-cf48-409b-8649-ac5327ab3bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:30.935322Z",
     "iopub.status.busy": "2023-06-25T08:29:30.935322Z",
     "iopub.status.idle": "2023-06-25T08:29:32.455272Z",
     "shell.execute_reply": "2023-06-25T08:29:32.455272Z",
     "shell.execute_reply.started": "2023-06-25T08:29:30.935322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 8545,  9733,  5073,  1402,  1183, 13340, 14725,  3724,  5828,\n",
      "        2792]), array([3009,  191,  232,    1,    4,  597,   27,    7,  779, 3330],\n",
      "      dtype=int64))\n",
      "[1 0 0 0 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "news_skip_gram_gen = skip_gram_data_generator(sequences=news_sequences, \n",
    "                                              window_size=4, \n",
    "                                              batch_size=10, \n",
    "                                              negative_samples=5, \n",
    "                                              vocab_size=n_vocab)\n",
    "\n",
    "for btc, bl in news_skip_gram_gen:\n",
    "    print(btc)\n",
    "    print(bl)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e7ab70-d444-4765-8c21-a27408c20632",
   "metadata": {},
   "source": [
    "## 6. Implementing Skip-gram Architechture with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e6d8f-f073-4897-bcc5-147949a03d66",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters\n",
    "Here we define several hyperparameters including `batch_size`(amount of samples in a single batch), `embedding_size`(size of embedding vectors), `window_size`(context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d4c5c5c-e89c-41db-908b-ea482b0a31d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:32.457442Z",
     "iopub.status.busy": "2023-06-25T08:29:32.456434Z",
     "iopub.status.idle": "2023-06-25T08:29:32.471137Z",
     "shell.execute_reply": "2023-06-25T08:29:32.471137Z",
     "shell.execute_reply.started": "2023-06-25T08:29:32.457442Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data points in a single batch\n",
    "batch_size = 4096 \n",
    "\n",
    "# Dimension of the embedding vector.\n",
    "embedding_size = 128 \n",
    "\n",
    "# We use a window size of n on either side of target word\n",
    "window_size=1 \n",
    "\n",
    "# Number of negative samples generated per example\n",
    "negative_samples = 4 \n",
    "\n",
    "# Number of epochs to train for\n",
    "epochs = 5 \n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "\n",
    "# We sample valid datapoints randomly from a large window \n",
    "# without always being deterministic\n",
    "valid_window = 250\n",
    "\n",
    "# When selecting valid examples, we select some of the most frequent \n",
    "# words as well as some moderately rare words as well\n",
    "np.random.seed(54321)\n",
    "random.seed(54321)\n",
    "\n",
    "vaild_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_term_ids = np.append(vaild_term_ids, \n",
    "                           random.sample(range(1000, 1000+valid_window), valid_size),\n",
    "                           axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc3a6f-8a91-4ebf-afa9-489a19b00aad",
   "metadata": {},
   "source": [
    "### Defining the Model \n",
    "\n",
    "Here we will define the skip-gram model. It contains two embedding layers; context and target embeddings. The final output is computed as the dot product between the corresponding lookups from the context and target embeddings for a given batch of context and target words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f50ed-da02-43ee-99cb-a30f9ea07ae2",
   "metadata": {},
   "source": [
    "* **\n",
    "**The skip-gram model**\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/skipgram_3.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6616a76-4cac-4bc9-a432-8e7c881947c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:32.473413Z",
     "iopub.status.busy": "2023-06-25T08:29:32.472409Z",
     "iopub.status.idle": "2023-06-25T08:29:32.487566Z",
     "shell.execute_reply": "2023-06-25T08:29:32.486435Z",
     "shell.execute_reply.started": "2023-06-25T08:29:32.473413Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46b84584-94c1-4ee3-8da2-c0a3c617f2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:32.487566Z",
     "iopub.status.busy": "2023-06-25T08:29:32.487566Z",
     "iopub.status.idle": "2023-06-25T08:29:32.595781Z",
     "shell.execute_reply": "2023-06-25T08:29:32.595781Z",
     "shell.execute_reply.started": "2023-06-25T08:29:32.487566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " context (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " target (InputLayer)            [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " context_embedding (Embedding)  (None, 128)          1920128     ['context[0][0]']                \n",
      "                                                                                                  \n",
      " target_embedding (Embedding)   (None, 128)          1920128     ['target[0][0]']                 \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['context_embedding[0][0]',      \n",
      "                                                                  'target_embedding[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,840,256\n",
      "Trainable params: 3,840,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We will define two i/p layers:\n",
    "\n",
    "# Inputs - skipgrams() function outputs target, context in that order\n",
    "# we will use the same order\n",
    "input_1 = tf.keras.layers.Input(shape=(), name=\"target\")\n",
    "input_2 = tf.keras.layers.Input(shape=(), name=\"context\")\n",
    "\n",
    "# Note how the shape is defined as () . When defining the\n",
    "# shape argument, the actual output shape will have a new\n",
    "# undefined dimension (i.e. None sized) added. In other\n",
    "# words, the final output shape will be [None]\n",
    "\n",
    "\n",
    "# Next, we define two embedding layers: a target embedding\n",
    "# layer and a context embedding layer. These layers will be\n",
    "# used to look up the embeddings for target and context word\n",
    "# IDs that will be generated by the input generation function.\n",
    "\n",
    "context_embedding_layer = tf.keras.layers.Embedding(input_dim=n_vocab, \n",
    "                                                    output_dim=embedding_size,\n",
    "                                                    name=\"context_embedding\")\n",
    "\n",
    "target_embedding_layer = tf.keras.layers.Embedding(input_dim=n_vocab,\n",
    "                                                   output_dim=embedding_size,\n",
    "                                                   name=\"target_embedding\")\n",
    "\n",
    "\n",
    "# LookUp outputs of the embedding layers\n",
    "target_out = target_embedding_layer(input_1)\n",
    "context_out = context_embedding_layer(input_2)\n",
    "\n",
    "\n",
    "# Computing the dot product between the two \n",
    "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
    "\n",
    "# Defining the model\n",
    "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2],\n",
    "                                        outputs=out,\n",
    "                                        name=\"skip_gram_model\")\n",
    "\n",
    "# Compiling the model\n",
    "skip_gram_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                        optimizer=\"adam\")\n",
    "\n",
    "# Model Summary\n",
    "skip_gram_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d08cc-7d7b-4563-91c1-b85c083b3dc0",
   "metadata": {},
   "source": [
    "### Training & Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2f294-4e06-4c9c-94f5-04edf61aac93",
   "metadata": {},
   "source": [
    "The idea of word vectors is that words sharing semantic similarity will have a smaller distance between them, whereas words with no similarity will be far apart. **To compute the similarities between words, we can use the cosine distance.** \n",
    "\n",
    "We picked a set of random word IDs and stored them in valid term ids during our hyperparameter discussion. We will implement a way to compute the closest k words to each of those terms at the end of every epoch. \n",
    "\n",
    "For this, we utilize Keras callbacks. **Keras callbacks give you a way to execute some important operation(s) at the end of every training iteration, epoch, prediction step, and so on.**\n",
    "\n",
    "Since we are using a custom evaluation mechanism(cosine distances) for word vectors, we will need to implement our own callback. \n",
    "\n",
    "Our callback will take a list of word IDs intended as the validation words, a model containing the embedding matrix, and a Tokenizer to decode word IDs:\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c2856-fcad-4f95-bc57-8863c4edbe00",
   "metadata": {},
   "source": [
    "#### Calculating Word Similarities \n",
    "\n",
    "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below. Furthermore, we define the computations as a callback, which will automatically run at the end of an epoch during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b240d329-9395-443e-8222-884555bdce98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:30:07.589285Z",
     "iopub.status.busy": "2023-06-25T08:30:07.589285Z",
     "iopub.status.idle": "2023-06-25T08:30:07.601727Z",
     "shell.execute_reply": "2023-06-25T08:30:07.600721Z",
     "shell.execute_reply.started": "2023-06-25T08:30:07.589285Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValidationCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
    "        \n",
    "        self.valid_term_ids = valid_term_ids\n",
    "        self.model_with_embeddings = model_with_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Validation Logic\"\"\"\n",
    "        \n",
    "        # We will use context embeddings to get the most similar words\n",
    "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
    "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
    "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Get the embeddings corresponding to valid_term_ids\n",
    "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
    "        \n",
    "        # Compute the similarity between valid_term_ids and all the embeddings\n",
    "        # V x d (d x D) => V x D\n",
    "        top_k = 5 # Top k items will be displayed\n",
    "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
    "        \n",
    "        # Invert similarity matrix to negative\n",
    "        # Ignore the first one because that would be the same word as the probe word\n",
    "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
    "                \n",
    "        # Print the output\n",
    "        for i, term_id in enumerate(valid_term_ids):\n",
    "            \n",
    "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
    "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
    "        \n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a665ec2-58bc-4bcb-be04-ea099a57fbce",
   "metadata": {},
   "source": [
    "### Running the Skip-Gram Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58cbdbd6-3ed0-47a4-9eda-e94205dce58b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:30:08.389269Z",
     "iopub.status.busy": "2023-06-25T08:30:08.387763Z",
     "iopub.status.idle": "2023-06-25T08:30:13.098901Z",
     "shell.execute_reply": "2023-06-25T08:30:13.097897Z",
     "shell.execute_reply.started": "2023-06-25T08:30:08.389269Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 started\n",
      "      1/Unknown - 0s 37ms/step - loss: 0.6931election: pumped, sophie, elusive, translated, conventional\n",
      "own: stressing, assistance, alive, casino, detain\n",
      "with: two, decorated, size, generic, bee\n",
      "you: items, mccartney, villa's, existed, villalba's\n",
      "were: pops, seasoned, fans, obligations, lease\n",
      "win: pioneered, yeading, arena, reservoir, hyderabad\n",
      "those: table, toe, carl, granted, dems'\n",
      "music: winger, revitalise, pharma, enrico, 550\n",
      "also: typically, abolish, uwb, adjustments, curry\n",
      "third: racer, archer, patterson, settlement, recruit\n",
      "best: punters, retreat, labelled, dvd, squarepants\n",
      "him: appetite, 1940s, jumper, 1999, dec\n",
      "too: treatments, inspectors, serial, met, ingenious\n",
      "some: britain, trialled, mendes, lamont, neighbouring\n",
      "through: forums, dipped, managed, coincidence, cueto's\n",
      "mr: sounded, findings, filmmaking, musically, appears\n",
      "post: gauge, counsel, nationality, thousands, trotter\n",
      "kept: spurs, theatre, mayors, frequent, worldcom's\n",
      "anyone: wpp, owns, witnessed, budd, pencilled\n",
      "believed: go, arbor, stanley, broadreach, options\n",
      "stay: joel, worth, governing, webpage, bundle\n",
      "states: premiere, case, paul, bored, background\n",
      "successful: tactic, households, buenos, affordable, treated\n",
      "serious: inner, edged, samoa, schumacher, valencia\n",
      "voice: user's, apparent, ideal, occasional, dog\n",
      "gadgets: allowed, joachim, afforded, reverend, knox\n",
      "becoming: understand, assessed, controversial, feels, invest\n",
      "fair: size, kitts, showed, sukul, ofcom\n",
      "hollywood: cage, customer's, marvellous, inherently, eases\n",
      "attempt: damaging, volumes, related, strategists, telewest\n",
      "file: harry, cord, shallow, potentially, consortium\n",
      "light: hawkins, austrian, pointing, portion, adjusts\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.6931\n",
      "Epoch: 2/5 started\n",
      "      1/Unknown - 0s 92ms/step - loss: 0.6929election: pumped, sophie, elusive, translated, conventional\n",
      "own: assistance, stressing, alive, casino, detain\n",
      "with: two, decorated, size, generic, restrictions\n",
      "you: items, existed, mccartney, villa's, villalba's\n",
      "were: pops, fans, seasoned, obligations, lease\n",
      "win: pioneered, yeading, arena, reservoir, hyderabad\n",
      "those: table, carl, toe, granted, dems'\n",
      "music: winger, revitalise, pharma, enrico, 550\n",
      "also: typically, abolish, uwb, adjustments, curry\n",
      "third: racer, archer, patterson, settlement, recruit\n",
      "best: punters, retreat, labelled, dvd, squarepants\n",
      "him: appetite, 1940s, jumper, 1999, dec\n",
      "too: treatments, inspectors, serial, met, ingenious\n",
      "some: britain, trialled, mendes, lamont, neighbouring\n",
      "through: forums, dipped, managed, coincidence, cueto's\n",
      "mr: sounded, findings, filmmaking, musically, appears\n",
      "post: gauge, counsel, nationality, thousands, trotter\n",
      "kept: spurs, theatre, mayors, frequent, mangala\n",
      "anyone: wpp, owns, witnessed, budd, pencilled\n",
      "believed: go, arbor, stanley, broadreach, options\n",
      "stay: joel, worth, governing, webpage, bundle\n",
      "states: premiere, case, paul, bored, background\n",
      "successful: tactic, households, buenos, affordable, treated\n",
      "serious: inner, edged, samoa, schumacher, valencia\n",
      "voice: user's, apparent, ideal, occasional, dog\n",
      "gadgets: allowed, joachim, afforded, reverend, knox\n",
      "becoming: understand, assessed, controversial, feels, invest\n",
      "fair: size, kitts, showed, sukul, ofcom\n",
      "hollywood: cage, customer's, marvellous, inherently, eases\n",
      "attempt: damaging, volumes, strategists, related, telewest\n",
      "file: harry, cord, shallow, potentially, consortium\n",
      "light: hawkins, austrian, pointing, portion, adjusts\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6929\n",
      "Epoch: 3/5 started\n",
      "      1/Unknown - 0s 37ms/step - loss: 0.6929election: pumped, sophie, elusive, translated, conventional\n",
      "own: assistance, stressing, alive, casino, detain\n",
      "with: two, size, decorated, generic, restrictions\n",
      "you: items, existed, mccartney, villa's, villalba's\n",
      "were: pops, fans, seasoned, obligations, lease\n",
      "win: pioneered, yeading, arena, reservoir, hyderabad\n",
      "those: table, carl, toe, granted, dems'\n",
      "music: winger, revitalise, pharma, enrico, 550\n",
      "also: typically, abolish, uwb, adjustments, worsen\n",
      "third: racer, archer, patterson, settlement, recruit\n",
      "best: punters, retreat, labelled, dvd, squarepants\n",
      "him: appetite, 1940s, 1999, jumper, dec\n",
      "too: treatments, inspectors, serial, met, ingenious\n",
      "some: britain, trialled, mendes, lamont, neighbouring\n",
      "through: forums, dipped, managed, coincidence, cueto's\n",
      "mr: sounded, findings, filmmaking, musically, appears\n",
      "post: gauge, counsel, nationality, thousands, schizophrenic\n",
      "kept: spurs, theatre, mayors, frequent, mangala\n",
      "anyone: wpp, owns, witnessed, budd, pencilled\n",
      "believed: go, arbor, stanley, broadreach, options\n",
      "stay: joel, worth, governing, webpage, bundle\n",
      "states: premiere, case, paul, bored, background\n",
      "successful: tactic, households, buenos, affordable, treated\n",
      "serious: inner, edged, samoa, schumacher, valencia\n",
      "voice: user's, apparent, ideal, occasional, dog\n",
      "gadgets: allowed, joachim, afforded, reverend, knox\n",
      "becoming: understand, assessed, controversial, feels, invest\n",
      "fair: size, kitts, showed, sukul, eton\n",
      "hollywood: cage, customer's, marvellous, inherently, eases\n",
      "attempt: damaging, volumes, strategists, related, telewest\n",
      "file: harry, cord, shallow, potentially, consortium\n",
      "light: hawkins, austrian, pointing, portion, adjusts\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.6929\n",
      "Epoch: 4/5 started\n",
      "      1/Unknown - 0s 13ms/step - loss: 0.6933election: pumped, sophie, elusive, translated, conventional\n",
      "own: assistance, stressing, alive, casino, detain\n",
      "with: two, size, decorated, generic, restrictions\n",
      "you: items, existed, mccartney, villalba's, villa's\n",
      "were: pops, fans, seasoned, obligations, lease\n",
      "win: pioneered, yeading, arena, reservoir, hyderabad\n",
      "those: table, carl, toe, granted, dems'\n",
      "music: winger, revitalise, pharma, enrico, 550\n",
      "also: typically, abolish, uwb, worsen, adjustments\n",
      "third: racer, archer, patterson, settlement, recruit\n",
      "best: punters, retreat, labelled, dvd, squarepants\n",
      "him: appetite, 1940s, 1999, jumper, dec\n",
      "too: treatments, inspectors, serial, met, ingenious\n",
      "some: britain, trialled, mendes, lamont, service's\n",
      "through: forums, dipped, managed, coincidence, cueto's\n",
      "mr: sounded, findings, filmmaking, musically, appears\n",
      "post: gauge, counsel, nationality, thousands, schizophrenic\n",
      "kept: spurs, theatre, mayors, frequent, mangala\n",
      "anyone: wpp, owns, witnessed, budd, pencilled\n",
      "believed: go, arbor, stanley, broadreach, options\n",
      "stay: joel, worth, governing, webpage, bundle\n",
      "states: premiere, case, paul, bored, background\n",
      "successful: households, tactic, buenos, affordable, treated\n",
      "serious: inner, edged, samoa, schumacher, valencia\n",
      "voice: user's, apparent, ideal, occasional, dog\n",
      "gadgets: allowed, joachim, afforded, reverend, knox\n",
      "becoming: understand, assessed, controversial, feels, invest\n",
      "fair: size, kitts, showed, sukul, ofcom\n",
      "hollywood: cage, customer's, marvellous, inherently, eases\n",
      "attempt: damaging, volumes, strategists, related, telewest\n",
      "file: harry, cord, shallow, potentially, consortium\n",
      "light: hawkins, austrian, pointing, portion, adjusts\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6933\n",
      "Epoch: 5/5 started\n",
      "      1/Unknown - 0s 13ms/step - loss: 0.6931election: pumped, sophie, elusive, translated, conventional\n",
      "own: assistance, stressing, alive, casino, detain\n",
      "with: two, size, decorated, generic, restrictions\n",
      "you: items, existed, villalba's, villa's, mccartney\n",
      "were: pops, fans, seasoned, obligations, lease\n",
      "win: pioneered, yeading, arena, reservoir, hyderabad\n",
      "those: table, carl, toe, granted, dems'\n",
      "music: winger, revitalise, pharma, enrico, 550\n",
      "also: typically, abolish, uwb, worsen, adjustments\n",
      "third: racer, patterson, archer, settlement, recruit\n",
      "best: punters, retreat, labelled, dvd, squarepants\n",
      "him: appetite, 1940s, 1999, jumper, dec\n",
      "too: treatments, inspectors, serial, met, ingenious\n",
      "some: britain, trialled, mendes, lamont, service's\n",
      "through: forums, dipped, managed, coincidence, cueto's\n",
      "mr: sounded, findings, filmmaking, musically, appears\n",
      "post: gauge, counsel, nationality, thousands, schizophrenic\n",
      "kept: spurs, theatre, mayors, frequent, mangala\n",
      "anyone: wpp, owns, witnessed, budd, pencilled\n",
      "believed: go, arbor, stanley, broadreach, options\n",
      "stay: joel, worth, governing, webpage, bundle\n",
      "states: premiere, case, paul, bored, background\n",
      "successful: households, tactic, buenos, affordable, treated\n",
      "serious: inner, edged, samoa, schumacher, valencia\n",
      "voice: user's, apparent, ideal, occasional, dog\n",
      "gadgets: allowed, joachim, afforded, reverend, knox\n",
      "becoming: understand, assessed, controversial, feels, invest\n",
      "fair: kitts, size, showed, sukul, ofcom\n",
      "hollywood: cage, customer's, marvellous, inherently, eases\n",
      "attempt: damaging, volumes, strategists, related, telewest\n",
      "file: harry, shallow, cord, potentially, consortium\n",
      "light: hawkins, austrian, pointing, portion, adjusts\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
    "\n",
    "for ei in range(epochs):\n",
    "    \n",
    "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
    "    \n",
    "    news_skip_gram_gen = skip_gram_data_generator(\n",
    "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
    "    )\n",
    "    \n",
    "    skip_gram_model.fit(\n",
    "        news_skip_gram_gen, epochs=1, \n",
    "        callbacks=skipgram_validation_callback,        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92d1fc3d-84d4-4c1a-b2e5-a4f382f0948e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:40.037350Z",
     "iopub.status.busy": "2023-06-25T08:29:40.036345Z",
     "iopub.status.idle": "2023-06-25T08:29:40.051125Z",
     "shell.execute_reply": "2023-06-25T08:29:40.050125Z",
     "shell.execute_reply.started": "2023-06-25T08:29:40.037350Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()),\n",
    "                                  key=lambda x: x[0])[:vocab_size-1])\n",
    "        \n",
    "    words_sorted = [None] + list(words_sorted)\n",
    "    \n",
    "    pd.DataFrame(model.get_layer(\"context_embedding\").get_weights()[0], \n",
    "                 index = words_sorted).to_pickle(os.path.join(save_dir, \"context_embedding.pkl\"))\n",
    "    \n",
    "    pd.DataFrame(model.get_layer(\"target_embedding\").get_weights()[0], \n",
    "                 index = words_sorted).to_pickle(os.path.join(save_dir, \"target_embedding.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b4d9940-9ce9-40c6-8c0f-3435c4137332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-25T08:29:40.052126Z",
     "iopub.status.busy": "2023-06-25T08:29:40.052126Z",
     "iopub.status.idle": "2023-06-25T08:29:40.112229Z",
     "shell.execute_reply": "2023-06-25T08:29:40.112229Z",
     "shell.execute_reply.started": "2023-06-25T08:29:40.052126Z"
    }
   },
   "outputs": [],
   "source": [
    "save_embeddings(skip_gram_model, tokenizer, \n",
    "                n_vocab, save_dir='skipgram_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c00672-3ee7-42fc-9f30-992a9e8ec8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
