{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86fc67c-64df-46b3-958b-be9d124d3508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. The BLEU score â€“ Evaluating the MT systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3d4e6-f5f4-40fa-ae0e-db7f97be6f94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.1 What is BLEU ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855d764-c87a-4dcd-9f59-918b8cb78dca",
   "metadata": {},
   "source": [
    "- **BLEU** stands for **Bilingual Evaluation Understudy** and is a way of automatically evaluating MT(machine translation) systems.\n",
    "\n",
    "- *BLEU score measures the quality/accuracy of translations produced by the model. BLEU score looks at `n`-grams of tokens produced by the decoder to measure how \"close\" the predicted translation is to the actual ground truth sequence.*\n",
    "\n",
    "- *The BLEU score denotes the number of `n`-grams (for example, unigrams and bigrams) of candidate translation that matched in the reference translation. So the higher the BLEU score, the better the MT system.*\n",
    "\n",
    "Reference: [Tensorflow GitHub - Code Implementation](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)\n",
    "\n",
    "* **\n",
    "\n",
    "Let's consider an example to learn the calculations of the BLEU score. \n",
    "\n",
    "- Say we have two **candidate sentences** (i.e., a sentence predicted by our MT system) and a **reference sentence** (i.e., the corresponding actual translation) for some given source sentence:\n",
    "    - *Reference_1* $\\rightarrow \\textit{The cat sat on the mat} \\leftarrow$ *Actual Translation*\n",
    "    \n",
    "    - *Candidate_1* $\\rightarrow \\textit{The cat is on the mat} \\leftarrow$ *Predicted Translation*\n",
    "\n",
    "* ** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2d84d-4096-43da-adf9-4a0c438acc05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.2 *Why Precision fails in MT ?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc968f5-2a05-4e74-a39f-f91b0d057e64",
   "metadata": {},
   "source": [
    "- To see how good the translation is, we can use one measure, **precision**. \n",
    "\n",
    "    - *Precision is a measure of how many words in the candidate are actually present in the reference.* \n",
    "    \n",
    "    - In general, if you consider a classification problem with two classes (denoted by negative and positive), precision is given by the following formula: $$Precision = \\frac{\\text{No. of samples correctly classified as +ve}}{\\text{all samples classified as +ve}}$$\n",
    "    \n",
    "    - Letâ€™s now calculate the precision for candidate 1: $$Precision = \\frac{\\text{# of times each word of candidate appeared in reference}}{\\text{/# of words in candidate}}$$\n",
    "    \n",
    "    - Mathematically, $$Precision = \\frac{\\sum_{\\text{unigram} \\in \\text{Candidate}}\\text{Is Found In Ref}(unigram)}{|\\text{Candidate}|}$$ <br></br>$$\\textit{Precision for candidatee_1} = \\frac{5}{6}$$\n",
    "    \n",
    "    - $\\textit{This is also known as 1-gram precision since we consider a single word at a time.}$\n",
    "    \n",
    "Now letâ€™s introduce a new candidate:\n",
    "\n",
    "- *Candidate_2* $\\rightarrow \\textit{The the the cat cat cat}$\n",
    "\n",
    "- *It is not hard for a human to see that candidate 1 is far better than candidate 2. Letâ€™s calculate the precision:* $$\\textit{Precision for candidatee_2} = \\frac{6}{6} = 1$$\n",
    "\n",
    ">ðŸ—ï¸ **As we can see, the precision score disagrees with the judgment we made. Therefore, precision alone cannot be trusted to be a good measure of the quality of a translation.**\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac9321-c249-411d-a842-cb3dbfc2966d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.3 *Modified Precision* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a50d77d-9d92-4ad7-82e1-107c04ac1b43",
   "metadata": {},
   "source": [
    "To address the precision limitation, we can use a **modified 1-gram precision**. \n",
    "\n",
    "- ***The modified precision clips the number of occurrences of each unique word in the candidate by the number of times that word appeared in the reference:*** \n",
    "\n",
    "$$p_1 = \\frac{\\sum_{\\text{unigram} \\in \\text{{Candidate}}}Min(\\text{Occurences}(unigram), {unigram}_{max})}{|\\text{Candidate}|}$$ \n",
    "\n",
    "\n",
    "- Therefore, for candidates 1 and 2, the modified precision would be as follows: \n",
    "\n",
    "$$\\textit{Mod-1-gram-Precision Candidate 1} = \\frac{(1+1+1+1+1)}{6} = \\frac{5}{6}$$\n",
    "\n",
    "$$\\textit{Mod-1-gram-Precision Candidate 2} = \\frac{(1+1+1)}{6} = \\frac{3}{6}$$\n",
    "\n",
    "\n",
    "*We can already see that this is a good modification as the precision of candidate 2 is reduced. This can be extended to any n-gram by considering n words at a time instead of a single word.*\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad96e4-bc60-4b7c-9309-c379be8aa8f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.4 *Brevity penalty*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b66c06-9365-478c-9731-d301c49038e7",
   "metadata": {},
   "source": [
    "- Precision naturally prefers small sentences. \n",
    "\n",
    "- This raises a question in evaluation, as the MT system might generate small sentences for longer references and still have higher precision. Therefore, a brevity penalty is introduced to avoid this. \n",
    "\n",
    "- The brevity penalty is calculated by the following: \n",
    "\n",
    "$$\n",
    "BP =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } c > r \\\\\n",
    "e^{1 - \\frac{r}{c}} & \\text{if } c \\leq r\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "*Here, $c = \\textit{candidate sentence length}$ & $r = \\textit{reference sentence length}$*. \n",
    "\n",
    "In our example, we calculate it as shown here:\n",
    "- $\\textit{BP for candidate-1} = e^{1 - \\frac{6}{6}} = e^0 = 1$\n",
    "- $\\textit{BP for candidate-2} = e^{1 - \\frac{6}{6}} = e^0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc840d2-7c7a-4369-aebd-e41cd493f9e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1.5 *The final BLEU Score*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850a949-1796-4071-b68d-6465b9dce00d",
   "metadata": {},
   "source": [
    "- Next, to calculate the BLEU score, we first calculate several different modified n-gram precisions for a bunch of different $n=1,2, \\ldots ,N$ values. We will then calculate the weighted geometric mean of the $n-gram$ precisions:\n",
    "\n",
    "$$BELU = BP \\times \\exp\\left(\\sum_{i=1}^{N}w_n p_n\\right)$$\n",
    "\n",
    "- Here, $w_n$ is the weight for the modified $n-gram$ precision $p_n$. \n",
    "\n",
    "- By default, equal weights are used for all $n-gram$ values. \n",
    "\n",
    "In conclusion, BLEU calculates a modified $n-gram$ precision and penalizes the modified $n-gram$ precision with a brevity penalty. The modified n-gram precision avoids potential high precision values given to meaningless sentences (for example, candidate 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847b6e1-3d76-443f-9442-ed07fd104c24",
   "metadata": {},
   "source": [
    "## 2. Other applications of Seq2Seq models â€“ ChatBots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5787c0-28be-4ff9-af75-7b37d49e8040",
   "metadata": {},
   "source": [
    "- One other popular application of sequence-to-sequence models is in creating chatbots. *A chatbot is a computer program that is able to have a realistic conversation with a human.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc715b51-77f0-49c2-b545-ef33c27a7cfe",
   "metadata": {},
   "source": [
    "### 2.1 Training a ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c813d01-79b5-489c-8597-517b0afce249",
   "metadata": {},
   "source": [
    "- Training a chatbot is similar to training a NMT(Neural Machine Translation) model. The only difference would be how the source and target sentence pairs are formed.\n",
    "\n",
    "- In the NMT system, the sentence pairs consist of a source sentence and the corresponding translation in a target language for that sentence. \n",
    "\n",
    "- *However, in training a chatbot, the data is extracted from the dialogue between two people. The source sentences would be the sentences/phrases uttered by person A, and the target sentences would be the replies to person A made by person B.* \n",
    "\n",
    "- Datasets that can be used for training a chatbot:\n",
    "\n",
    "    - [Movie dialogues between people](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "    - [Reddit comments dataset](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)\n",
    "    - [Maluuba dialogue dataset](https://datasets.maluuba.com/Frames)\n",
    "    - [Ubuntu dialogue corpus](http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/)\n",
    "    - [NIPS conversational intelligence challenge](http://convai.io/)\n",
    "    - [Microsoft Research social media text corpus](https://tinyurl.com/y7ha9rc5)\n",
    "    \n",
    "    \n",
    "Below figure shows the similarity of a chatbot system to an NMT system. *For example, we train a chatbot with a dataset consisting of dialogues between two people. The encoder takes in the sentences/phrases spoken by one person, where the decoder is trained to predict the other personâ€™s response. After training in such a way, we can use the chatbot to provide a response to a given question:*\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/chatbots.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f94a0-f316-408d-a2e6-7663bf67093e",
   "metadata": {},
   "source": [
    "### 2.2 Evaluating chatbots â€“ the Turing Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e043b8-cad1-4572-88e1-b8a5514f0375",
   "metadata": {},
   "source": [
    "- After building a chatbot, one way to evaluate its effectiveness is using the **Turing test**. \n",
    "\n",
    "- **The Turing test** was invented by Alan Turing in the 1950s as a way of measuring the intelligence of a machine.\n",
    "\n",
    "- The experiment settings are well suited for evaluating chatbots. The experiment is set up as follows:\n",
    "\n",
    "    - There are three parties involved: \n",
    "        - **an evaluator** (i.e., a human)(A), \n",
    "        - **another human** (B), and \n",
    "        - **a machine** (C)<br></br> \n",
    "\n",
    "    - The three of them sit in three different rooms so that none of them can see the others. \n",
    "\n",
    "    - The only communication medium is text, which is typed into a computer by one party, and the receiver sees the text on a computer on their side. \n",
    "    \n",
    "    - The evaluator communicates with both the human and the machine. \n",
    "\n",
    "    - And at the end of the conversation, the evaluator is to distinguish the machine from the human. \n",
    "\n",
    "    - If the evaluator cannot make the distinction, the machine is said to have passed the Turing test. \n",
    "\n",
    "The setup is illustrated:\n",
    "<div align='center'>\n",
    "    <img src='images/turing_test.png'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
