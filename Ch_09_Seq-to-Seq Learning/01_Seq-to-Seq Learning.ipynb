{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6376faa5-0f5e-43b6-9cc8-401b660582bc",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h1>Sequence-to-Sequence Learning – Neural Machine Translation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63cd3b-ae56-423d-aefe-18d073c2d211",
   "metadata": {},
   "source": [
    "- **Sequence-to-sequence learning** is the term used for tasks that require mapping an arbitrary-length sequence to another arbitrary-length sequence. \n",
    "\n",
    "- This is one of the most sophisticated tasks in NLP, which involves learning many-to-many mappings. \n",
    "\n",
    "- Examples of this task include **Neural Machine Translation (NMT)** and creating **chatbots**. NMT is where we translate a sentence from one language (source language) to another (target language) like Google Translate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf5513-a110-4fac-ab83-c048233c73dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Understanding Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38ea58-89a6-42ac-8f87-5e9f1fb2f9aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Intuition behind NMT systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3450b-ee42-42eb-af41-468ec8646af5",
   "metadata": {},
   "source": [
    "Let’s understand the intuition underlying an NMT system’s design. \n",
    "\n",
    "- For translating a sentence from English to German, say: `I went home --> Ich ging nach Hause`. The way we translate this is:\n",
    "    - First, you read the English sentence, and then you create a thought or concept about what this sentence represents or implies, in your mind. And finally, you translate the sentence into German.<br></br>\n",
    "    \n",
    "- The same idea is used for building NMT systems. \n",
    "    - The **encoder** reads the source sentence (that is, similar to you reading the English sentence). \n",
    "    \n",
    "    - Then the encoder outputs a **context vector** (the context vector corresponds to the thought/concept you imagined after reading the sentence). \n",
    "    \n",
    "    - Finally, the **decoder** takes in the context vectors and outputs the translation in German:\n",
    "    \n",
    "<div align='center'>\n",
    "    <img src='images/nmt_system.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08f809-6f54-41e7-b008-37b6358ae9f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### NMT Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a57cf-e02e-4ea2-a1e8-cf216aa5171d",
   "metadata": {},
   "source": [
    "- NMT is a encoder-decoder architecture. \n",
    "\n",
    "- The **encoder** converts a sentence from a given source language into a thought vector (i.e. a contextualized representation), and the **decoder** decodes or translates the thought into a target language.\n",
    "\n",
    "- The left-hand side of the context vector denotes the encoder (which takes a source sentence in word by word to train a time-series model). \n",
    "\n",
    "- The right-hand side denotes the decoder, which outputs word by word (while using the previous word as the current input) the corresponding translation of the source sentence. \n",
    "\n",
    "- We will also use embedding layers (for both the source and target languages) where the semantics of the individual tokens will be learned and fed as inputs to the models:\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/nmt_system1.png'/>\n",
    "</div>\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1370e9-e94e-4f5b-983e-8de0e8d5b93a",
   "metadata": {},
   "source": [
    "The objective of the NMT is to maximize the log-likelihood, given a source sentence $x_s$ and its corresponding translation $y_T$ : $$\\frac{1}{N} \\sum_{i = 1}^{N} \\log P(y_T | x_s)$$ Here, $N$ refers to the number of source and target sentence inputs we have as training data.\n",
    "\n",
    "Then, during inference, for a given source sentence, $x_s^{infer}$, we will find the $Y_T^{best}$ translation as:\n",
    "\n",
    "$$Y_T^{best} = {argmax}_{y\\in Y_T} P(y_T | x_s^{infer})$$\n",
    "\n",
    "* **\n",
    "\n",
    "Let's connect the dots b/w the embedding layer, the encoder, the context vector, and the decoder:\n",
    "\n",
    "1. We use two word embedding layers, one for the source language and the other for the target, to better represent the semantics b/w the words of the respective languages.\n",
    "\n",
    "2. The encoder is responsible for generating a thought vector or a context vector that represents what is meant by the source language.\n",
    "    - The encoder is an RNN cell.\n",
    "    \n",
    "    - At time step $t_0$ the enocoder is initialized with a zero vector by default. After finally getting trained on the sequence of source sentences/words, It produces a context vector which is it's final external hidden state.<br></br>\n",
    "    \n",
    "3. The idea of the context vector is to represent a sentence of a source language concisely.\n",
    "    - Also, in contrast to how the encoder’s state is initialized (that is, it is initialized with zeros), the context vector becomes the initial state for the decoder.\n",
    "    \n",
    "    - This creates a linkage between the encoder and the decoder and makes the whole model end-to-end differentiable. <br></br>\n",
    "    \n",
    "4. The decoder is responsible for decoding the context vector into the desired translation. Our decoder is an RNN as well.\n",
    "    - The context vector is the only piece of information that is available to the decoder about the source sentence. Thus, it is a crucial link b/w encoder and decoder.\n",
    "    \n",
    "    - After getting initialized with the context vector as it's initial state the decoder then learns the patterns in the target text.\n",
    "    \n",
    "    - Though it is possible for the encoder and decoder to share the same set of weights, it is usually better to use two different networks for the encoder and the decoder. This increases the number of parameters in our model, allowing us to learn the translations more effectively.\n",
    "    \n",
    "    - For the prediction we use something like softmax function to predict the words.\n",
    "    \n",
    "    \n",
    "The full NMT system with the details of how the GRU cell in the encoder connects to the GRU cell in the decoder, and how the softmax layer is used to output predictions, is shown:\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/nmt_gru.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e3539-3dad-4b0b-9545-7ee2bde0366e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Machine Translation: English to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf47ae10-54fb-488b-a965-efa636b1d5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.1\n",
      "Tensorflow GPU Access status: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Set the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "print(f\"Tensorflow GPU Access status: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074d30c-61b8-4cbe-b024-75de06574679",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb221def-7bb0-41c2-9738-5aa815ddb923",
   "metadata": {},
   "source": [
    "WMT-14 English-German translation data from https://nlp.stanford.edu/projects/nmt/. There are ~4.5 million sentence pairs available. However, we will use only 250,000 sentence pairs due to computational feasibility.\n",
    "\n",
    "The required files are:\n",
    "- File containing German sentences: `train.de`\n",
    "- File containing English sentences: `train.en`\n",
    "- File containing German vocabulary: `vocab.50K.de`\n",
    "- File containing English vocabulary: `vocab.50K.en`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5751a-9e81-4362-95ac-4a128a99e2ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac88b69b-f079-4588-acd5-bdc37d3c7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sentences = 250000\n",
    "\n",
    "# Loading English sentences\n",
    "original_en_sentences = []\n",
    "with open('data/train.en', 'r', encoding='utf-8') as en_file:\n",
    "    for i,row in enumerate(en_file):\n",
    "        # if i < 50: continue # or i==22183 or i==27781 or i==81827: continue\n",
    "        if i >= n_sentences: break\n",
    "        original_en_sentences.append(row.strip().split(\" \"))\n",
    "        \n",
    "# Loading German sentences\n",
    "original_de_sentences = []\n",
    "with open('data/train.de', 'r', encoding='utf-8') as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        # if i < 50: continue # or i==22183 or i==27781 or i==81827: continue\n",
    "        if i >= n_sentences: break\n",
    "        original_de_sentences.append(row.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd3b0e6-4cb1-43dd-a033-437f51d5cc40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 250000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_en_sentences), len(original_de_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11101413-9723-41a8-a214-5d754c4441c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .\n",
      "German: iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .\n",
      "\n",
      "English: iron cement protects the ingot against the hot , abrasive steel casting process .\n",
      "German: Nach der Aushärtung schützt iron cement die Kokille gegen den heissen , abrasiven Stahlguss .\n",
      "\n",
      "English: a fire restant repair cement for fire places , ovens , open fireplaces etc .\n",
      "German: feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc.\n",
      "\n",
      "English: Construction and repair of highways and ...\n",
      "German: Der Bau und die Reparatur der Autostraßen ...\n",
      "\n",
      "English: An announcement must be commercial character .\n",
      "German: die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen .\n",
      "\n",
      "English: Goods and services advancement through the P.O.Box system is NOT ALLOWED .\n",
      "German: der Vertrieb Ihrer Waren und Dienstleistungen durch das Postfach ##AT##-##AT## System WIRD NICHT ZUGELASSEN .\n",
      "\n",
      "English: Deliveries ( spam ) and other improper information deleted .\n",
      "German: die Werbeversande ( Spam ) und andere unkorrekte Informationen werden gelöscht .\n",
      "\n",
      "English: Translator Internet is a Toolbar for MS Internet Explorer .\n",
      "German: ACDSee 9 Photo Manager Organize your photos . Share your world .\n",
      "\n",
      "English: It allows you to translate in real time any web pasge from one language to another .\n",
      "German: No matter what kind of photos you take - of friends and family or artistic shots as a hobby - you need photo software that organizes your shots AND allows you to view , fix , and share them quickly and easily .\n",
      "\n",
      "English: You only have to select languages and TI does all the work for you ! Automatic dictionary updates ....\n",
      "German: ACDSee 9 makes organizing your photos exactly that : Quick and easy , so you can play with and share the great photos you &apos; ve got ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a few sentences\n",
    "for en_s, de_s in zip(original_en_sentences[:10], original_de_sentences[:10]):\n",
    "    print(f\"English: {' '.join(en_s)}\\nGerman: {' '.join(de_s)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8319d-be59-4d56-9d05-95fc1f230698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Adding special tokens\n",
    "\n",
    "We add special tokens `<s>` and `</s>` to denote the beginning and end of sequences respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76cbc6-24fd-469f-a85c-9cc7e106938d",
   "metadata": {},
   "source": [
    "- **This is a very important step for Seq2Seq models. `<s>` and `</s>` tokens serve an extremely important role during model inference.** \n",
    "\n",
    "- At inference time, we will be using the decoder to predict one word at a time, by using the output of the previous time step as an input. This way we can predict for an arbitrary number of time steps. \n",
    "\n",
    "- Using `<s>` as the starting token gives us a way to signal to the decoder that it should start predicting tokens from the target language. \n",
    "\n",
    "- Next, if we do not use the `</s>` token to mark the end of a sentence, we cannot signal the decoder to end a sentence. This can lead the model to enter an infinite loop of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75ef05d-25fc-4677-95c3-193eb27ab4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_sentences = [[\"<s>\"]+sent+[\"</s>\"] for sent in original_en_sentences]\n",
    "de_sentences = [[\"<s>\"]+sent+[\"</s>\"] for sent in original_de_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c3e26b-e5a9-4216-966e-219e32fb5657",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: <s> iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould . </s>\n",
      "German: <s> iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird . </s>\n",
      "\n",
      "English: <s> iron cement protects the ingot against the hot , abrasive steel casting process . </s>\n",
      "German: <s> Nach der Aushärtung schützt iron cement die Kokille gegen den heissen , abrasiven Stahlguss . </s>\n",
      "\n",
      "English: <s> a fire restant repair cement for fire places , ovens , open fireplaces etc . </s>\n",
      "German: <s> feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc. </s>\n",
      "\n",
      "English: <s> Construction and repair of highways and ... </s>\n",
      "German: <s> Der Bau und die Reparatur der Autostraßen ... </s>\n",
      "\n",
      "English: <s> An announcement must be commercial character . </s>\n",
      "German: <s> die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen . </s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a few sentences\n",
    "for en_s, de_s in zip(en_sentences[:5], de_sentences[:5]):\n",
    "    print(f\"English: {' '.join(en_s)}\\nGerman: {' '.join(de_s)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f427f-ff4e-4aa2-a6c8-c028fed3964e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train, validation and test split\n",
    "\n",
    "Here we split the full dataset as follows:\n",
    "- Train - 80%\n",
    "- Validation - 10%\n",
    "- Test - 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d207681-c128-4205-9229-539baa33a89e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 200000\n",
      "Valid size: 25000\n",
      "Test size: 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_en_sentences, valid_test_en_sentences, train_de_sentences, valid_test_de_sentences = train_test_split(\n",
    "    np.array(en_sentences, dtype=object), np.array(de_sentences, dtype=object), test_size=0.2\n",
    ")\n",
    "\n",
    "valid_en_sentences, test_en_sentences, valid_de_sentences, test_de_sentences = train_test_split(\n",
    "    valid_test_en_sentences, valid_test_de_sentences, test_size=0.5)\n",
    "\n",
    "print(f\"Train size: {len(train_en_sentences)}\")\n",
    "print(f\"Valid size: {len(valid_en_sentences)}\")\n",
    "print(f\"Test size: {len(test_en_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ec557-5162-46ed-8e33-45e0cb817cce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Analyse lengths of sequences\n",
    "\n",
    "A key statistic we have to understand at this point is how long, generally, the sentences in our corpus are. It is quite likely that the two languages will have different sentence lengths.\n",
    "\n",
    "\n",
    "- Here we can see that 95% of English sentences have 54 tokens, where 95% of German sentences have 49 tokens\n",
    "\n",
    "- We use the 80% percentile of the sequence lengths for each language as a threshold to,\n",
    "    - Truncate sequences longer than that\n",
    "    - Add a special token (`<pad>`) to bring shorter sentences to that length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37847e9f-1021-4e83-9d7b-87e7d77020d1",
   "metadata": {},
   "source": [
    ">Note that we are only using the training data for this calculation. If you include validation or test datasets in these calculations, we may be leaking data about validation and test data. Therefore, it’s best to only use the training dataset for these calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b6700c-3fd4-473a-a284-85ba7b00f3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths (English)\n",
      "count    200000.000000\n",
      "mean         26.864400\n",
      "std          13.587173\n",
      "min           8.000000\n",
      "5%           11.000000\n",
      "50%          24.000000\n",
      "80%          36.000000\n",
      "90%          45.000000\n",
      "95%          54.000000\n",
      "max         102.000000\n",
      "dtype: float64\n",
      "\n",
      "Sequence lengths (German)\n",
      "count    200000.000000\n",
      "mean         24.757705\n",
      "std          12.426796\n",
      "min           8.000000\n",
      "5%           11.000000\n",
      "50%          22.000000\n",
      "80%          33.000000\n",
      "90%          41.000000\n",
      "95%          49.000000\n",
      "max         102.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Sequence lengths (English)\")\n",
    "print(pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05,0.5,0.8,0.9,0.95]))\n",
    "\n",
    "print(\"\\nSequence lengths (German)\")\n",
    "print(pd.Series(train_de_sentences).str.len().describe(percentiles=[0.05,0.5,0.8,0.9,0.95]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7201f-d79e-40dd-b642-728e8634c158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Padding the sentences to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efefdfc3-3565-4b13-a960-2e03bf31041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_en_seq_length = 36\n",
    "n_de_seq_length = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e543ad-e331-42f1-832c-dfb4895b1b70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "pad_token = '<pad>'\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(train_en_sentences, maxlen=n_en_seq_length,\n",
    "                                          value=pad_token, dtype=object, truncating='post',\n",
    "                                          padding='post')\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(valid_en_sentences, maxlen=n_en_seq_length,\n",
    "                                          value=pad_token, dtype=object, truncating='post',\n",
    "                                          padding='post')\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(test_en_sentences, maxlen=n_en_seq_length,\n",
    "                                         value=pad_token, dtype=object, truncating='post',\n",
    "                                         padding='post')\n",
    "\n",
    "\n",
    "train_de_sentences_padded = pad_sequences(train_de_sentences, maxlen=n_de_seq_length,\n",
    "                                          value=pad_token, dtype=object, truncating='post',\n",
    "                                          padding='post')\n",
    "\n",
    "valid_de_sentences_padded = pad_sequences(valid_de_sentences, maxlen=n_de_seq_length,\n",
    "                                          value=pad_token, dtype=object, truncating='post',\n",
    "                                          padding='post')\n",
    "\n",
    "test_de_sentences_padded = pad_sequences(test_de_sentences, maxlen=n_de_seq_length,\n",
    "                                         value=pad_token, dtype=object, truncating='post',\n",
    "                                         padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba797ded-77d0-423d-b2a7-2b2887abb402",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some validation sentences ...\n",
      "\n",
      "English: <s> For DVDs and Videos , delivery charges and handling fees are already included in the price . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Germen: <s> In den Preisangaben für DVDs und Videos sind die Versandkosten und Bearbeitungsgebühren bereits enthalten . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <s> So configure the \\ xampp \\ apache \\ bin \\ php.ini for web changes . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Germen: <s> Das ist zwar auch nicht ganz falsch , sollte PHP als Konsolenprogramm ( cli ) benutzt werden . In der Regel aber wird PHP im XAMPP über den Apache Webserver via mod\n",
      "\n",
      "English: <s> Modern , elegant and equipped well , this excellent residence of the news hotel is a fantastic base of the festivities . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Germen: <s> Modern , elegant und gut ausgerüstet , ist dieser ausgezeichnete Wohnsitz des Nachrichtenhotels eine fantastische Unterseite der Festlichkeiten . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "**************************************************\n",
      "Some test sentences ...\n",
      "\n",
      "English: <s> Welcome to the largest catalogue of private accommodation on the Island Pag with direct contact to houseowners ! Let your vacation begin on this web pages , find the perfect accommodation , check availability online\n",
      "Germen: <s> Möge Ihr Sommerurlaub schon auf diesen Seiten beginnen , finden Sie ideale Unterkunft , prüfen Sie Unterkunft auf Verfügbarkeit und buchen Sie diese direkt bei dem Inhaber des jeweiligen Objektes . </s>\n",
      "\n",
      "English: <s> The Maritim ’ s rooms and suites feature an exclusive Italian design , air conditioning , comfortable beds , a large work desk and satellite TV . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Germen: <s> Die Zimmer und Suiten bestechen mit exklusivem italienischen Design , Klimaanlage , bequemen Betten , einem großen Schreibtisch und Sat ##AT##-##AT## TV . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "English: <s> Guinness Storehouse ( 17 km ) - For an alternative night out this summer , look no further than Dublin &apos;s Guinness Storehouse . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Germen: <s> Guinness Lagerhaus ( 17 Km ) - nach einem alternativen Programm für eine Sommernacht brauchen Sie nicht länger zu suchen . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Some validation sentences ...\\n\")\n",
    "for en_sent, de_sent in zip(valid_en_sentences_padded[:3], valid_de_sentences_padded[:3]):\n",
    "    en_sent_str = ' '.join(en_sent)\n",
    "    de_sent_str = ' '.join(de_sent)\n",
    "    print(f\"English: {en_sent_str}\\nGermen: {de_sent_str}\\n\")\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Some test sentences ...\\n\")\n",
    "for en_sent, de_sent in zip(test_en_sentences_padded[:3], test_de_sentences_padded[:3]):\n",
    "    en_sent_str = ' '.join(en_sent)\n",
    "    de_sent_str = ' '.join(de_sent)\n",
    "    print(f\"English: {en_sent_str}\\nGermen: {de_sent_str}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a83889-8aec-4913-b6ae-f6aae3b61101",
   "metadata": {},
   "source": [
    "* **\n",
    "<div align='center'>\n",
    "    <h3>Preprocessing Trick: Reversing the source sentence</h3>\n",
    "</div>\n",
    "\n",
    "We can also perform a special trick on the source sentences. Say we have the sentence\n",
    "**ABC** in the source language, which we want to translate to $\\alpha \\beta \\gamma \\phi$  in the target language. \n",
    "\n",
    "We will first reverse the source sentences so that the sentence ABC is read as CBA. This means that in order to translate **ABC** to $\\alpha \\beta \\gamma \\phi$, we need to feed in **CBA**.\n",
    "\n",
    "- This improves the performance of our model significantly, especially **when the source and target languages share the same sentence structure (for example, subject-verb-object).**\n",
    "\n",
    "- Let’s try to understand why this helps. Mainly, it helps to build good communication between the encoder and the decoder. Let’s start from the previous example. We will concatenate the source and target sentences: $$ABC\\alpha \\beta \\gamma$$\n",
    "\n",
    "- If you calculate the distance (that is, the number of words separating two words)\n",
    "from A to $\\alpha$ or B to $\\beta$, they will be the same. However, consider this when you reverse the source sentence, as shown here: $$CBA\\alpha \\beta \\gamma$$\n",
    "\n",
    "\n",
    "- Here, A is very close to $\\alpha$ and so on. Also, to build good translations, building good communications at the very start is important. This simple trick can possibly help NMT systems to improve their performance.\n",
    "\n",
    ">**Note:** that the source sentence reversing step is a subjective preprocessing step. This might not be necessary for some translational tasks. \n",
    "\n",
    "- For example, if your translation task is to translate from Japanese (which is often written in *subject-object-verb* format) to Filipino (often written *verb-subject-object*), then reversing the source sentence might actually cause harm rather than helping. This is because by reversing the text in Japanese, you are increasing the distance between the starting element of the target sentence (that is, the verb (Japanese)) and the corresponding source language entity (that is, the verb (Filipino)).\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107b75c-931d-46e0-962e-52a4b935de45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading Vocabulary \n",
    "\n",
    "Let's build the vocabulary dictionaries for both the source (German) and target (English) languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcab27-eca8-423f-8879-e2171b0c78fd",
   "metadata": {},
   "source": [
    "Originally, each vocabulary contains 50,000 tokens. However, we’ll take only half of this to reduce the memory requirement. \n",
    "\n",
    "> **Note:** that we allow one extra token as there’s a special token `<unk>` to denote **out-of-vocabulary (OOV)** words. \n",
    "    \n",
    "With a 50,000-token vocabulary, it is quite easy to run out of memory due to the size of the final prediction layer we’ll build. \n",
    "\n",
    "- **While cutting back on the size of the vocabulary, we have to make sure that we preserve the most common 25,000 words:**\n",
    "    - Fortunately, each vocabulary file is organized such that words are ordered by their frequency of occurrence (high to low). Therefore, we just need to read the first 25,001 lines of text from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba3c51f-0ef8-4ae9-b277-7390a818c329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# +1 for to denote out-of-vocabulary (OOV) words as <unk>.\n",
    "n_vocab = 25000 + 1\n",
    "\n",
    "en_vocabulary = []\n",
    "with open('data/vocab.50K.en', 'r', encoding='utf-8') as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "        if ri  >= n_vocab: break\n",
    "            \n",
    "        en_vocabulary.append(row.strip())\n",
    "        \n",
    "\n",
    "de_vocabulary = []\n",
    "with open('data/vocab.50K.de', 'r', encoding='utf-8') as de_file:\n",
    "    for ri, row in enumerate(de_file):\n",
    "        if ri >= n_vocab: break\n",
    "            \n",
    "        de_vocabulary.append(row.strip())\n",
    "        \n",
    "\n",
    "# Each of the vocabularies contain the special OOV token <unk> as the first line. \n",
    "# We pop out that, from the en_vocabulary and de_vocabulary lists as we need this \n",
    "# for the next step:\n",
    "en_unk_token = en_vocabulary.pop(0)\n",
    "de_unk_token = de_vocabulary.pop(0)\n",
    "\n",
    "print(en_unk_token, de_unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8b1753-741f-4d67-b1d0-12df12107fa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## String lookup layer: Converting tokens to IDs\n",
    "\n",
    "Here we define a `StringLookup` layer for each language, which will convert string tokens to numerical IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00bccf8-c85e-48d0-a872-ecf5f2dc5294",
   "metadata": {},
   "source": [
    "- After getting the vocabulary of our text data, **we have one more text processing operation remaining, that is, converting the processed text tokens into numerical IDs.** \n",
    "\n",
    "- We are going to use a `tf.keras.layers.StringLookup` to create a layer in our model that converts each token into a numerical ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6c2f27-7926-47b6-ad45-54819f660a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_lookup_layer = tf.keras.layers.StringLookup(vocabulary=en_vocabulary, oov_token=en_unk_token,\n",
    "                                               mask_token=pad_token, pad_to_max_tokens=False)\n",
    "\n",
    "de_lookup_layer = tf.keras.layers.StringLookup(vocabulary=de_vocabulary, oov_token=de_unk_token,\n",
    "                                               mask_token=pad_token, pad_to_max_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935727cb-3ba0-4dd9-a185-ff548647840e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let’s understand the arguments provided to this layer:\n",
    "- `vocabulary` – Contains a list of words that are found in the corpus (except certain special tokens that will be discussed below)\n",
    "\n",
    "- `oov_token` – A special out-of-vocabulary token that will be used to replace tokens not listed in the vocabulary\n",
    "\n",
    "- `mask_token` – A special token that will be used to mask inputs (e.g. uninformative padded tokens)\n",
    "\n",
    "- `pad_to_max_tokens` – If padding should occur to bring arbitrary-length sequences in a batch of data to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e48f48d-3f10-48a9-aee1-65a6867f1360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word IDs: [ 4304 10519  6386     4     1   179     4  1840     5 19429  2315  7705\n",
      "   224     6]\n",
      "Sample vocabulary: ['<pad>', '<unk>', '<s>', '</s>', 'the', ',', '.', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for']\n"
     ]
    }
   ],
   "source": [
    "wid_sample = en_lookup_layer(\n",
    "    \"iron cement protects the ingot against the hot , abrasive steel casting process .\".split(\" \")\n",
    ")\n",
    "print(f\"Word IDs: {wid_sample}\")\n",
    "print(f\"Sample vocabulary: {en_lookup_layer.get_vocabulary()[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "754d2df2-83ef-41cc-a300-2713d34a82ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int64, numpy=array([4304,    0,    1], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lookup_layer(['iron', '<pad>', '<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdac171-86f1-4a7e-8800-cf28ab0245ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align='center'>\n",
    "    <h2><b>Defining the Model</b></h2>\n",
    "</div>\n",
    "\n",
    "\n",
    "Here we define the model. We'll be focusing on the following primary components.\n",
    "\n",
    "1. **Encoder** - Convert an English token sequence to a context vect\n",
    "\n",
    "2. **Decoder** - Consumes the context vector and generate predictions\n",
    "\n",
    "3. **Decoder Attention** - Allows the decoder to look at any encoder state in order to learn about the source sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435a5b1-d97c-4273-addd-cadf8ac6aaa1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defining the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fceeaa-deea-4241-ae27-f2e18522b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Defining the encoder layers\n",
    "encoder_input = layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "# Converting tokens to IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input) # wid here means word_id\n",
    "\n",
    "# Embedding layer and lookup\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(encoder_wid_out)\n",
    "\n",
    "# Encoder GRU layer\n",
    "encoder_gru_out, encoder_gru_last_state = layers.GRU(256, return_sequences=True, return_state=True)(encoder_emb_out)\n",
    "\n",
    "# Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfdf3d-be28-41da-94da-c35489aa8008",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h3>Explanation of the Encoder model</h3>\n",
    "</div>\n",
    "\n",
    "1. We start the encoder with an input layer. The input layer will take in a batch of sequences of tokens. Each sequence of tokens  is `n_en_seq_length` elements long. Remember that we padded or truncated the sentences to make sure all of them have a fixed length of `n_en_seq_length`:\n",
    "```\n",
    "# Defining the encoder layers\n",
    "encoder_input = layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "```\n",
    "\n",
    "2. Next we use the previously defined `StringLookup` layer to convert the string tokens into word IDs. As we saw, the `StringLookup` layer can take a list of unique words (i.e. a vocabulary) and create a lookup operation to convert a given token into a numerical ID:\n",
    "```\n",
    "# Converting tokens to IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "```\n",
    "\n",
    "3. With the tokens converted into IDs, we route the generated word IDs to a token embedding layer. We pass in the size of the vocabulary (derived from the en_lookup_layer's `get_vocabulary()` method) and the embedding size (128) and finally we ask the layer to mask any zero-valued inputs as they don’t contain any information: \n",
    "```\n",
    "# Embedding layer and lookup\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = layers.Embedding(en_full_vocab_size, 128, \n",
    "                                   mask_zero=True)(encoder_wid_out)\n",
    "```\n",
    "\n",
    "4. The output of the embedding layer is stored in encoder_emb_out. Next we define a GRU layer to process the sequence of English token embeddings:\n",
    "\n",
    "    - Note how we are setting both the `return_sequences` and `return_state` arguments to `True`. \n",
    "\n",
    "    - To recap, `return_sequences` returns the full sequence of hidden states as the output (instead of returning only the last), where `return_state` returns the last state of the model as an additional output. \n",
    "\n",
    "    - We need both these outputs to build the rest of our model. For example, we need to pass the last state of the encoder to the decoder as the initial state. For that, we need the last state of the encoder (stored in `encoder_gru_last_state`). \n",
    "    \n",
    "```\n",
    "# Encoder GRU layer\n",
    "encoder_gru_out, enocder_gru_last_state = layers.GRU(256, return_sequences=True, \n",
    "                                             return_state=True)(encoder_emb_out)\n",
    "```\n",
    "\n",
    "\n",
    "5. We now have everything to define the encoder part of our model. It takes in a batch of sequences of string tokens and returns the full sequence of GRU hidden states as the output:\n",
    "```\n",
    "# Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11750fab-74de-4280-ab02-4b8ca070b39d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defining the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c49270-105e-491f-a109-2b82afc010d9",
   "metadata": {},
   "source": [
    "Our decoder will be more complex than the encoder. **The objective of the decoder is, given the last encoder state and the previous token the decoder predicted, predict the next token.** \n",
    "\n",
    "For example, for the German sentence: `<s> ich ging zum Laden </s>` --> `i went to the store`\n",
    "\n",
    "We define:\n",
    "\n",
    "|  |  |  |  |  |  |\n",
    "| ----- | --- | ---- | ---- | --- | ----- |\n",
    "| **Input** | `<s>` | ich | ging | zum | Laden |\n",
    "| **Output** | ich | ging | zum | Laden | `</s>` |\n",
    "\n",
    "This technique is known as **teacher forcing**. In other words, the decoder is leveraging previous tokens of the target itself to predict the next token. This makes the translation task easier for the model.\n",
    "\n",
    "* **\n",
    "\n",
    "Training model architecture (Note that we're using GRU cells instead of LSTMs as shown in the image)\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/model_arc.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084461e-b3c3-490e-b2bd-86186dc9bca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "To feed in previous tokens predicted by the decoder, we need an input layer for the decoder.\n",
    "\n",
    "|  |  |  |  |  |  | |\n",
    "| ----- | --- | ---- | ---- | --- | ----- | --- |\n",
    "| **Input** | `<s>` | ich | ging | zum | Laden |---> i/p length is $n-1$ |\n",
    "| **Output** | ich | ging | zum | Laden | `</s>` |---> o/p length is $n-1$  |\n",
    "\n",
    "`<s> ich ging zum Laden </s>` ---> Actual length is $n$\n",
    "\n",
    "Like we saw in this table, When formulating the decoder inputs and outputs this way, for a sequence of tokens with length $n$, the input and output are $n-1$ tokens long. Thus in the decoder i/p layer we passed the `shape=(n_de_seq_length-1,)`:\n",
    "```\n",
    "# Defining the decoder layers \n",
    "decoder_input = layers.Input(shape=(n_de_seq_length-1,), dtype=tf.string)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bf40ca-6e8e-4617-94a6-d36ddd41d0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the decoder layers \n",
    "decoder_input = layers.Input(shape=(n_de_seq_length-1,), dtype=tf.string)\n",
    "# Converting tokens to IDs (Decoder)\n",
    "decoder_wid_out = de_lookup_layer(decoder_input) # wid here means word_id\n",
    "\n",
    "# Embedding layer and lookup (decoder)\n",
    "de_full_vocab_size = len(de_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = layers.Embedding(de_full_vocab_size, 128, mask_zero=True)(decoder_wid_out)\n",
    "\n",
    "# Decoder GRU layer\n",
    "decoder_gru_out = layers.GRU(256, return_sequences=True)(decoder_emb_out, \n",
    "                                                         initial_state=encoder_gru_last_state)\n",
    "\n",
    "# Note that we are passing the encoder’s last state to a special argument called \n",
    "# initial_state in the GRU’s call() method. This ensures that the decoder uses the \n",
    "# encoder's last state to initialize its memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af2005-bffb-49ae-8a56-126094d7a61a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Attention: Analyzing the encoder states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911e27d-6f13-45dc-8887-c01c813294a8",
   "metadata": {},
   "source": [
    "- The next step of our journey takes us to one of the most important concepts in machine learning, **'attention'**. \n",
    "\n",
    "- *So far, the decoder had to rely on the encoder's last state as the 'only' input/signal about the source language. This is like asking to summarize a sentence using a single word. Generally, when doing so, you lose a lot of the meaning and message in this conversion.* \n",
    "\n",
    "- **Attention** alleviates this problem.\n",
    "\n",
    "\n",
    "***Instead of relying just on the encoder's last state, attention enables the decoder to analyze the complete history of state outputs. The decoder does this at every step of the prediction and creates a weighted average of all the state outputs depending on what it needs to produce at that step.***\n",
    "\n",
    "*For example, in the translation `I went to the shop -> ich ging zum Laden`, when predicting the word `ging`, the decoder will pay more attention to the first part of the English sentence than the latter.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b5c7e-d615-4ac3-bf0b-cc2aa930d88c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\large{\\rightarrow}$ The context/thought vector is a performance bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312bca2-4d0f-4ea3-a57d-47d6bf546c3f",
   "metadata": {},
   "source": [
    "As we have seen in the encoder-decoder architecture of NMT, that the encoder part spits out a summazied representation of the source language sentence as *'context/thought vector'*, which basically creates a link b/w the encoder and the decoder; which later the decoder uses to translate the sentence.\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/enc_dec.png'/>\n",
    "</div>\n",
    "\n",
    "- ***To understand why the context/thought vector is a performance bottleneck***,\n",
    "\n",
    "    - Let's imagine translating the foll. English sentence: $$\\text{I went to the flower market to buy some flowers}$$\n",
    "\n",
    "    - This translates to the following: $$\\text{Ich ging zum Blumenmarkt, um Blumen zu kaufen}$$ \n",
    "\n",
    "- If we are to compress this into a fixed-length vector, the resulting vector needs to contain these:\n",
    "    - *Information about the subject (I)*\n",
    "    - *Information about the verbs (buy and went)*\n",
    "    - *Information about the objects (flowers and flower market)*\n",
    "    - *Interaction of the subjects, verbs, and objects with each other in the sentence*<br></br>\n",
    "\n",
    "\n",
    "- *Generally, the context vector has a size of 128 or 256 elements.* Reliance on the context vector to store all this information with a small-sized vector is very impractical and an extremely difficult requirement for the system. \n",
    "\n",
    "- *Therefore, most of the time, the context vector fails to provide the complete information required to make a good translation.*\n",
    "\n",
    "- **This results in an underperforming decoder that suboptimally translates a sentence.**\n",
    "\n",
    "\n",
    "- *To make the problem worse, during decoding the context vector is observed only in the beginning. Thereafter, the decoder GRU must memorize the context vector until the end of the translation. This becomes more and more difficult for long sentences.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed891a3f-203d-4401-99cc-6a6873f98294",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\large{\\rightarrow}$ How **Attention** deals with this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeffd4f-82fe-4f7a-893f-f4d842f90ab6",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h4>Attention sidesteps this issue:</h4>\n",
    "</div>\n",
    "\n",
    "\n",
    "1. **With attention, the decoder will have access to the full state history of the encoder for each decoding time step.** \n",
    "    - *This allows the decoder to access a very rich representation of the source sentence.*<br></br> \n",
    "\n",
    "2. Furthermore, the attention mechanism introduces a `softmax` layer **that allows the decoder to calculate a weighted mean of the past observed encoder states, which will be used as the context vector for the decoder.** \n",
    "    - *This allows the decoder to pay different amounts of attention to different words at different decoding steps.*<br></br>\n",
    "    \n",
    "    \n",
    "<div align='center'>\n",
    "    <h4>Conceptual breakdown of the Attention Mechanism</h4>\n",
    "    <img src='images/att_mech_nmt.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d75f7-a517-4aea-a7e0-ed1fdc491a8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **The Bahdanau Attention Mechanism**\n",
    "\n",
    "Also called ***Additive Attention***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21cf269-520d-4a9c-bec6-59ab9183ed21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### *Computing Attention:*\n",
    "\n",
    "- [The Bahdanau Attention Mechanism - ML Mastery](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/)\n",
    "- [The Bahdanau Attention Mechanism - dl.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/bahdanau-attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221ed94-e1c4-4e39-aac5-7217d761110e",
   "metadata": {},
   "source": [
    "The Bahdanau attention mechanism introduced in the paper [Neural Machine Translation by Learning to Jointly Align and Translate](https://arxiv.org/abs/1409.0473), by [Dzmitry Bahdanau](https://rizar.github.io/).\n",
    "\n",
    "**We'll implement a slightly different version of it, due to the limitations of TensorFlow.** \n",
    "\n",
    "Some notations:\n",
    "- Encoder's $j^{th}$ hidden state: $h_j$\n",
    "- $i^{th}$ target token: $y_i$\n",
    "- $i^{th}$ decode hidden state in the $i^{th}$ time step: $s_i$\n",
    "- Context Vector: $c_i$\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95824bf-6b81-4415-ae9d-6b1dcad970dc",
   "metadata": {},
   "source": [
    "- Our **decoder GRU** is a function of an input $y_i$ and a previous step's hidden state $s_{i-1}$. This can be represented as follows: $${GRU}_{dec} = \\mathcal{f}(y_i, s_{i-1})$$ \n",
    "\n",
    "    - Here, $\\mathcal{f}$ represents the actual update rules used to calculate $y_i$ and $s_{i-1}$.<br></br>\n",
    "    \n",
    "- *With the attention mechanism, we are introducing a new time-dependent context vector $c_i$ for the $i^{th}$ decoding step:* \n",
    "\n",
    "    - **The $c_i$ vector is a weighted mean of the hidden states of all the unrolled encoder steps.**\n",
    "\n",
    "    - A higher weight will be given to the $j^{th}$ hidden state of the encoder if the $j^{th}$ word is more important for translating the $i^{th}$ word in the target language. \n",
    "\n",
    "    - This means the model can learn which words are important at which time step, regardless of the directionality of the two languages or alignment mismatches.<br></br>\n",
    "\n",
    "- Now the decoder GRU becomes this: $${GRU}_{dec} = \\mathcal{f}(y_i, s_{i-1}, c_i)$$ \n",
    "\n",
    "\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fe3ad-5def-47fc-8d9e-6938cb310b88",
   "metadata": {},
   "source": [
    "- Conceptually, the attention mechanism can be thought of as a separate layer and as illustrated. As shown, **attention functions as a layer**. \n",
    "\n",
    "- The attention layer is responsible for producing the context vector $c_i$ for the $i^{th}$ time step of the decoding process. $c_i$ is calculated as: $$c_i = \\sum_{j=1}^{L}\\alpha_{ij}h_j$$\n",
    "\n",
    "    - Here, $L$ is the number of words in the source sentence, and \n",
    "    \n",
    "    - $\\alpha_{ij}$ is a normalized weight representing the importance of the $j^{th}$ encoder hidden state for calculating the $i^{th}$ decoder prediction.<br></br>\n",
    "    \n",
    "\n",
    "- $\\alpha_{ij}$ is calculated using something called **energy value**.\n",
    "    - We represent $e_{ij}$ as the energy of the encoder’s $j^{th}$ position for predicting the decoder’s $i^{th}$ position. \n",
    "    \n",
    "    - $e_{ij}$ is computed using a small fully connected network as follows: $$e_{ij} = \\nu_{a}^{T} \\tanh(W_a s_{i-1} + U_a h_j)$$ \n",
    "    \n",
    "    - In other words, $e_{ij}$ is calculated with a multilayer perceptron whose weights are $\\nu_{a}$, $W_a$, and $U_a$ ; and<br></br> $s_{i-1}$ (decoder’s previous hidden state from $(i-1)^{th}$ time step) and $h_j$ (encoder’s $j^{th}$ hidden output) are the inputs to this network.<br></br>\n",
    "    \n",
    "-  Finally, we compute the normalized energy values (i.e. weights) using softmax normalization over all encoder timesteps: $$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{L} \\exp({e_{ik}})}$$ <br></br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <h4>The Attention Mechanism</h4>\n",
    "    <img src='images/bahdanau_attention.png'/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ec1bf-4f0e-4f93-baa5-b44ecf42a462",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### $\\large{\\rightarrow}$ *Simple & Intuitive Explanation of Bahdanau Attention*\n",
    "\n",
    "Credit: ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d464d6-31df-494c-9ddf-d306806a4747",
   "metadata": {},
   "source": [
    "- **The Bahdanau Attention Mechanism, also known as Additive Attention**, is like a spotlight that helps a machine learning model focus on the most relevant parts of a long piece of information when making decisions, just like how you pay attention to different words when reading a sentence.\n",
    "\n",
    "Here's a simple and intuitive explanation:\n",
    "\n",
    "- Imagine you're translating a sentence from one language to another, and the sentence is quite long. Bahdanau Attention is like having a little assistant who highlights specific words in the original sentence for you as you translate.\n",
    "\n",
    "    1. **The Sentence**: Let's say you have a long sentence in a foreign language you want to translate, like \"The big blue car drove quickly down the winding mountain road.\"\n",
    "\n",
    "    2. **The Assistant**: Your Bahdanau Attention assistant looks at each word in the sentence and decides which words are the most important for you to pay attention to while translating.\n",
    "\n",
    "    3. **Highlighting**: It highlights certain words, like \"big,\" \"blue,\" and \"car,\" which are the key pieces of information for understanding the sentence.\n",
    "\n",
    "    4. **Translating**: As you translate, you focus more on the highlighted words, so you might say something like, \"The important thing here is that there's a big blue car.\" You give extra importance to those highlighted words because they carry the crucial details.\n",
    "\n",
    "    5. **Dynamic Attention**: What's cool is that the assistant can change its highlights for different sentences. If the next sentence is, \"The small red bicycle went slowly up the steep hill,\" it will highlight different words like \"small,\" \"red,\" and \"bicycle.\"\n",
    "\n",
    "In summary, Bahdanau Attention is like having a helpful spotlight that guides you through understanding and translating sentences by emphasizing the important words. It's a way for machines to focus on the relevant parts of information when processing sequences of data, making them more efficient and accurate in tasks like translation, summarization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb45547-cc42-4d1d-84f8-7ed76af46c27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## *Implementing Attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5874f7a-e1f9-4b4e-b89c-5beac8b5d615",
   "metadata": {},
   "source": [
    "- **As stated above we’ll implement a slightly different variation of Bahdanau attention.**\n",
    "    - *In the traditional Bahdanau attention mechanism, which is commonly used in sequence-to-sequence models, the attention scores are computed at each time step(time-dependent context vector) within the RNN model. These scores are then used to weigh the importance of different parts of the input sequence when making predictions.*\n",
    "    \n",
    "    - *TensorFlow currently does not support an attention mechanism that can be iteratively computed for each time step, similar to how an RNN works.*<br></br>\n",
    "\n",
    "- **Therefore, we are going to decouple the attention mechanism from the GRU model and have it computed separately.** \n",
    "\n",
    "    - *We will concatenate the attention output with the hidden output of the GRU layer and feed it to the final prediction layer. In other words, we are not feeding attention output to the GRU model, but directly to the prediction layer as illustrated below:*\n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "    <img src='images/att_mech_implemented.png'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32722b8b-df47-42e9-abdf-d6e1777cec46",
   "metadata": {},
   "source": [
    "* **\n",
    "\n",
    "To implement attention, we define a class called `BahdanauAttention` (which inherits from the `tf.keras.layers.Layer` class) and override two functions in that:\n",
    "- `__init__()` – Defines the layer's initialization logic\n",
    "- `call()` – Defines the computational logic of the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afda5a08-9c75-42e1-8e27-716705ee1ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        \n",
    "        # Additive attention layer, a.k.a. Bahdanau-style attention\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "        \n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "        \n",
    "        # Compute Wa.ht\n",
    "        wa_query = self.Wa(query)\n",
    "        \n",
    "        # Compute Ua.ht\n",
    "        ua_key = self.Ua(key)\n",
    "        \n",
    "        # Compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "        \n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(inputs = [wa_query, value, ua_key],\n",
    "                                                           mask = [query_mask, value_mask, value_mask],\n",
    "                                                           return_attention_scores = True)\n",
    "        \n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcf021-dd88-434a-93ff-f11bf00a30f9",
   "metadata": {},
   "source": [
    "<h3 align='center'>Explanation of Bahdanau Attention Code</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899e952-3640-4815-9002-2bf0db9de9d5",
   "metadata": {},
   "source": [
    "1. **the `__init__()` function:**\n",
    "    1. Here we are defining 3 layers: `weight matrix W_a`, `weight matrix U_a`, and the `AdditiveAttention` layer, which contains the attention computation logic.\n",
    "    \n",
    "    2. The `AdditiveAttention` layer takes in a `query`, `value`, and a `key`.\n",
    "        1. The `query` is the decoder states\n",
    "        2. The `value` & `key` are all the encoder states produced\n",
    "        \n",
    "        \n",
    "```\n",
    "def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # Weights to compute Bahdanau attention\n",
    "    self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    # Additive attention layer, a.k.a. Bahdanau-style attention\n",
    "    self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "```\n",
    "\n",
    "* **\n",
    "\n",
    "2. **the `call()` function:**\n",
    "    1. The first thing to note is that this func. takes in `query`, `key`, and `value` . These three elements will drive the attention computation. \n",
    "    \n",
    "    2. *In Bahdanau attention, you can think of the `key` and `value` as being the same thing*.\n",
    "    \n",
    "    3. *The `query` will represent each decoder GRU's hidden states for each time step, and the `value (or key)` will represent each encoder GRU’s hidden states for each time step.*\n",
    "    \n",
    "    4. *In other words, we are querying an output for each decoder position based on values provided by the encoder’s hidden states.*\n",
    "    \n",
    "    \n",
    "```\n",
    "def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "    # Compute Wa.ht\n",
    "    wa_query = self.Wa(query)\n",
    "\n",
    "    # Compute Ua.ht\n",
    "    ua_key = self.Ua(key)\n",
    "\n",
    "    # Compute masks\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    # Compute the attention\n",
    "    context_vector, attention_weights = self.attention(\n",
    "                        inputs = [wa_query, value, ua_key],\n",
    "                        mask = [query_mask, value_mask, value_mask],\n",
    "                        return_attention_scores = True\n",
    "                        )\n",
    "\n",
    "    if not return_attention_scores:\n",
    "        return context_vector\n",
    "    else:\n",
    "        return context_vector, attention_weights\n",
    "```\n",
    "\n",
    "* **\n",
    "\n",
    "3. Let's look at the computations we need to calculate as discussed in Computing Attention section:\n",
    "    1. Calculate **energy value** : $$e_{ij} = \\nu_{a}^{T} \\tanh(W_a s_{i-1} + U_a h_j)$$ \n",
    "    \n",
    "    2. Calculate normalized weights that represents the importance of encoder's hidden states while translating a sentence with the decoder : $$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{L} \\exp({e_{ik}})}$$\n",
    "    \n",
    "    3. Calculate context vector which is a weighted mean of the hidden states of all the unrolled encoder steps : $$c_i = \\sum_{j=1}^{L} \\alpha_{ij} h_{j}$$\n",
    "    \n",
    "* **\n",
    "\n",
    "4. First we compute `wa_query` (represents $W_{a}s_{i-1}$) and `ua_key` (represents $U_{a}h_{j}$), the we pass these values to the attention layer to perform the following steps:\n",
    "\n",
    "    - As discussed in the [tensorflow doc.](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention):\n",
    "    \n",
    "    - Inputs are `query` tensor of shape `[batch_size, Tq, dim]`, `value` tensor of shape `[batch_size, Tv, dim]` and `key` tensor of shape `[batch_size, Tv, dim]`. The calculation follows the steps:\n",
    "\n",
    "        1. Reshapes `wa_query` from `[batch_size, Tq, dim]` to shape `[batch_size, Tq, 1, dim]` and `ua_key` from `[batch_size, Tv, dim]` shape to `[batch_size, 1, Tv, dim]`.\n",
    "\n",
    "        2. Calculates `scores` with shape `[batch_size, Tq, Tv]` as: `scores = tf.reduce_sum(tf.tanh(query + key), axis=-1)`.\n",
    "\n",
    "        3. Uses `scores` to calculate a `distribution` with shape `[batch_size, Tq, Tv]` using `softmax` activation: `distribution = tf.nn.softmax(scores)`.\n",
    "\n",
    "        4. Uses `distribution` to create a linear combination of value with shape `[batch_size, Tq, dim]`.\n",
    "\n",
    "        5. Returns `tf.matmul(distribution, value)`, which represents a weighted average of all encoder states (i.e. value).\n",
    "        \n",
    "        \n",
    "- Here, you can see that `step B` performs the first equation, `step C` performs the second equation, and finally `step E` performs the third equation. \n",
    "\n",
    "- Another thing worth noting is that `step B` does not mention $\\nu_a$ from the first equation. $\\nu_a$ is essentially a weight matrix with which we compute the dot product. We can introduce this weight matrix by setting `use_scale=True` when defining the `AdditiveAttention` layer:\n",
    "\n",
    ">`self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)`\n",
    "\n",
    "\n",
    "- Another important argument is the `return_attention_scores` argument when calling the `AdditiveAttention` layer. This gives us the distribution weight matrix defined in `step C`. We will use this to visualize where the model was paying attention when decoding the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b148a-0e46-417d-8daf-93705fe3dc95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Defining the final model\n",
    "\n",
    "Continuing the implementation of **Decoder Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f6d87c6-0f5b-4fe1-afd0-77f7b6ee27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Attention to the decoder\n",
    "# The attention mechanism (inputs: [query, value, key])\n",
    "\n",
    "# When defining attention, we also pass a mask that denotes which tokens \n",
    "# need to be ignored when computing outputs (e.g. padded tokens).\n",
    "\n",
    "decoder_attn_out, attn_weights = BahdanauAttention(units=256)(\n",
    "                                    query=decoder_gru_out, \n",
    "                                    key=encoder_gru_out, value=encoder_gru_out,\n",
    "                                    mask=(encoder_wid_out != 0),\n",
    "                                    return_attention_scores=True\n",
    "                                 )\n",
    "\n",
    "# Concatenate decoder GRU output and the attention output \n",
    "# to feed the prediction layer\n",
    "context_and_rnn_output = layers.Concatenate(axis=-1)([decoder_attn_out, decoder_gru_out])\n",
    "\n",
    "# Final Prediction layer\n",
    "# de_full_vocab_size -> Size of the target language(German) vocab\n",
    "decoder_out = layers.Dense(de_full_vocab_size, \n",
    "                           activation='softmax')(context_and_rnn_output)\n",
    "\n",
    "\n",
    "# Now with the encoder and the decoder fully defined,\n",
    "# Let's define final Seq2Seq Model\n",
    "seq2seq_model = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input],\n",
    "                                      outputs=decoder_out)\n",
    "\n",
    "\n",
    "# Compile the Seq2Seq model\n",
    "seq2seq_model.compile(loss='sparse_categorical_crossentropy', \n",
    "                      optimizer='adam',\n",
    "                      metrics='accuracy')\n",
    "\n",
    "# seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009d8d1-2f8a-4d58-9c60-f256a534a73c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Secondary Model: `attention_visualizer`\n",
    "\n",
    "We are also going to define a secondary model called the `attention_visualizer`.\n",
    "\n",
    "- *The `attention_visualizer` can generate attention patterns for a given set of inputs. This is a handy way to know if the model is paying attention to the correct words during the decoding process. This visualizer model will be used once the full model is trained.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "079bbbe1-0a98-48af-bf79-2f1339e5394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input],\n",
    "                                             outputs=[attn_weights, decoder_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69bf01-bced-4ba2-8255-da4f09d4bf22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **Complete Code: Seq2Seq Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9389fa9-e6f4-43fe-b6d7-d7755e4257ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 36)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " string_lookup (StringLookup)   (None, 36)           0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " string_lookup_1 (StringLookup)  (None, 32)          0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 36, 128)      3200256     ['string_lookup[1][0]']          \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 32, 128)      3200256     ['string_lookup_1[1][0]']        \n",
      "                                                                                                  \n",
      " gru (GRU)                      [(None, 36, 256),    296448      ['embedding[0][0]']              \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 32, 256)      296448      ['embedding_1[0][0]',            \n",
      "                                                                  'gru[0][1]']                    \n",
      "                                                                                                  \n",
      " tf.__operators__.ne (TFOpLambd  (None, 36)          0           ['string_lookup[1][0]']          \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " bahdanau_attention (BahdanauAt  ((None, 32, 256),   131328      ['gru_1[0][0]',                  \n",
      " tention)                        (None, 32, 36))                  'gru[0][0]',                    \n",
      "                                                                  'tf.__operators__.ne[0][0]',    \n",
      "                                                                  'gru[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 512)      0           ['bahdanau_attention[0][0]',     \n",
      "                                                                  'gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32, 25002)    12826026    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,950,762\n",
      "Trainable params: 19,950,762\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Defining the encoder layers\n",
    "encoder_input = layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "# Converting tokens to IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input) # wid here means word_id\n",
    "\n",
    "# Embedding layer and lookup\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(encoder_wid_out)\n",
    "\n",
    "# Encoder GRU layer\n",
    "encoder_gru_out, encoder_gru_last_state = layers.GRU(256, return_sequences=True, \n",
    "                                                     return_state=True)(encoder_emb_out)\n",
    "\n",
    "# Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Defining the decoder layers \n",
    "decoder_input = layers.Input(shape=(n_de_seq_length-1,), dtype=tf.string)\n",
    "# Converting tokens to IDs (Decoder)\n",
    "decoder_wid_out = de_lookup_layer(decoder_input) # wid here means word_id\n",
    "\n",
    "# Embedding layer and lookup (decoder)\n",
    "de_full_vocab_size = len(de_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = layers.Embedding(de_full_vocab_size, 128, mask_zero=True)(decoder_wid_out)\n",
    "\n",
    "# Decoder GRU layer\n",
    "decoder_gru_out = layers.GRU(256, return_sequences=True)(decoder_emb_out, \n",
    "                                                         initial_state=encoder_gru_last_state)\n",
    "\n",
    "# Note that we are passing the encoder’s last state to a special argument called \n",
    "# initial_state in the GRU’s call() method. This ensures that the decoder uses the \n",
    "# encoder's last state to initialize its memory.\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Adding Attention to the decoder\n",
    "# The attention mechanism (inputs: [query, value, key])\n",
    "\n",
    "# When defining attention, we also pass a mask that denotes which tokens \n",
    "# need to be ignored when computing outputs (e.g. padded tokens).\n",
    "\n",
    "decoder_attn_out, attn_weights = BahdanauAttention(units=256)(\n",
    "                                    query=decoder_gru_out, \n",
    "                                    key=encoder_gru_out, value=encoder_gru_out,\n",
    "                                    mask=(encoder_wid_out != 0),\n",
    "                                    return_attention_scores=True\n",
    "                                 )\n",
    "\n",
    "# Concatenate decoder GRU output and the attention output \n",
    "# to feed the prediction layer\n",
    "context_and_rnn_output = layers.Concatenate(axis=-1)([decoder_attn_out, decoder_gru_out])\n",
    "\n",
    "# Final Prediction layer\n",
    "# de_full_vocab_size -> Size of the target language(German) vocab\n",
    "decoder_out = layers.Dense(de_full_vocab_size, \n",
    "                           activation='softmax')(context_and_rnn_output)\n",
    "\n",
    "\n",
    "# Now with the encoder and the decoder fully defined,\n",
    "# Let's define final Seq2Seq Model\n",
    "seq2seq_model = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input],\n",
    "                                      outputs=decoder_out)\n",
    "\n",
    "\n",
    "# Define a secondary model called the attention_visualizer\n",
    "# We will use this model later to visualize attention patterns\n",
    "attention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input],\n",
    "                                             outputs=[attn_weights, decoder_out])\n",
    "\n",
    "# Compile the Seq2Seq model\n",
    "seq2seq_model.compile(loss='sparse_categorical_crossentropy', \n",
    "                      optimizer='adam',\n",
    "                      metrics='accuracy')\n",
    "\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1e682-0ba6-465a-b4b8-4ca15aebacdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Computing BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e1a4a-9264-4348-b650-727dc23872c0",
   "metadata": {},
   "source": [
    "- BLEU score measures the quality/accuracy of translations produced by the model. BLEU score looks at `n`-grams of tokens produced by the decoder to measure how \"close\" the predicted translation is to the actual ground truth sequence.\n",
    "\n",
    "- The BLEU score denotes the number of `n`-grams (for example, unigrams and bigrams) of candidate translation that matched in the reference translation. So the higher the BLEU score, the better the MT system.\n",
    "\n",
    "Reference: [Tensorflow GitHub](https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6b8823b-cfb6-4b28-8087-7750f28c8e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "        \"\"\"Computes the BLEU score (Metric for machine translation)\"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and <s>/</s> tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"<\\/s>.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [sent if len(sent)>0 else en_unk_token for sent in translations ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            translations = np.char.split(translations).tolist()\n",
    "            \n",
    "            return translations\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785aa0b-b93b-4bd3-9750-97f8c3f44773",
   "metadata": {},
   "source": [
    "### Computing BLEU example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1bca3cd-d4ca-4007-a883-d3ab8b28d019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [[de_unk_token, de_unk_token, 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [[de_unk_token, 'einmal', 'mÃssen', en_unk_token, 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(f\"BLEU score with longer correctly predicte phrases: {bleu1}\")\n",
    "print(f\"BLEU score without longer correctly predicte phrases: {bleu2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13873aaf-aa10-4fed-bbde-837fb23d3db4",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <h2><b>Training the NMT</b></h2>\n",
    "</div>\n",
    "\n",
    "Here we define several utility functions to train the model:\n",
    "\n",
    "* `prepare_data()` - Prepares source/target language sentences by generating encoder inputs, decoder inputs and decoder labels\n",
    "* `shuffle_data()` - Shuffle the generated inputs randomly\n",
    "* `train_model()` - Train the model on a given dataset\n",
    "* `evaluate_model()` - Evaluates the model on a given dataset\n",
    "* `check_for_nan()` - Check if any samples causing NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3e087-7a9d-4b0a-a5a8-3a73fbb6017a",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src='images/training_nmt.png' title='Training NMT'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8df9f1ca-1581-4c15-aeb8-71b4072c8171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(['train', 'valid', 'test'], [train_xy, valid_xy, test_xy]):\n",
    "        \n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        de_inputs = data_y[:,:-1]\n",
    "        de_labels = de_lookup_layer(data_y[:,1:]).numpy()\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ea5bbc6-037b-4559-b1a8-7b31b665b981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72f73d9c-6415-46a9-b77f-baf9ef0246c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_for_nans(loss, model, en_lookup_layer, de_lookup_layer):\n",
    "    \n",
    "    if np.isnan(loss):\n",
    "        for r_i in range(len(y)):\n",
    "            loss_sample, _  = model.evaluate([x[0][r_i:r_i+1], x[1][r_i:r_i+1]], y[r_i:r_i+1], verbose=0)\n",
    "            if np.isnan(loss_sample):\n",
    "\n",
    "                print('='*25, 'nan detected', '='*25)\n",
    "                print('train_batch', i, 'r_i', r_i)\n",
    "                print('en_input ->', x[0][r_i].tolist())\n",
    "                print('en_input_wid ->', en_lookup_layer(x[0][r_i]).numpy().tolist())\n",
    "                print('de_input ->', x[1][r_i].tolist())\n",
    "                print('de_input_wid ->', de_lookup_layer(x[1][r_i]).numpy().tolist())\n",
    "                print('de_output_wid ->', y[r_i].tolist())\n",
    "\n",
    "                if r_i > 0:\n",
    "                    print('='*25, 'no-nan', '='*25)\n",
    "                    print('en_input ->', x[0][r_i-1].tolist())\n",
    "                    print('en_input_wid ->', en_lookup_layer(x[0][r_i-1]).numpy().tolist())\n",
    "                    print('de_input ->', x[1][r_i-1].tolist())\n",
    "                    print('de_input_wid ->', de_lookup_layer(x[1][r_i-1]).numpy().tolist())\n",
    "                    print('de_output_wid ->', y[r_i-1].tolist())\n",
    "                    return\n",
    "                else:\n",
    "                    continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97508182-43ac-48b6-b717-bcc82d23a6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, en_lookup_layer, de_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle=True, predict_bleu_at_training=False):\n",
    "    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            blue_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw,de_inputs_raw,de_labels), shuffle_inds  = shuffle_data(\n",
    "                data_dict['train']['encoder_inputs'],\n",
    "                data_dict['train']['decoder_inputs'],\n",
    "                data_dict['train']['decoder_labels'],\n",
    "                shuffle_inds\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw,de_inputs_raw,de_labels)  = (\n",
    "                data_dict['train']['encoder_inputs'],\n",
    "                data_dict['train']['decoder_inputs'],\n",
    "                data_dict['train']['decoder_labels'],\n",
    "            )\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "        \n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(f\"Training batch {i+1}/{n_train_batches}\", end='\\r')\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = de_labels[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            \n",
    "            # Check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, de_lookup_layer)\n",
    "                \n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)        \n",
    "            # Evaluate the model to get the metrics\n",
    "            #loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            \n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            \n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        if predict_bleu_at_training:\n",
    "            print(f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\")\n",
    "        else:\n",
    "            print(f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\")\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
    "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
    "        val_de_labels = data_dict['valid']['decoder_labels']\n",
    "            \n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model, de_lookup_layer, val_en_inputs, val_de_inputs, val_de_labels, batch_size\n",
    "        )\n",
    "            \n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(f\"\\t(valid) loss: {val_loss} - accuracy: {val_accuracy} - bleu: {val_bleu}\")\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #    \n",
    "    \n",
    "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
    "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
    "    test_de_labels = data_dict['test']['decoder_labels']\n",
    "            \n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model, de_lookup_layer, test_en_inputs, test_de_inputs, test_de_labels, batch_size\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n(test) loss: {test_loss} - accuracy: {test_accuracy} - bleu: {test_bleu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24b901ec-14d0-4c80-8f75-765c29e4f91d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, de_lookup_layer, en_inputs_raw, de_inputs_raw, de_labels, batch_size):\n",
    "    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    \n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(f\"Evaluating batch {i+1}/{n_batches}\", end='\\r')\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = de_labels[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e42f14-4ea1-42cd-bb10-fbf9932d1dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
